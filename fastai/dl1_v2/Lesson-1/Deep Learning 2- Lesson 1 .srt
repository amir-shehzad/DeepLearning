1
00:00:00,000 --> 00:00:03,810
hi everybody welcome to practical deep

2
00:00:03,810 --> 00:00:06,480
learning for coders this is part one of

3
00:00:06,480 --> 00:00:11,790
our two-part course I'm presenting this

4
00:00:11,790 --> 00:00:15,619
from the data Institute in San Francisco

5
00:00:15,619 --> 00:00:19,230
will be doing seven lessons in this part

6
00:00:19,230 --> 00:00:21,210
of the course most of them will be about

7
00:00:21,210 --> 00:00:22,410
a couple of hours long

8
00:00:22,410 --> 00:00:23,910
this first one may be a little bit

9
00:00:23,910 --> 00:00:27,900
shorter practical deep learning for

10
00:00:27,900 --> 00:00:30,179
coders is all about getting you up and

11
00:00:30,179 --> 00:00:32,520
running with deep learning in practice

12
00:00:32,520 --> 00:00:35,399
getting world-class results and it's a

13
00:00:35,399 --> 00:00:38,040
really coding focused approach as the

14
00:00:38,040 --> 00:00:39,629
name suggests but we're not going to

15
00:00:39,629 --> 00:00:41,790
dumb it down by the end of the course

16
00:00:41,790 --> 00:00:44,219
you all have learned all of the theory

17
00:00:44,219 --> 00:00:46,620
in details that are necessary to rebuild

18
00:00:46,620 --> 00:00:48,750
all of the world-class results we're

19
00:00:48,750 --> 00:00:50,329
learning about from scratch

20
00:00:50,329 --> 00:00:53,520
now I should mention that our videos are

21
00:00:53,520 --> 00:00:56,579
hosted on YouTube that we strongly

22
00:00:56,579 --> 00:00:58,739
recommend watching them via our website

23
00:00:58,739 --> 00:01:02,640
at course fast a I although they're

24
00:01:02,640 --> 00:01:05,189
exactly the same videos the important

25
00:01:05,189 --> 00:01:07,049
thing about watching them through our

26
00:01:07,049 --> 00:01:08,939
website is that you'll get all of the

27
00:01:08,939 --> 00:01:10,979
information you need about kind of

28
00:01:10,979 --> 00:01:14,189
updates to libraries my own locations

29
00:01:14,189 --> 00:01:16,650
further information frequently asked

30
00:01:16,650 --> 00:01:19,409
questions and so forth so if you're

31
00:01:19,409 --> 00:01:21,720
currently on YouTube watching this why

32
00:01:21,720 --> 00:01:23,189
don't you switch over to crosstalk fast

33
00:01:23,189 --> 00:01:25,290
at AI now and start watching through

34
00:01:25,290 --> 00:01:27,060
there and make sure you read all of the

35
00:01:27,060 --> 00:01:29,549
material on the page before you start

36
00:01:29,549 --> 00:01:30,869
just to make sure that you've got

37
00:01:30,869 --> 00:01:34,200
everything you need the other thing to

38
00:01:34,200 --> 00:01:36,090
mention is that there is a really great

39
00:01:36,090 --> 00:01:41,880
strong community at forums faster I from

40
00:01:41,880 --> 00:01:44,310
time to time you will find that you get

41
00:01:44,310 --> 00:01:47,310
stuck you may get stuck very early on

42
00:01:47,310 --> 00:01:49,290
you may not get stuck for quite a while

43
00:01:49,290 --> 00:01:50,790
but at some point you might get stuck

44
00:01:50,790 --> 00:01:53,790
with understanding why something works

45
00:01:53,790 --> 00:01:55,770
the way it does or there may be some

46
00:01:55,770 --> 00:01:57,930
computer problem that you have or so

47
00:01:57,930 --> 00:02:00,840
forth on forums don't fast at AI there

48
00:02:00,840 --> 00:02:02,939
are thousands of other learners talking

49
00:02:02,939 --> 00:02:05,310
about every lesson and lots of other

50
00:02:05,310 --> 00:02:07,799
topics besides it's the most active deep

51
00:02:07,799 --> 00:02:09,390
learning community on the internet by

52
00:02:09,390 --> 00:02:12,930
far so definitely register there

53
00:02:12,930 --> 00:02:14,939
and start getting involved you'll get a

54
00:02:14,939 --> 00:02:16,739
lot more out of this course if you do

55
00:02:16,739 --> 00:02:22,739
that so we're going to start by doing

56
00:02:22,739 --> 00:02:24,750
some coding this is an approach we're

57
00:02:24,750 --> 00:02:26,010
going to be talking about in the moment

58
00:02:26,010 --> 00:02:29,510
called the top-down approach to study

59
00:02:29,510 --> 00:02:33,090
but let's learn it by doing it so let's

60
00:02:33,090 --> 00:02:35,640
go ahead and try and actually train a

61
00:02:35,640 --> 00:02:38,310
neural network now in order to train a

62
00:02:38,310 --> 00:02:41,189
neural network you almost certainly want

63
00:02:41,189 --> 00:02:45,209
a GPU GPU of is a graphics processing a

64
00:02:45,209 --> 00:02:48,659
graphics processing unit it's the things

65
00:02:48,659 --> 00:02:52,409
that companies use to help you play

66
00:02:52,409 --> 00:02:56,159
games better they let your computer

67
00:02:56,159 --> 00:02:58,470
render the game much more quickly than

68
00:02:58,470 --> 00:03:01,620
your CPU okay we'll be talking about

69
00:03:01,620 --> 00:03:04,829
them more shortly but for now I'm going

70
00:03:04,829 --> 00:03:06,689
to show you how you can get access to a

71
00:03:06,689 --> 00:03:11,159
GPU specifically you're going to need an

72
00:03:11,159 --> 00:03:14,430
NVIDIA GPU because only NVIDIA GPUs

73
00:03:14,430 --> 00:03:17,190
support something called cooter cooter

74
00:03:17,190 --> 00:03:19,709
is the language and framework that

75
00:03:19,709 --> 00:03:22,919
nearly all deep learning libraries and

76
00:03:22,919 --> 00:03:26,329
practitioners use to do their work

77
00:03:26,329 --> 00:03:28,620
obviously it's not ideal that we're

78
00:03:28,620 --> 00:03:30,510
stuck with one particular vendors cards

79
00:03:30,510 --> 00:03:32,190
and over time we hope to see more

80
00:03:32,190 --> 00:03:34,230
competition in this base but for now we

81
00:03:34,230 --> 00:03:38,459
do need an NVIDIA GPU your laptop almost

82
00:03:38,459 --> 00:03:40,379
certainly doesn't have one unless you

83
00:03:40,379 --> 00:03:42,150
specifically went out of your way to buy

84
00:03:42,150 --> 00:03:46,889
like a gaming laptop so almost certainly

85
00:03:46,889 --> 00:03:50,190
you will need to rent one and the good

86
00:03:50,190 --> 00:03:53,400
news is that renting access paying by

87
00:03:53,400 --> 00:03:55,859
the second or a GPU based computer is

88
00:03:55,859 --> 00:03:59,220
pretty easy and pretty cheap I'm going

89
00:03:59,220 --> 00:04:03,150
to show you a couple of options the

90
00:04:03,150 --> 00:04:06,079
first option I'll show you which is

91
00:04:06,079 --> 00:04:09,379
probably the easiest is called Kressel

92
00:04:09,379 --> 00:04:15,269
if you go to kress allcom and click on

93
00:04:15,269 --> 00:04:16,620
sign up or if you've been there before

94
00:04:16,620 --> 00:04:19,739
sign-in you will find yourself at this

95
00:04:19,739 --> 00:04:22,019
screen which has a big button that says

96
00:04:22,019 --> 00:04:24,719
start jupiter and another switch called

97
00:04:24,719 --> 00:04:25,530
enable GP

98
00:04:25,530 --> 00:04:27,510
you so if we make sure that is set to

99
00:04:27,510 --> 00:04:30,300
true the Nabal GPU is on and we click

100
00:04:30,300 --> 00:04:37,040
start Jupiter and we click start Jupiter

101
00:04:37,040 --> 00:04:39,300
it's going to launch us into something

102
00:04:39,300 --> 00:04:40,860
called Jupiter notebook

103
00:04:40,860 --> 00:04:44,100
Jupiter notebook in a recent survey of

104
00:04:44,100 --> 00:04:46,440
tens of thousands of data scientists was

105
00:04:46,440 --> 00:04:48,990
rated as the third most important tool

106
00:04:48,990 --> 00:04:51,690
in the data scientist toolbox it's

107
00:04:51,690 --> 00:04:52,770
really important that you get to learn

108
00:04:52,770 --> 00:04:54,900
it well and all of our courses will be

109
00:04:54,900 --> 00:04:57,480
run through Jupiter yes Rachel you have

110
00:04:57,480 --> 00:04:59,460
a question or a comment I just wanted to

111
00:04:59,460 --> 00:05:01,650
point out that you get I believe 10 3

112
00:05:01,650 --> 00:05:06,860
hours so if you wanted to tricresyl out

113
00:05:06,860 --> 00:05:10,410
yeah he might have changed that recently

114
00:05:10,410 --> 00:05:12,060
to less hours but you can check the FAQ

115
00:05:12,060 --> 00:05:13,350
or the pricing but you certainly get

116
00:05:13,350 --> 00:05:16,320
some three hours the pricing varies

117
00:05:16,320 --> 00:05:18,090
because this is actually runs on top of

118
00:05:18,090 --> 00:05:19,860
Amazon Web Services so at the moment

119
00:05:19,860 --> 00:05:24,090
it's 60 cents an hour the nice thing is

120
00:05:24,090 --> 00:05:25,950
though that you can always turn it turn

121
00:05:25,950 --> 00:05:27,630
it on you know start your Jupiter

122
00:05:27,630 --> 00:05:29,580
without the CP without the GPU running

123
00:05:29,580 --> 00:05:31,440
and pay you a tenth of that price which

124
00:05:31,440 --> 00:05:34,229
is pretty cool

125
00:05:34,229 --> 00:05:36,030
so Jupiter notebook is something we'll

126
00:05:36,030 --> 00:05:38,250
be doing all of this course in and so to

127
00:05:38,250 --> 00:05:40,140
get started here we're going to find our

128
00:05:40,140 --> 00:05:42,150
particular course so we're go to courses

129
00:05:42,150 --> 00:05:48,510
and would go to fast a o2 and there they

130
00:05:48,510 --> 00:05:51,120
are things have been moving around a

131
00:05:51,120 --> 00:05:52,590
little bit so it may be in a different

132
00:05:52,590 --> 00:05:55,169
spot for you when you look at this and

133
00:05:55,169 --> 00:05:56,310
we'll make sure all the information

134
00:05:56,310 --> 00:05:59,090
current information is on the website

135
00:05:59,090 --> 00:06:02,430
now having said that that's you know the

136
00:06:02,430 --> 00:06:05,280
crystal approach is you know as you can

137
00:06:05,280 --> 00:06:08,960
see it's basically instant and and easy

138
00:06:08,960 --> 00:06:13,020
but if you've got you know an extra hour

139
00:06:13,020 --> 00:06:16,140
or so to get going and even better

140
00:06:16,140 --> 00:06:22,190
option is something called paper space

141
00:06:22,190 --> 00:06:25,140
paper space unlike Chris or doesn't run

142
00:06:25,140 --> 00:06:26,810
on top of Amazon they have their own

143
00:06:26,810 --> 00:06:32,389
machines

144
00:06:32,389 --> 00:06:36,449
and if I click on so here's here's paper

145
00:06:36,449 --> 00:06:38,959
space and so if I click on new machine I

146
00:06:38,959 --> 00:06:41,309
can pick which one of the three data

147
00:06:41,309 --> 00:06:43,319
centers to use so pick that one closest

148
00:06:43,319 --> 00:06:47,069
to you so I'll say a West Coast and then

149
00:06:47,069 --> 00:06:51,769
I'll say Linux and I'll say Ubuntu 16

150
00:06:51,769 --> 00:06:54,779
and then it says choose machine and you

151
00:06:54,779 --> 00:06:55,709
can see there's various different

152
00:06:55,709 --> 00:06:58,489
machines I can choose from and

153
00:06:58,489 --> 00:07:01,889
pay-by-the-hour so this is pretty cool

154
00:07:01,889 --> 00:07:04,800
for 40 cents an hour so it's cheaper

155
00:07:04,800 --> 00:07:07,169
than cresol I get a machine that's

156
00:07:07,169 --> 00:07:09,119
actually going to be much faster than

157
00:07:09,119 --> 00:07:12,269
Crescent 67 our machine or for 65 cents

158
00:07:12,269 --> 00:07:15,509
an hour way way way faster all right so

159
00:07:15,509 --> 00:07:17,580
I'm going to actually show you how to

160
00:07:17,580 --> 00:07:19,679
get started with the paper space

161
00:07:19,679 --> 00:07:22,769
approach because that actually is going

162
00:07:22,769 --> 00:07:26,099
to do everything from scratch you may

163
00:07:26,099 --> 00:07:27,389
find if you trailers

164
00:07:27,389 --> 00:07:30,179
do the 65 cents an hour one that it may

165
00:07:30,179 --> 00:07:32,339
require you to contact paper space to

166
00:07:32,339 --> 00:07:34,889
say like why do you want it and it's

167
00:07:34,889 --> 00:07:36,809
just an anti fraud the thing so if you

168
00:07:36,809 --> 00:07:41,849
say faster AI there then they'll quickly

169
00:07:41,849 --> 00:07:43,469
get you up and running so I'm going to

170
00:07:43,469 --> 00:07:45,209
use the cheapest one here 40 cents an

171
00:07:45,209 --> 00:07:50,579
hour you can pick how much draw it you

172
00:07:50,579 --> 00:07:54,360
want and note that you pay for a month

173
00:07:54,360 --> 00:07:56,219
of storage as soon as you start the

174
00:07:56,219 --> 00:07:58,439
machine up alright so don't start and

175
00:07:58,439 --> 00:07:59,909
stop lots of machines because each time

176
00:07:59,909 --> 00:08:01,349
you pay for that month of storage

177
00:08:01,349 --> 00:08:05,159
I think the 250 gig $7 a month option is

178
00:08:05,159 --> 00:08:07,860
pretty good but you only need 50 gig so

179
00:08:07,860 --> 00:08:08,879
if you're trying to minimize the price

180
00:08:08,879 --> 00:08:12,749
you can go there the only other thing

181
00:08:12,749 --> 00:08:16,019
you need to do is turn on public IP so

182
00:08:16,019 --> 00:08:17,550
that we can actually log into this and

183
00:08:17,550 --> 00:08:20,279
we can turn off auto snapshot to save

184
00:08:20,279 --> 00:08:27,450
the money or not having backups

185
00:08:27,450 --> 00:08:29,860
all right so if you then click on create

186
00:08:29,860 --> 00:08:32,380
your paper space about a minute later

187
00:08:32,380 --> 00:08:37,120
you will find that your machine will pop

188
00:08:37,120 --> 00:08:42,520
up here is my Ubuntu 1604 machine if you

189
00:08:42,520 --> 00:08:46,360
check your email you will find that they

190
00:08:46,360 --> 00:08:49,270
have emailed you a password so you can

191
00:08:49,270 --> 00:08:50,050
copy that

192
00:08:50,050 --> 00:08:54,190
and you can go to your machine and enter

193
00:08:54,190 --> 00:08:56,500
your password now to paste the password

194
00:08:56,500 --> 00:09:00,490
you would press ctrl shift V or on Mac

195
00:09:00,490 --> 00:09:04,000
against Apple shift V so it's slightly

196
00:09:04,000 --> 00:09:06,610
different to normal pasting or of course

197
00:09:06,610 --> 00:09:11,410
you can just type it in and here we are

198
00:09:11,410 --> 00:09:13,180
now we can make a little bit more room

199
00:09:13,180 --> 00:09:15,270
here by clicking on those little arrows

200
00:09:15,270 --> 00:09:18,550
there can zoom in a little bit and so as

201
00:09:18,550 --> 00:09:20,260
you can see we've got like a terminal

202
00:09:20,260 --> 00:09:24,760
that's sitting inside our browser it's

203
00:09:24,760 --> 00:09:26,170
just kind of quite a handy way to do it

204
00:09:26,170 --> 00:09:28,990
so now we need to configure this further

205
00:09:28,990 --> 00:09:31,240
course and so the way you can figure it

206
00:09:31,240 --> 00:09:37,990
for the course is you type curl HTTP

207
00:09:37,990 --> 00:09:41,260
colon slash slash files dot fast ai

208
00:09:41,260 --> 00:09:49,170
slash setup slash paper space type -

209
00:09:49,170 --> 00:09:50,290
okay

210
00:09:50,290 --> 00:09:52,840
and so that's then going to run a script

211
00:09:52,840 --> 00:09:55,270
which is going to set up all of the

212
00:09:55,270 --> 00:10:00,480
crudo drivers special Python Reaper

213
00:10:00,480 --> 00:10:03,120
Python distribution we use called

214
00:10:03,120 --> 00:10:05,830
anaconda all of the libraries or other

215
00:10:05,830 --> 00:10:09,100
courses and the data we use for the

216
00:10:09,100 --> 00:10:11,500
first part of the course ok so that

217
00:10:11,500 --> 00:10:14,290
takes an hour or so and when it's

218
00:10:14,290 --> 00:10:16,870
finished running you'll need to reboot

219
00:10:16,870 --> 00:10:19,750
your computer so to reboot not your own

220
00:10:19,750 --> 00:10:21,550
computer but your pages paste computer

221
00:10:21,550 --> 00:10:23,170
and so to do that you can just click on

222
00:10:23,170 --> 00:10:25,450
this little circular restart machine

223
00:10:25,450 --> 00:10:28,030
button ok and when it comes back up

224
00:10:28,030 --> 00:10:30,730
you'll be ready to go so what you'll

225
00:10:30,730 --> 00:10:34,930
find is if you've now got an anaconda

226
00:10:34,930 --> 00:10:36,700
three directory that's where your python

227
00:10:36,700 --> 00:10:40,030
is you've got a data directory which

228
00:10:40,030 --> 00:10:41,209
contains the data for

229
00:10:41,209 --> 00:10:43,129
the first part of this course first

230
00:10:43,129 --> 00:10:45,769
lesson which is their dogs and cats and

231
00:10:45,769 --> 00:10:49,869
you've got a fast AI directory and that

232
00:10:49,869 --> 00:10:53,420
contains everything for this course so

233
00:10:53,420 --> 00:10:58,879
what you should do is CD fast AI and

234
00:10:58,879 --> 00:11:00,949
from time to time you should go get Paul

235
00:11:00,949 --> 00:11:02,869
and that will just make sure that all of

236
00:11:02,869 --> 00:11:06,980
your fast AI stuff is up-to-date and

237
00:11:06,980 --> 00:11:09,139
also from time to time you might want to

238
00:11:09,139 --> 00:11:10,550
just check that your Python libraries

239
00:11:10,550 --> 00:11:11,959
are up-to-date and so you can type

240
00:11:11,959 --> 00:11:17,029
Condor and update to do that alright so

241
00:11:17,029 --> 00:11:19,279
make sure that you've CD into fast AI

242
00:11:19,279 --> 00:11:26,410
and then you can type Jupiter notebook

243
00:11:26,410 --> 00:11:28,189
all right there it is

244
00:11:28,189 --> 00:11:30,379
so we now have a Jupiter notebook

245
00:11:30,379 --> 00:11:32,569
servant running and we want to connect

246
00:11:32,569 --> 00:11:34,850
to that and so you can see here it says

247
00:11:34,850 --> 00:11:38,059
copy paste this URL into your browser

248
00:11:38,059 --> 00:11:40,730
when you connect so if you double click

249
00:11:40,730 --> 00:11:46,850
on it then that will actually that will

250
00:11:46,850 --> 00:11:49,639
actually copy it for you then you can go

251
00:11:49,639 --> 00:11:52,309
and paste it but you need to change this

252
00:11:52,309 --> 00:11:55,790
local host to be the paper space IP

253
00:11:55,790 --> 00:11:57,829
address so if you click on the little

254
00:11:57,829 --> 00:12:00,499
arrows to go smaller you can see the IP

255
00:12:00,499 --> 00:12:03,049
addresses here so I'll just copy that

256
00:12:03,049 --> 00:12:08,149
and paste it where it used to say local

257
00:12:08,149 --> 00:12:10,699
host okay so it's now HTTP and then my

258
00:12:10,699 --> 00:12:13,339
IP and then everything else a copied

259
00:12:13,339 --> 00:12:17,779
before and so there it is so this is the

260
00:12:17,779 --> 00:12:22,249
faster I get repo and our courses are

261
00:12:22,249 --> 00:12:25,579
all in courses and in there the deep

262
00:12:25,579 --> 00:12:28,939
learning part one is DL one and in there

263
00:12:28,939 --> 00:12:33,589
you will find lesson one by Mb ipython

264
00:12:33,589 --> 00:12:38,910
notebook

265
00:12:38,910 --> 00:12:41,920
so here we are ready to go depending

266
00:12:41,920 --> 00:12:43,690
whether you're using Kressel or paper

267
00:12:43,690 --> 00:12:45,220
space or something else if you check

268
00:12:45,220 --> 00:12:47,350
course it's not fast today I will keep

269
00:12:47,350 --> 00:12:49,630
putting additional videos and links to

270
00:12:49,630 --> 00:12:51,150
information about how to set up other

271
00:12:51,150 --> 00:12:54,430
you know good Jupiter notebook providers

272
00:12:54,430 --> 00:13:01,150
as well so to run a cell in Jupiter

273
00:13:01,150 --> 00:13:03,240
notebook you select the cell and you

274
00:13:03,240 --> 00:13:07,240
hold down shift and press Enter or if

275
00:13:07,240 --> 00:13:09,400
you've got the tool bar showing you can

276
00:13:09,400 --> 00:13:12,310
just click on the little Run button so

277
00:13:12,310 --> 00:13:14,580
you'll notice that some cells contain

278
00:13:14,580 --> 00:13:18,280
code and some contain text and some

279
00:13:18,280 --> 00:13:20,140
contain pictures and some contain videos

280
00:13:20,140 --> 00:13:24,370
so this environment basically has you

281
00:13:24,370 --> 00:13:26,680
know it's a it's a way that we can give

282
00:13:26,680 --> 00:13:29,290
you access to and a way to run

283
00:13:29,290 --> 00:13:31,720
experiments and to kind of tell you

284
00:13:31,720 --> 00:13:34,390
what's going on so pictures

285
00:13:34,390 --> 00:13:36,420
this is why it's like a super popular

286
00:13:36,420 --> 00:13:39,910
tool in data science a data science is

287
00:13:39,910 --> 00:13:41,770
kind of all about running experiments

288
00:13:41,770 --> 00:13:46,420
really so let's go ahead and click run

289
00:13:46,420 --> 00:13:49,300
and you'll see that cell turn into a

290
00:13:49,300 --> 00:13:50,950
star the one turn into a star for a

291
00:13:50,950 --> 00:13:53,260
moment and then it finished running okay

292
00:13:53,260 --> 00:13:55,060
so let's try the next one this time

293
00:13:55,060 --> 00:13:56,530
instead of using the toolbar I'm going

294
00:13:56,530 --> 00:13:59,470
to hold down shift and press ENTER and

295
00:13:59,470 --> 00:14:01,030
you can see again it turn into a star

296
00:14:01,030 --> 00:14:03,040
and then it said to so if I hold down

297
00:14:03,040 --> 00:14:05,170
shift and keep pressing enter it just

298
00:14:05,170 --> 00:14:08,350
keeps running each so right so I can put

299
00:14:08,350 --> 00:14:10,630
anything I like for example one plus one

300
00:14:10,630 --> 00:14:16,900
is two so what we're going to do is

301
00:14:16,900 --> 00:14:20,770
we're going to yes Rachel this is just a

302
00:14:20,770 --> 00:14:22,660
side note but I wanted to point out that

303
00:14:22,660 --> 00:14:25,390
we're using Python 3 here yes thank you

304
00:14:25,390 --> 00:14:29,020
pythons are still using Python - mmhmm

305
00:14:29,020 --> 00:14:32,770
yeah um and it is important to switch to

306
00:14:32,770 --> 00:14:36,640
Python 3 you know now well for Class A I

307
00:14:36,640 --> 00:14:39,910
you require it but you know increasingly

308
00:14:39,910 --> 00:14:42,970
a lot of libraries are removing support

309
00:14:42,970 --> 00:14:48,499
for Python - thanks Rachel

310
00:14:48,499 --> 00:14:50,719
now it mentions here that you can

311
00:14:50,719 --> 00:14:52,729
download the data set for this lesson

312
00:14:52,729 --> 00:14:56,619
from this location if you're using

313
00:14:56,619 --> 00:15:00,409
crystal or the paper space script that

314
00:15:00,409 --> 00:15:02,089
we just used to set up that this will

315
00:15:02,089 --> 00:15:03,709
already be and made available for you

316
00:15:03,709 --> 00:15:05,809
okay if you're not you'll need to W get

317
00:15:05,809 --> 00:15:11,479
it as now Kressel is quite a bit slower

318
00:15:11,479 --> 00:15:15,649
than paper space and also it there are

319
00:15:15,649 --> 00:15:16,969
some particular things it doesn't

320
00:15:16,969 --> 00:15:19,639
support that we really need and so there

321
00:15:19,639 --> 00:15:21,439
are a couple of extra steps if you're

322
00:15:21,439 --> 00:15:23,449
using cresol you have to run two more

323
00:15:23,449 --> 00:15:25,609
cells right so you can see these are

324
00:15:25,609 --> 00:15:27,049
commented out there quite hashes at the

325
00:15:27,049 --> 00:15:29,569
start so if you remove the hashes from

326
00:15:29,569 --> 00:15:31,489
these and run these two additional cells

327
00:15:31,489 --> 00:15:34,039
that just runs the stuff that the stuff

328
00:15:34,039 --> 00:15:35,749
that you only need for crystal I'm using

329
00:15:35,749 --> 00:15:38,139
paper space so I'm not going to run it

330
00:15:38,139 --> 00:15:45,829
ok so inside our data so we set up this

331
00:15:45,829 --> 00:15:48,949
path to data slash dogs cats that's pre

332
00:15:48,949 --> 00:15:51,019
set up for you and so inside there you

333
00:15:51,019 --> 00:15:52,249
can see here I can use an exclamation

334
00:15:52,249 --> 00:15:57,799
mark to basically say I don't want to

335
00:15:57,799 --> 00:16:00,139
run Python but I want to run bash right

336
00:16:00,139 --> 00:16:02,539
I want to run shell so this runs a bash

337
00:16:02,539 --> 00:16:04,789
command and the bit inside the curly

338
00:16:04,789 --> 00:16:07,429
brackets actually refers however to a

339
00:16:07,429 --> 00:16:09,889
python variable so inserts that python

340
00:16:09,889 --> 00:16:11,839
variable into the batch command so

341
00:16:11,839 --> 00:16:13,849
here's the contents of our folder

342
00:16:13,849 --> 00:16:16,309
there's a training set and a validation

343
00:16:16,309 --> 00:16:19,369
set if you're not familiar with the idea

344
00:16:19,369 --> 00:16:22,039
of training sets and validation sets it

345
00:16:22,039 --> 00:16:23,839
would be a very good idea to check out

346
00:16:23,839 --> 00:16:26,649
our practical machine learning course

347
00:16:26,649 --> 00:16:29,179
which tells you a lot about this kind of

348
00:16:29,179 --> 00:16:32,209
stuff if like yeah the basics of how to

349
00:16:32,209 --> 00:16:34,849
setup and run machine learning projects

350
00:16:34,849 --> 00:16:37,849
more generally would you recommend that

351
00:16:37,849 --> 00:16:40,509
people take that course before this one

352
00:16:40,509 --> 00:16:43,579
actually a lot of students who would you

353
00:16:43,579 --> 00:16:45,559
know as they went through these as said

354
00:16:45,559 --> 00:16:46,759
they'll they've liked doing them

355
00:16:46,759 --> 00:16:49,279
together so you can kind of check it out

356
00:16:49,279 --> 00:16:55,299
and see the machine learning course

357
00:16:55,299 --> 00:16:57,949
yeah they cover with some similar stuff

358
00:16:57,949 --> 00:16:59,569
but all in different directions so

359
00:16:59,569 --> 00:17:01,249
people have done both seen you know say

360
00:17:01,249 --> 00:17:02,089
they find it they

361
00:17:02,089 --> 00:17:04,909
they each support each other I wouldn't

362
00:17:04,909 --> 00:17:07,429
say it's a prerequisite but you know if

363
00:17:07,429 --> 00:17:09,049
I do if I say something like hey this is

364
00:17:09,049 --> 00:17:09,980
the training set and this is a

365
00:17:09,980 --> 00:17:11,390
validation set and you're going I don't

366
00:17:11,390 --> 00:17:12,289
know what that means

367
00:17:12,289 --> 00:17:14,539
at least Google but do a quick read you

368
00:17:14,539 --> 00:17:16,970
know because we're assuming that you

369
00:17:16,970 --> 00:17:20,630
know the very basics of kind of what

370
00:17:20,630 --> 00:17:22,909
machine learning is and does to some

371
00:17:22,909 --> 00:17:25,279
extent and I have a whole blog post on

372
00:17:25,279 --> 00:17:27,319
this topic as well okay and we'll make

373
00:17:27,319 --> 00:17:29,539
sure that you link to that from Pastor

374
00:17:29,539 --> 00:17:29,990
day night

375
00:17:29,990 --> 00:17:31,610
and as we just wanted to say in general

376
00:17:31,610 --> 00:17:35,000
with fasting our philosophy is to kind

377
00:17:35,000 --> 00:17:37,309
of learn things on an as-needed basis

378
00:17:37,309 --> 00:17:39,590
yeah exactly don't try and learn

379
00:17:39,590 --> 00:17:41,210
everything that you think you might need

380
00:17:41,210 --> 00:17:42,649
first otherwise you'll never get around

381
00:17:42,649 --> 00:17:44,059
elite learning the stuff you actually

382
00:17:44,059 --> 00:17:46,340
want to learn exactly that shows up in

383
00:17:46,340 --> 00:17:48,919
deep learning I think particularly a lot

384
00:17:48,919 --> 00:17:53,570
yes okay so in our validation folder

385
00:17:53,570 --> 00:17:55,039
there's a cat's folder and a dog's

386
00:17:55,039 --> 00:17:57,919
folder and then inside the validation

387
00:17:57,919 --> 00:18:00,909
cats folder is a whole bunch of JPEGs

388
00:18:00,909 --> 00:18:03,500
the reason that it's set up like this is

389
00:18:03,500 --> 00:18:05,510
that this is kind of the most common

390
00:18:05,510 --> 00:18:08,059
standard approach for how image

391
00:18:08,059 --> 00:18:10,700
classification datasets shared and

392
00:18:10,700 --> 00:18:12,710
provided and the idea is that each

393
00:18:12,710 --> 00:18:15,350
folder tells you the label

394
00:18:15,350 --> 00:18:18,169
so there's each of these images is

395
00:18:18,169 --> 00:18:21,350
labeled cats and each of the images and

396
00:18:21,350 --> 00:18:23,570
the dogs folder is labelled dogs okay

397
00:18:23,570 --> 00:18:25,970
this is how chaos works as well for

398
00:18:25,970 --> 00:18:31,070
example so this is a pretty standard way

399
00:18:31,070 --> 00:18:37,460
to share image classification files so

400
00:18:37,460 --> 00:18:40,100
we can have a look so if you go plot dot

401
00:18:40,100 --> 00:18:43,399
a.m. show we can see an example of the

402
00:18:43,399 --> 00:18:47,440
first of the cats if you haven't seen

403
00:18:47,440 --> 00:18:51,529
this before this is a Python 3.6 format

404
00:18:51,529 --> 00:18:53,750
string so you can google for that if you

405
00:18:53,750 --> 00:18:54,919
haven't seen it it's a very convenient

406
00:18:54,919 --> 00:18:56,779
way to do string formatting and we use

407
00:18:56,779 --> 00:19:01,190
it a lot so there's no cat but we're

408
00:19:01,190 --> 00:19:03,230
going to mainly be interested in the

409
00:19:03,230 --> 00:19:05,289
underlying data that makes up that cat

410
00:19:05,289 --> 00:19:10,220
so specifically it's an image whose

411
00:19:10,220 --> 00:19:12,110
shape that is the dimensions of the

412
00:19:12,110 --> 00:19:14,390
array is 198 by 1/7

413
00:19:14,390 --> 00:19:16,490
9x3 is it's a three dimensional array

414
00:19:16,490 --> 00:19:19,610
plus a quarter rank three tensor and

415
00:19:19,610 --> 00:19:21,740
here are the first four rows and four

416
00:19:21,740 --> 00:19:26,350
columns of that image so as you can see

417
00:19:26,350 --> 00:19:32,330
each of those cells has three items in

418
00:19:32,330 --> 00:19:35,060
it and this is the red green and blue

419
00:19:35,060 --> 00:19:37,940
pixel values between naught and 255 so

420
00:19:37,940 --> 00:19:40,550
here's a little subset of what a picture

421
00:19:40,550 --> 00:19:42,820
actually looks like inside your computer

422
00:19:42,820 --> 00:19:46,400
so that's that that's will be our idea

423
00:19:46,400 --> 00:19:48,530
is to take these kinds of numbers and

424
00:19:48,530 --> 00:19:50,990
use them to predict whether those kinds

425
00:19:50,990 --> 00:19:53,770
of numbers represent a cat or a dog

426
00:19:53,770 --> 00:19:56,000
based on looking at lots of pictures of

427
00:19:56,000 --> 00:19:58,850
cats and dogs so that's a pretty hard

428
00:19:58,850 --> 00:20:01,580
thing to do and at the point in time

429
00:20:01,580 --> 00:20:04,850
when this this this data set actually

430
00:20:04,850 --> 00:20:06,350
comes from a caracal competition the

431
00:20:06,350 --> 00:20:08,300
dogs versus cats caracal competition and

432
00:20:08,300 --> 00:20:10,430
when it was released in I think it was

433
00:20:10,430 --> 00:20:13,850
2012 the state of the art was 80%

434
00:20:13,850 --> 00:20:16,160
accuracy so computers weren't really

435
00:20:16,160 --> 00:20:19,490
able to at all accurately recognize dogs

436
00:20:19,490 --> 00:20:23,660
versus cats so let's go ahead and train

437
00:20:23,660 --> 00:20:32,420
a model so here are the three lines of

438
00:20:32,420 --> 00:20:36,170
code necessary to train a model and so

439
00:20:36,170 --> 00:20:38,030
let's go ahead and run it so I'll click

440
00:20:38,030 --> 00:20:39,650
on this on the cell I'll press shift

441
00:20:39,650 --> 00:20:45,260
enter and then we'll wait a couple of

442
00:20:45,260 --> 00:20:47,210
seconds for it to pop up and there it

443
00:20:47,210 --> 00:20:52,280
goes okay and it's training and so I've

444
00:20:52,280 --> 00:20:53,960
asked it to do three epochs so that

445
00:20:53,960 --> 00:20:55,490
means it's going to look at every image

446
00:20:55,490 --> 00:20:57,650
three times in total or look at the

447
00:20:57,650 --> 00:21:00,170
entire set of images three times that's

448
00:21:00,170 --> 00:21:02,720
what we mean by an epoch and as we do

449
00:21:02,720 --> 00:21:07,430
it's going to print out the accuracy is

450
00:21:07,430 --> 00:21:09,020
that's lasted for three numbers it

451
00:21:09,020 --> 00:21:11,680
prints out on the validation set okay

452
00:21:11,680 --> 00:21:13,790
the first three numbers we'll talk about

453
00:21:13,790 --> 00:21:16,160
later in short they're the value of the

454
00:21:16,160 --> 00:21:17,750
loss function which is in this case the

455
00:21:17,750 --> 00:21:19,820
cross-entropy loss for the training set

456
00:21:19,820 --> 00:21:21,950
and the validation set and then right at

457
00:21:21,950 --> 00:21:24,200
the start here is the epoch number so

458
00:21:24,200 --> 00:21:28,820
you can see it's getting about 90%

459
00:21:28,820 --> 00:21:33,260
see and it took 17 seconds so you can

460
00:21:33,260 --> 00:21:35,990
see we've come a long way since 2012 and

461
00:21:35,990 --> 00:21:39,920
in fact even in the competition this

462
00:21:39,920 --> 00:21:42,500
actually would have won the caracal

463
00:21:42,500 --> 00:21:44,450
competition of that time the best in the

464
00:21:44,450 --> 00:21:46,880
caracal competition was 98.9 and we're

465
00:21:46,880 --> 00:21:50,240
getting about 99% so this play surprised

466
00:21:50,240 --> 00:21:54,470
you that we're getting a you know Kaggle

467
00:21:54,470 --> 00:21:59,350
winning as of 20 and of 2012 early 2013

468
00:21:59,350 --> 00:22:02,780
kaggle winning image classifier in 17

469
00:22:02,780 --> 00:22:08,840
seconds that and three lines of code and

470
00:22:08,840 --> 00:22:10,250
I think that's because like a lot of

471
00:22:10,250 --> 00:22:13,520
people assume that deep learning takes a

472
00:22:13,520 --> 00:22:17,060
huge amount of time and lots of

473
00:22:17,060 --> 00:22:19,760
resources and lots of data and as you'll

474
00:22:19,760 --> 00:22:22,370
learn in this course that in general

475
00:22:22,370 --> 00:22:25,580
rule isn't true one of the ways we've

476
00:22:25,580 --> 00:22:28,490
made it much simpler is that this code

477
00:22:28,490 --> 00:22:32,020
is written on top of a library we built

478
00:22:32,020 --> 00:22:35,810
imaginatively called fast AI the faster

479
00:22:35,810 --> 00:22:38,090
a library is basically a library which

480
00:22:38,090 --> 00:22:41,930
takes all of the best practices

481
00:22:41,930 --> 00:22:44,060
approaches that we can find and so each

482
00:22:44,060 --> 00:22:46,640
time a paper comes out you know we that

483
00:22:46,640 --> 00:22:48,980
looks interesting we test it out if it

484
00:22:48,980 --> 00:22:50,750
works well for a variety of data sets

485
00:22:50,750 --> 00:22:52,580
and we can figure out how to tune it we

486
00:22:52,580 --> 00:22:54,920
implement it in fast AI and so faster I

487
00:22:54,920 --> 00:22:56,420
kind of curates all this stuff and

488
00:22:56,420 --> 00:22:59,360
packages up for you and much of the time

489
00:22:59,360 --> 00:23:01,730
but most the time kind of automatically

490
00:23:01,730 --> 00:23:03,110
figures out the best way to handle

491
00:23:03,110 --> 00:23:05,630
things so the first day our library is

492
00:23:05,630 --> 00:23:06,860
why we were able to do this in just

493
00:23:06,860 --> 00:23:09,680
three lines of code and the reason that

494
00:23:09,680 --> 00:23:11,000
we were able to make the faster I

495
00:23:11,000 --> 00:23:13,940
library work so well is because it

496
00:23:13,940 --> 00:23:15,620
interns it's on top of something called

497
00:23:15,620 --> 00:23:20,630
pi torch which is a really flexible deep

498
00:23:20,630 --> 00:23:23,030
learning and machine learning and GPU

499
00:23:23,030 --> 00:23:27,130
computation library written by Facebook

500
00:23:27,130 --> 00:23:29,720
most people are more familiar with

501
00:23:29,720 --> 00:23:30,560
tensorflow

502
00:23:30,560 --> 00:23:33,140
than pi torch because google markets

503
00:23:33,140 --> 00:23:35,690
that pretty heavily but most of the top

504
00:23:35,690 --> 00:23:37,730
researchers I know nowadays at least the

505
00:23:37,730 --> 00:23:39,320
ones that are at Google have switched

506
00:23:39,320 --> 00:23:42,500
across to PI torch yes Rachel

507
00:23:42,500 --> 00:23:43,940
and we'll be covering some pie torts

508
00:23:43,940 --> 00:23:46,070
later in the course yeah it's I mean one

509
00:23:46,070 --> 00:23:49,580
of the things that hopefully you'll

510
00:23:49,580 --> 00:23:52,490
really like about last AI is that it's

511
00:23:52,490 --> 00:23:54,530
really flexible that you can use all

512
00:23:54,530 --> 00:23:56,510
these kind of curated best practices as

513
00:23:56,510 --> 00:23:59,120
much as little as you want and so really

514
00:23:59,120 --> 00:24:01,669
easy to hook in at any point and write

515
00:24:01,669 --> 00:24:04,309
your own data augmentation write your

516
00:24:04,309 --> 00:24:06,620
own loss function write your own network

517
00:24:06,620 --> 00:24:08,780
architecture whatever and so we'll do

518
00:24:08,780 --> 00:24:12,710
all of those things in this course so

519
00:24:12,710 --> 00:24:15,770
what does this model look like well what

520
00:24:15,770 --> 00:24:20,000
we can do is we can take a look at so

521
00:24:20,000 --> 00:24:22,120
what are the what is the validation set

522
00:24:22,120 --> 00:24:24,740
dependent variable the Y look like and

523
00:24:24,740 --> 00:24:27,169
it's just a bunch of zeros and ones okay

524
00:24:27,169 --> 00:24:29,659
so the zeros if we look at data dot

525
00:24:29,659 --> 00:24:31,820
classes the zeros represent cats the

526
00:24:31,820 --> 00:24:34,039
ones represent dogs you'll see here

527
00:24:34,039 --> 00:24:35,360
there's basically two objects I'm

528
00:24:35,360 --> 00:24:36,799
working with one is an object called

529
00:24:36,799 --> 00:24:39,140
data which contains the validation and

530
00:24:39,140 --> 00:24:41,090
training data and another one is the

531
00:24:41,090 --> 00:24:43,159
object called learn which contains the

532
00:24:43,159 --> 00:24:45,919
model right so anytime you want to find

533
00:24:45,919 --> 00:24:47,210
something out about the data we can look

534
00:24:47,210 --> 00:24:50,360
inside data so we're going to get

535
00:24:50,360 --> 00:24:52,190
predictions through our validation set

536
00:24:52,190 --> 00:24:54,770
and so to do that we can call learned

537
00:24:54,770 --> 00:24:59,059
predict and so you can see here the

538
00:24:59,059 --> 00:25:01,010
first ten predictions and what it's

539
00:25:01,010 --> 00:25:04,039
giving you as a prediction for dog and a

540
00:25:04,039 --> 00:25:07,100
prediction for cat now the way PI Torche

541
00:25:07,100 --> 00:25:08,960
generally works and therefore fast AI

542
00:25:08,960 --> 00:25:11,600
also works is that most models return

543
00:25:11,600 --> 00:25:16,130
the log of the predictions rather than

544
00:25:16,130 --> 00:25:18,140
the probabilities themselves we'll learn

545
00:25:18,140 --> 00:25:20,780
why that is later in the course so for

546
00:25:20,780 --> 00:25:21,950
now recognize that to get your

547
00:25:21,950 --> 00:25:25,370
probabilities you have to get e to the

548
00:25:25,370 --> 00:25:28,330
power of you'll see here we're using

549
00:25:28,330 --> 00:25:31,760
numpy NP is none play if you're not

550
00:25:31,760 --> 00:25:33,620
familiar with numpy that is one of the

551
00:25:33,620 --> 00:25:35,600
things that we assume that you have some

552
00:25:35,600 --> 00:25:38,299
familiarity with so be sure to check out

553
00:25:38,299 --> 00:25:40,549
the material on cost I passed at AI to

554
00:25:40,549 --> 00:25:45,559
learn the basics of number it's the way

555
00:25:45,559 --> 00:25:51,620
that - handles all of the fast numerical

556
00:25:51,620 --> 00:25:54,590
programming array computation that kind

557
00:25:54,590 --> 00:25:55,840
of thing

558
00:25:55,840 --> 00:25:58,660
okay so we can get the probabilities

559
00:25:58,660 --> 00:26:03,100
using that using MP dot X and there's a

560
00:26:03,100 --> 00:26:04,630
few functions here that you can look at

561
00:26:04,630 --> 00:26:05,950
yourself if you're interested that's

562
00:26:05,950 --> 00:26:07,390
just some flooding functions that we'll

563
00:26:07,390 --> 00:26:12,750
use and so we can now plot some random

564
00:26:12,750 --> 00:26:17,650
correct images and so here are some

565
00:26:17,650 --> 00:26:20,590
images that it's correct about okay and

566
00:26:20,590 --> 00:26:23,050
so remember one is a dog so anything

567
00:26:23,050 --> 00:26:26,200
greater than 0.5 is dog and zero is a

568
00:26:26,200 --> 00:26:28,420
cat so this is what 10 to the negative 5

569
00:26:28,420 --> 00:26:32,010
obviously a cat here are some which are

570
00:26:32,010 --> 00:26:34,810
incorrect alright so you can see that

571
00:26:34,810 --> 00:26:36,280
some of these which it thinks are

572
00:26:36,280 --> 00:26:38,860
incorrect obviously are just the you

573
00:26:38,860 --> 00:26:40,210
know images that shouldn't be there at

574
00:26:40,210 --> 00:26:43,480
all but clearly this one which it called

575
00:26:43,480 --> 00:26:47,260
a a dog is not at all a dog so there are

576
00:26:47,260 --> 00:26:52,660
some obvious mistakes we can also take a

577
00:26:52,660 --> 00:26:56,890
look at which cats is it the most

578
00:26:56,890 --> 00:26:59,860
confident are cats which dogs are the

579
00:26:59,860 --> 00:27:03,360
most doglike the most confident dogs

580
00:27:03,360 --> 00:27:05,650
perhaps more interestingly we can also

581
00:27:05,650 --> 00:27:07,240
see which cats is that the most

582
00:27:07,240 --> 00:27:09,700
confident are actually dogs so which

583
00:27:09,700 --> 00:27:12,330
ones it is it the most wrong about and

584
00:27:12,330 --> 00:27:15,880
same thing for the ones the dog said it

585
00:27:15,880 --> 00:27:18,130
really thinks of cats and again some of

586
00:27:18,130 --> 00:27:20,740
these are just pretty weird I guess

587
00:27:20,740 --> 00:27:23,890
there is a dog in there yes Rachel I see

588
00:27:23,890 --> 00:27:25,750
suit you want to see more about why you

589
00:27:25,750 --> 00:27:28,450
would want to look at your data yeah

590
00:27:28,450 --> 00:27:33,940
sure so yeah so finally I just mentioned

591
00:27:33,940 --> 00:27:35,950
the last one we've got here is to see

592
00:27:35,950 --> 00:27:37,960
which ones have the probability closest

593
00:27:37,960 --> 00:27:39,840
to 0.5 so these are the ones that the

594
00:27:39,840 --> 00:27:42,070
model knows it doesn't really know what

595
00:27:42,070 --> 00:27:43,990
to do with and some of these it's not

596
00:27:43,990 --> 00:27:48,310
surprising so yeah I mean this is kind

597
00:27:48,310 --> 00:27:51,970
of like always the first thing I do

598
00:27:51,970 --> 00:27:54,460
after I build a model is to try to find

599
00:27:54,460 --> 00:27:57,300
a way to like visualize what it's built

600
00:27:57,300 --> 00:27:59,230
because if I want to make the model

601
00:27:59,230 --> 00:28:01,900
better then I need to take advantage of

602
00:28:01,900 --> 00:28:03,850
the things that's doing well and fix the

603
00:28:03,850 --> 00:28:05,710
things that's doing badly and so in this

604
00:28:05,710 --> 00:28:08,600
case and off

605
00:28:08,600 --> 00:28:10,220
this is the case I've learned something

606
00:28:10,220 --> 00:28:11,780
about the data set itself which is that

607
00:28:11,780 --> 00:28:13,640
there are some things that are in here

608
00:28:13,640 --> 00:28:17,270
that probably shouldn't be but I've also

609
00:28:17,270 --> 00:28:21,350
like it's also clear that this model has

610
00:28:21,350 --> 00:28:25,010
room to improve like to me that's pretty

611
00:28:25,010 --> 00:28:28,100
obviously a dog but one thing I'm

612
00:28:28,100 --> 00:28:31,160
suspicious about here is this image is

613
00:28:31,160 --> 00:28:38,030
very kind of fat and short and as we all

614
00:28:38,030 --> 00:28:41,600
learn the way these algorithms work is

615
00:28:41,600 --> 00:28:44,060
it kind of grabs a square piece at a

616
00:28:44,060 --> 00:28:45,320
time

617
00:28:45,320 --> 00:28:47,240
so this rather makes me suspicious that

618
00:28:47,240 --> 00:28:48,140
we're going to need to use something

619
00:28:48,140 --> 00:28:50,270
called data augmentation that we'll

620
00:28:50,270 --> 00:28:52,820
learn about learn about later to handle

621
00:28:52,820 --> 00:29:00,320
this properly okay so that's it right

622
00:29:00,320 --> 00:29:05,360
we've now built we've now built an image

623
00:29:05,360 --> 00:29:07,700
classifier and something that you should

624
00:29:07,700 --> 00:29:13,210
try now is to grab some data yourself

625
00:29:13,210 --> 00:29:17,390
some pictures of two or more different

626
00:29:17,390 --> 00:29:19,550
types of thing put them in different

627
00:29:19,550 --> 00:29:22,580
folders and run the same three lines of

628
00:29:22,580 --> 00:29:27,500
code on them okay and you'll find that

629
00:29:27,500 --> 00:29:31,040
it will work for that as well as long as

630
00:29:31,040 --> 00:29:33,550
that they are pictures of things like

631
00:29:33,550 --> 00:29:35,900
the kinds of things that people normally

632
00:29:35,900 --> 00:29:38,380
take photos of right so if their

633
00:29:38,380 --> 00:29:40,700
microscope microscope pictures or

634
00:29:40,700 --> 00:29:44,210
pathology pictures or CT scans or

635
00:29:44,210 --> 00:29:46,370
something this won't work very well as

636
00:29:46,370 --> 00:29:47,870
well learn about later there are some

637
00:29:47,870 --> 00:29:49,400
other things we didn't need to do to

638
00:29:49,400 --> 00:29:51,410
make that work but for things that look

639
00:29:51,410 --> 00:29:56,240
like normal photos these you can run

640
00:29:56,240 --> 00:29:58,580
exactly the same three lines of code and

641
00:29:58,580 --> 00:30:02,180
just point your path variable somewhere

642
00:30:02,180 --> 00:30:05,300
else to get your own image classifier so

643
00:30:05,300 --> 00:30:10,070
for example one student took those three

644
00:30:10,070 --> 00:30:12,320
lines of code downloaded for Google

645
00:30:12,320 --> 00:30:14,780
Images ten examples of pictures of

646
00:30:14,780 --> 00:30:17,120
people playing cricket ten examples of

647
00:30:17,120 --> 00:30:19,310
people playing baseball and build a

648
00:30:19,310 --> 00:30:22,460
classifier of those images which was new

649
00:30:22,460 --> 00:30:25,910
perfectly correct the same student

650
00:30:25,910 --> 00:30:28,220
actually also tried downloading seven

651
00:30:28,220 --> 00:30:32,330
pictures of Canadian currency seven

652
00:30:32,330 --> 00:30:34,610
pictures of American currency and again

653
00:30:34,610 --> 00:30:37,250
in that case the model was a hundred

654
00:30:37,250 --> 00:30:39,230
percent accurate so you can just go to

655
00:30:39,230 --> 00:30:41,390
Google Images if you like and download a

656
00:30:41,390 --> 00:30:43,040
few things of a few different classes

657
00:30:43,040 --> 00:30:45,680
and see see what works and tell us on

658
00:30:45,680 --> 00:30:47,810
the forum both your successes and your

659
00:30:47,810 --> 00:30:55,300
failures so what we just did was to

660
00:30:55,300 --> 00:30:58,340
train a neural network but we didn't

661
00:30:58,340 --> 00:30:59,780
first of all tell you what a neural

662
00:30:59,780 --> 00:31:02,800
network is or what training means or

663
00:31:02,800 --> 00:31:07,850
anything why is that well this is the

664
00:31:07,850 --> 00:31:10,730
start of our top-down approach to

665
00:31:10,730 --> 00:31:13,940
learning and basically the idea is that

666
00:31:13,940 --> 00:31:16,250
unlike the way math and technical

667
00:31:16,250 --> 00:31:18,620
subjects I usually talk where you learn

668
00:31:18,620 --> 00:31:21,350
every little element piece by piece and

669
00:31:21,350 --> 00:31:23,030
you don't actually get to put them all

670
00:31:23,030 --> 00:31:25,460
together and build your own image

671
00:31:25,460 --> 00:31:28,250
classifier until third year of graduate

672
00:31:28,250 --> 00:31:31,310
school our approach is to say from the

673
00:31:31,310 --> 00:31:34,370
start hey let's show you how to train an

674
00:31:34,370 --> 00:31:36,230
image classifier and you can start doing

675
00:31:36,230 --> 00:31:39,530
stuff and then gradually we dig deeper

676
00:31:39,530 --> 00:31:44,000
and deeper and deeper and so the idea is

677
00:31:44,000 --> 00:31:47,690
that throughout the course you're going

678
00:31:47,690 --> 00:31:50,360
to see like new problems that we want to

679
00:31:50,360 --> 00:31:53,330
solve so for example in the next lesson

680
00:31:53,330 --> 00:31:57,050
we'll look at well what if we're not

681
00:31:57,050 --> 00:31:59,210
looking at normal kinds of photos but

682
00:31:59,210 --> 00:32:01,400
we're looking at satellite images and

683
00:32:01,400 --> 00:32:03,470
we'll see why it is that this approach

684
00:32:03,470 --> 00:32:05,420
that we're learning today doesn't quite

685
00:32:05,420 --> 00:32:07,910
work as well and what things do we have

686
00:32:07,910 --> 00:32:09,800
to change and so we'll learn enough

687
00:32:09,800 --> 00:32:11,720
about the theory to understand why that

688
00:32:11,720 --> 00:32:13,520
happens and then we'll learn about the

689
00:32:13,520 --> 00:32:15,680
libraries and how we can change change

690
00:32:15,680 --> 00:32:17,210
things with the libraries to make that

691
00:32:17,210 --> 00:32:21,920
work better and so during the course we

692
00:32:21,920 --> 00:32:23,540
are gradually going to learn to solve

693
00:32:23,540 --> 00:32:26,000
more and more problems as we do so we

694
00:32:26,000 --> 00:32:28,010
all need to learn more and more parts of

695
00:32:28,010 --> 00:32:30,050
the library more and more bits of the

696
00:32:30,050 --> 00:32:32,990
theory until by the end we're actually

697
00:32:32,990 --> 00:32:35,580
going to learn how to create a

698
00:32:35,580 --> 00:32:39,540
world plus neural net architecture from

699
00:32:39,540 --> 00:32:41,970
scratch and our own training loop from

700
00:32:41,970 --> 00:32:43,860
scratch and so were actually built

701
00:32:43,860 --> 00:32:46,650
everything ourselves so that's the

702
00:32:46,650 --> 00:32:50,280
general approach yes Rachel and

703
00:32:50,280 --> 00:32:52,260
sometimes also call this the whole game

704
00:32:52,260 --> 00:32:55,250
which is inspired by harvard professor

705
00:32:55,250 --> 00:32:58,380
david perkins yeah and so the idea with

706
00:32:58,380 --> 00:33:00,450
the whole game is like this is more like

707
00:33:00,450 --> 00:33:02,640
how you would learn baseball or music

708
00:33:02,640 --> 00:33:04,890
with baseball you would get taken to a

709
00:33:04,890 --> 00:33:06,360
ball game you would learn what baseball

710
00:33:06,360 --> 00:33:10,440
is you would start playing it and it

711
00:33:10,440 --> 00:33:12,000
would only be years later that you might

712
00:33:12,000 --> 00:33:14,460
learn about the physics of how curveball

713
00:33:14,460 --> 00:33:17,640
works for example well with music we put

714
00:33:17,640 --> 00:33:20,040
a instrument in your hand and you start

715
00:33:20,040 --> 00:33:21,810
banging the drum or hitting the

716
00:33:21,810 --> 00:33:23,670
xylophone and it's not until years later

717
00:33:23,670 --> 00:33:25,380
that you learn about the circle of

718
00:33:25,380 --> 00:33:27,990
fifths and understand how to construct a

719
00:33:27,990 --> 00:33:32,040
cadence example so yeah so that this is

720
00:33:32,040 --> 00:33:33,390
kind of the approach we're using it's

721
00:33:33,390 --> 00:33:36,210
very inspired by David Perkins and other

722
00:33:36,210 --> 00:33:40,080
writers of Education so what that does

723
00:33:40,080 --> 00:33:42,330
mean is to take advantage of this as we

724
00:33:42,330 --> 00:33:45,480
peel back the layers we want you to keep

725
00:33:45,480 --> 00:33:47,280
like looking under the hood yourself as

726
00:33:47,280 --> 00:33:50,160
well like experiment a lot now because

727
00:33:50,160 --> 00:33:53,880
this is a very code driven approach so

728
00:33:53,880 --> 00:33:55,410
here's basically what happens right we

729
00:33:55,410 --> 00:33:58,410
start out looking today at convolutional

730
00:33:58,410 --> 00:34:01,380
neural networks for images and then in a

731
00:34:01,380 --> 00:34:03,060
couple of lessons we'll start to look at

732
00:34:03,060 --> 00:34:04,800
how to use neural nets to look at

733
00:34:04,800 --> 00:34:06,900
structured data and then look at

734
00:34:06,900 --> 00:34:09,120
language data and then look at

735
00:34:09,120 --> 00:34:13,350
recommendation system data and then we

736
00:34:13,350 --> 00:34:15,360
kind of then take all of those depths

737
00:34:15,360 --> 00:34:17,340
and we go backwards through them in

738
00:34:17,340 --> 00:34:19,290
reverse order so now you know by the end

739
00:34:19,290 --> 00:34:24,000
of that fourth piece you will know by

740
00:34:24,000 --> 00:34:26,190
the end of lesson four how to create a

741
00:34:26,190 --> 00:34:28,350
world-class image classifier a

742
00:34:28,350 --> 00:34:31,290
world-class structured data analysis

743
00:34:31,290 --> 00:34:34,440
program world-class language classifier

744
00:34:34,440 --> 00:34:37,590
broad class recommendation system and

745
00:34:37,590 --> 00:34:38,970
then we're going to go back over all of

746
00:34:38,970 --> 00:34:41,100
them again and learn in depth about like

747
00:34:41,100 --> 00:34:42,929
well what exactly did it do and how to

748
00:34:42,929 --> 00:34:45,000
do work and how do we change things

749
00:34:45,000 --> 00:34:46,590
around and use it in different

750
00:34:46,590 --> 00:34:48,760
situations for for

751
00:34:48,760 --> 00:34:51,270
recommendation systems structured data

752
00:34:51,270 --> 00:34:54,550
images and then finally back to language

753
00:34:54,550 --> 00:34:57,970
so that's how it's going to work so what

754
00:34:57,970 --> 00:35:00,190
that kind of means is that most students

755
00:35:00,190 --> 00:35:02,890
find that they tend to watch the videos

756
00:35:02,890 --> 00:35:07,570
two or three times but not like watch

757
00:35:07,570 --> 00:35:09,400
lesson one two or three times and listen

758
00:35:09,400 --> 00:35:10,900
to two or three times in verse and three

759
00:35:10,900 --> 00:35:12,550
three times but like they do the whole

760
00:35:12,550 --> 00:35:14,620
thing into end lessons one through seven

761
00:35:14,620 --> 00:35:17,500
and then go back and start lesson one

762
00:35:17,500 --> 00:35:20,950
again that's an approach which a lot of

763
00:35:20,950 --> 00:35:22,630
people find when they want to go back

764
00:35:22,630 --> 00:35:24,520
and understand all the details enough

765
00:35:24,520 --> 00:35:26,920
that can work pretty well so I would say

766
00:35:26,920 --> 00:35:29,200
you know aim to get through to the end

767
00:35:29,200 --> 00:35:34,150
of lesson seven you know as as quickly

768
00:35:34,150 --> 00:35:37,180
as you can rather than aiming to fully

769
00:35:37,180 --> 00:35:41,010
understand every detail from this data

770
00:35:41,010 --> 00:35:45,130
so basically the plan is that in today's

771
00:35:45,130 --> 00:35:49,600
lesson you'll learn in as few lines of

772
00:35:49,600 --> 00:35:51,820
code as possible with as few details as

773
00:35:51,820 --> 00:35:53,650
possible how do you actually build an

774
00:35:53,650 --> 00:35:55,750
image classifier with deep learning to

775
00:35:55,750 --> 00:35:58,480
do this to in this case say hey here are

776
00:35:58,480 --> 00:36:00,760
some pictures of dogs as opposed to

777
00:36:00,760 --> 00:36:04,300
pictures of cats then we're going to

778
00:36:04,300 --> 00:36:07,990
learn how to look at different kinds of

779
00:36:07,990 --> 00:36:09,850
images and particularly we're going to

780
00:36:09,850 --> 00:36:12,460
look at images of from satellites and

781
00:36:12,460 --> 00:36:13,950
we're going to say for a satellite image

782
00:36:13,950 --> 00:36:17,140
what kinds of things might you be seeing

783
00:36:17,140 --> 00:36:18,280
in that image and there could be

784
00:36:18,280 --> 00:36:20,320
multiple things that we're looking at so

785
00:36:20,320 --> 00:36:22,920
a multi-label classification problem

786
00:36:22,920 --> 00:36:25,750
from there we'll move to something which

787
00:36:25,750 --> 00:36:28,360
is perhaps the most widely applicable

788
00:36:28,360 --> 00:36:30,910
for the most people which is looking at

789
00:36:30,910 --> 00:36:33,610
what we call structured data so data

790
00:36:33,610 --> 00:36:37,680
about data that kind of comes from

791
00:36:37,680 --> 00:36:40,450
databases or spreadsheets so we're going

792
00:36:40,450 --> 00:36:42,070
to specifically look at this data set of

793
00:36:42,070 --> 00:36:44,560
predicting sales the number of things

794
00:36:44,560 --> 00:36:47,650
that are sold at different stores on

795
00:36:47,650 --> 00:36:49,780
different dates based on different

796
00:36:49,780 --> 00:36:52,360
holidays and and so on and so forth and

797
00:36:52,360 --> 00:36:53,860
so we're going to be doing its sales

798
00:36:53,860 --> 00:36:57,880
forecasting exercise after that we're

799
00:36:57,880 --> 00:36:59,620
going to look at language and we're

800
00:36:59,620 --> 00:37:01,360
going to figure out

801
00:37:01,360 --> 00:37:04,360
what this person thinks about the movie

802
00:37:04,360 --> 00:37:06,580
is on be given and will be able to

803
00:37:06,580 --> 00:37:08,530
figure out how to create just like we

804
00:37:08,530 --> 00:37:10,570
create image classifiers for any kind of

805
00:37:10,570 --> 00:37:12,640
image will learn to create in NLP

806
00:37:12,640 --> 00:37:15,250
classifiers to classify any kind of

807
00:37:15,250 --> 00:37:19,600
language in lots of different ways then

808
00:37:19,600 --> 00:37:20,230
we'll look at something called

809
00:37:20,230 --> 00:37:22,150
collaborative filtering which is used

810
00:37:22,150 --> 00:37:24,820
mainly for recommendation systems we're

811
00:37:24,820 --> 00:37:26,080
going to be looking at this data set

812
00:37:26,080 --> 00:37:27,910
that showed four different people for

813
00:37:27,910 --> 00:37:29,890
different movies what rating did they

814
00:37:29,890 --> 00:37:31,660
give it and here are some of the movies

815
00:37:31,660 --> 00:37:34,870
and so this is maybe an easier way to

816
00:37:34,870 --> 00:37:36,460
think about it is there are lots of

817
00:37:36,460 --> 00:37:38,530
different users and lots of different

818
00:37:38,530 --> 00:37:40,840
movies and then for each one we can look

819
00:37:40,840 --> 00:37:42,550
up for each user how much they liked

820
00:37:42,550 --> 00:37:44,770
that movie and the goal will be of

821
00:37:44,770 --> 00:37:47,290
course to predict for user movie

822
00:37:47,290 --> 00:37:49,360
combinations we haven't seen before are

823
00:37:49,360 --> 00:37:51,640
they likely to enjoy that movie or not

824
00:37:51,640 --> 00:37:54,670
and that's the really common approach

825
00:37:54,670 --> 00:37:57,190
used for like deciding what stuff to put

826
00:37:57,190 --> 00:37:59,020
on your homepage when somebody's

827
00:37:59,020 --> 00:38:00,940
visiting you know what book might they

828
00:38:00,940 --> 00:38:02,680
want to read or what film might they

829
00:38:02,680 --> 00:38:06,910
want to see or so forth from there we

830
00:38:06,910 --> 00:38:09,160
could have then dig back into language a

831
00:38:09,160 --> 00:38:11,220
bit more and we're going to look at

832
00:38:11,220 --> 00:38:13,630
actually we're gonna look at the

833
00:38:13,630 --> 00:38:15,250
writings of Nietzsche the philosopher

834
00:38:15,250 --> 00:38:17,250
and learn how to create our own

835
00:38:17,250 --> 00:38:19,810
Nietzsche philosophy from scratch

836
00:38:19,810 --> 00:38:22,210
character by character so this here

837
00:38:22,210 --> 00:38:24,820
perhaps that every life values a blood

838
00:38:24,820 --> 00:38:26,680
of intercourse when it senses there is

839
00:38:26,680 --> 00:38:28,480
unscrupulous who's very right sense to

840
00:38:28,480 --> 00:38:31,290
impulse love is not actually Nietzsche

841
00:38:31,290 --> 00:38:34,330
that's actually like some character by

842
00:38:34,330 --> 00:38:37,120
character generated text that we built

843
00:38:37,120 --> 00:38:41,560
with this recurrent neural network and

844
00:38:41,560 --> 00:38:43,240
then finally we're going to loop all the

845
00:38:43,240 --> 00:38:45,730
way back to computer vision again we're

846
00:38:45,730 --> 00:38:47,530
going to learn how not just to recognize

847
00:38:47,530 --> 00:38:49,810
cats from dogs how to actually find like

848
00:38:49,810 --> 00:38:51,910
where the cat is with this kind of hate

849
00:38:51,910 --> 00:38:54,190
map and we're also going to learn how to

850
00:38:54,190 --> 00:38:56,830
write our own architectures from scratch

851
00:38:56,830 --> 00:38:59,050
so this is an example of a resonate

852
00:38:59,050 --> 00:39:02,260
which is the kind of network that we are

853
00:39:02,260 --> 00:39:04,600
using in today's lesson for computer

854
00:39:04,600 --> 00:39:06,730
vision and so we'll actually end up

855
00:39:06,730 --> 00:39:08,830
building the network and the training

856
00:39:08,830 --> 00:39:10,900
loop from scratch and so they're

857
00:39:10,900 --> 00:39:12,730
basically the the steps that we're going

858
00:39:12,730 --> 00:39:14,410
to be taking from here

859
00:39:14,410 --> 00:39:15,849
and at each step we're going to be

860
00:39:15,849 --> 00:39:18,160
getting into increasing amounts of

861
00:39:18,160 --> 00:39:20,680
detail about how to actually do these

862
00:39:20,680 --> 00:39:25,780
things yourself so we've actually heard

863
00:39:25,780 --> 00:39:27,869
that from our students of past courses

864
00:39:27,869 --> 00:39:31,780
about what they've found and one of the

865
00:39:31,780 --> 00:39:34,089
things that we've heard a lot of

866
00:39:34,089 --> 00:39:37,119
students say is that there's been too

867
00:39:37,119 --> 00:39:41,829
much time on theory and research and not

868
00:39:41,829 --> 00:39:44,470
enough time running the code and even

869
00:39:44,470 --> 00:39:47,079
after we tell people about this morning

870
00:39:47,079 --> 00:39:48,849
where they still come to the end of the

871
00:39:48,849 --> 00:39:50,680
course not and say I wish I had taken

872
00:39:50,680 --> 00:39:54,099
more seriously that advice which is to

873
00:39:54,099 --> 00:39:56,799
keep running code so these are actual

874
00:39:56,799 --> 00:39:59,440
quotes from our forum in retrospect I

875
00:39:59,440 --> 00:40:00,760
should have spent the majority of my

876
00:40:00,760 --> 00:40:03,400
time on the actual code and the

877
00:40:03,400 --> 00:40:07,390
notebooks see what goes in see what

878
00:40:07,390 --> 00:40:13,359
comes out now this idea that you can

879
00:40:13,359 --> 00:40:17,049
create world-class models in a code

880
00:40:17,049 --> 00:40:19,210
first approach learning what you need as

881
00:40:19,210 --> 00:40:21,460
you go is very different to a lot of the

882
00:40:21,460 --> 00:40:23,230
advice you're read out there such as

883
00:40:23,230 --> 00:40:28,059
this person on the forum hacker news who

884
00:40:28,059 --> 00:40:30,400
claimed that the best way to become an

885
00:40:30,400 --> 00:40:34,869
m/l engineer is to learn all of math and

886
00:40:34,869 --> 00:40:38,140
C and C++ learn parallel programming

887
00:40:38,140 --> 00:40:40,240
learn ml algorithms implement them

888
00:40:40,240 --> 00:40:43,329
yourself using plain C and finally start

889
00:40:43,329 --> 00:40:46,119
doing ml so we would say if you want to

890
00:40:46,119 --> 00:40:47,980
become an effective practitioner do

891
00:40:47,980 --> 00:40:51,400
exactly the opposite of of this yes

892
00:40:51,400 --> 00:40:53,500
Rachel yeah I'm just highlighting that

893
00:40:53,500 --> 00:40:56,859
this is we think this is bad advice and

894
00:40:56,859 --> 00:40:58,569
this can be very discouraging for a lot

895
00:40:58,569 --> 00:41:00,549
of people to come across this yeah yeah

896
00:41:00,549 --> 00:41:03,010
it's it's it's it's you know we now have

897
00:41:03,010 --> 00:41:04,450
thousands and more tens of thousands of

898
00:41:04,450 --> 00:41:06,960
people that have done this course and

899
00:41:06,960 --> 00:41:11,170
have lots and lots of examples of people

900
00:41:11,170 --> 00:41:14,640
who are now running research labs or

901
00:41:14,640 --> 00:41:18,520
Google brain residence or you know have

902
00:41:18,520 --> 00:41:20,589
created patents based on deep learning

903
00:41:20,589 --> 00:41:22,270
and so forth who have done it by doing

904
00:41:22,270 --> 00:41:24,460
this course so the top-down approach

905
00:41:24,460 --> 00:41:27,859
works super well

906
00:41:27,859 --> 00:41:30,529
now one thing to mention is like we've

907
00:41:30,529 --> 00:41:32,749
we've now already learned how you can

908
00:41:32,749 --> 00:41:34,999
actually train a world-class image

909
00:41:34,999 --> 00:41:38,450
classifier in 17 seconds I should

910
00:41:38,450 --> 00:41:40,759
mention by the way the first time you

911
00:41:40,759 --> 00:41:43,160
run that code there are two things it

912
00:41:43,160 --> 00:41:45,170
has to do that take more than 17 seconds

913
00:41:45,170 --> 00:41:48,680
one is that it downloads a pre trained

914
00:41:48,680 --> 00:41:50,930
model from the internet so you'll see

915
00:41:50,930 --> 00:41:52,249
the first time you run it it'll say

916
00:41:52,249 --> 00:41:55,009
downloading model so that takes a minute

917
00:41:55,009 --> 00:41:58,700
or two also the first time you run it it

918
00:41:58,700 --> 00:42:01,369
pre computes and caches some of the

919
00:42:01,369 --> 00:42:02,869
intermediate information that it needs

920
00:42:02,869 --> 00:42:05,239
and that takes about a minute and a half

921
00:42:05,239 --> 00:42:08,210
as well so if the first time you run it

922
00:42:08,210 --> 00:42:11,420
it takes three or four minutes to

923
00:42:11,420 --> 00:42:13,579
download and pre compute stuff that's

924
00:42:13,579 --> 00:42:15,319
normal if you run it again you should

925
00:42:15,319 --> 00:42:20,809
find it takes 20 seconds or so so image

926
00:42:20,809 --> 00:42:21,589
classifiers

927
00:42:21,589 --> 00:42:24,950
you know you may not feel like you need

928
00:42:24,950 --> 00:42:27,890
to recognize cats versus dogs very often

929
00:42:27,890 --> 00:42:29,809
on a computer you can probably do it

930
00:42:29,809 --> 00:42:31,460
yourself pretty well but what's

931
00:42:31,460 --> 00:42:33,339
interestingly interesting is that these

932
00:42:33,339 --> 00:42:35,809
image classification algorithms are

933
00:42:35,809 --> 00:42:37,849
really useful for lots and lots of

934
00:42:37,849 --> 00:42:43,430
things for example alphago which became

935
00:42:43,430 --> 00:42:46,309
which beat the go world champion the way

936
00:42:46,309 --> 00:42:50,239
it worked was to use something at its

937
00:42:50,239 --> 00:42:53,210
heart that looked almost exactly like

938
00:42:53,210 --> 00:42:55,910
our dogs vs. cats image classification

939
00:42:55,910 --> 00:42:58,849
algorithm it looked at thousands and

940
00:42:58,849 --> 00:43:02,690
thousands of go boards and at for each

941
00:43:02,690 --> 00:43:04,279
one there was a label saying whether

942
00:43:04,279 --> 00:43:06,619
that go board ended up being the winning

943
00:43:06,619 --> 00:43:10,599
or the losing player and so it learnt

944
00:43:10,599 --> 00:43:13,700
basically an image classification that

945
00:43:13,700 --> 00:43:15,140
was able to look at a go board and

946
00:43:15,140 --> 00:43:16,339
figure out whether it was a good group

947
00:43:16,339 --> 00:43:19,160
or a bad code board and that's really

948
00:43:19,160 --> 00:43:22,789
the key most important step in playing

949
00:43:22,789 --> 00:43:25,489
Gowell is to know which which move is

950
00:43:25,489 --> 00:43:29,569
better another example is one of our

951
00:43:29,569 --> 00:43:33,890
earlier students who actually got a

952
00:43:33,890 --> 00:43:36,650
couple of patterns for this work looked

953
00:43:36,650 --> 00:43:40,880
at anti-fraud he had lots of

954
00:43:40,880 --> 00:43:42,859
all of his customers mouths movements

955
00:43:42,859 --> 00:43:45,880
because they they provided kind of these

956
00:43:45,880 --> 00:43:48,470
user tracking software to help avoid

957
00:43:48,470 --> 00:43:52,839
fraud and so he took the the mouse paths

958
00:43:52,839 --> 00:43:56,210
basically of the users on his customers

959
00:43:56,210 --> 00:43:59,019
websites turn them into pictures of

960
00:43:59,019 --> 00:44:01,400
where their mouse moved and how quickly

961
00:44:01,400 --> 00:44:04,819
it moved and then built a image

962
00:44:04,819 --> 00:44:07,549
classifier that took those images as

963
00:44:07,549 --> 00:44:10,460
input and as output it was was that a

964
00:44:10,460 --> 00:44:13,490
fraudulent transaction or not and turned

965
00:44:13,490 --> 00:44:15,470
out to go you know really great results

966
00:44:15,470 --> 00:44:19,630
for his company so image classifiers are

967
00:44:19,630 --> 00:44:22,670
like much more flexible than you might

968
00:44:22,670 --> 00:44:23,740
imagine

969
00:44:23,740 --> 00:44:28,460
so so this is how you know some of the

970
00:44:28,460 --> 00:44:29,619
ways you can use deep learning

971
00:44:29,619 --> 00:44:33,339
specifically for image recognition and

972
00:44:33,339 --> 00:44:37,609
it's worth understanding that deep

973
00:44:37,609 --> 00:44:41,000
learning is not you know just a word

974
00:44:41,000 --> 00:44:42,559
that means the same thing as machine

975
00:44:42,559 --> 00:44:44,299
learning right like what is it that

976
00:44:44,299 --> 00:44:45,529
we're actually doing here when we're

977
00:44:45,529 --> 00:44:48,410
doing deep learning instead deep

978
00:44:48,410 --> 00:44:50,390
learning is a kind of machine learning

979
00:44:50,390 --> 00:44:53,509
so machine learning was invented by this

980
00:44:53,509 --> 00:44:55,369
guy Arthur Samuels who was pretty

981
00:44:55,369 --> 00:44:55,910
amazing

982
00:44:55,910 --> 00:44:58,490
in the late 50s he got this IBM

983
00:44:58,490 --> 00:45:01,880
mainframe to play checkers better than

984
00:45:01,880 --> 00:45:04,880
he can and the way he did it was he

985
00:45:04,880 --> 00:45:07,359
invented machine learning he got the

986
00:45:07,359 --> 00:45:10,940
mainframe to play against itself lots of

987
00:45:10,940 --> 00:45:13,130
times and figure out which kinds of

988
00:45:13,130 --> 00:45:14,839
things led to victories and which kinds

989
00:45:14,839 --> 00:45:18,049
of things didn't and use that to kind of

990
00:45:18,049 --> 00:45:20,750
almost write its own program and Arthur

991
00:45:20,750 --> 00:45:23,059
Arthur Samuels actually said in 1962

992
00:45:23,059 --> 00:45:25,309
that he thought that one day the vast

993
00:45:25,309 --> 00:45:27,829
majority of computer software would be

994
00:45:27,829 --> 00:45:30,230
written using this machine learning

995
00:45:30,230 --> 00:45:32,720
approach rather than written by hand by

996
00:45:32,720 --> 00:45:34,900
writing the loops and so forth by hand

997
00:45:34,900 --> 00:45:38,569
so I guess that hasn't happened yet but

998
00:45:38,569 --> 00:45:40,700
it seems to be in the process of

999
00:45:40,700 --> 00:45:43,519
happening I think one of the reasons it

1000
00:45:43,519 --> 00:45:45,589
didn't happen for a long time is because

1001
00:45:45,589 --> 00:45:47,480
traditional machine learning actually

1002
00:45:47,480 --> 00:45:51,350
was very difficult and very

1003
00:45:51,350 --> 00:45:54,110
knowledge and time intensive so for

1004
00:45:54,110 --> 00:45:55,550
example here's something called the

1005
00:45:55,550 --> 00:45:59,060
computational pathologist or C path from

1006
00:45:59,060 --> 00:46:01,400
backwater and II back and II back back

1007
00:46:01,400 --> 00:46:04,250
when he was at Stanford he's now moved

1008
00:46:04,250 --> 00:46:07,490
on to somewhere on the East Coast no

1009
00:46:07,490 --> 00:46:11,450
Harvard I think and what he did was he

1010
00:46:11,450 --> 00:46:13,730
took these pathology slides of breast

1011
00:46:13,730 --> 00:46:18,770
cancer biopsies right and he worked with

1012
00:46:18,770 --> 00:46:21,440
lots of pathologists to come up with

1013
00:46:21,440 --> 00:46:24,770
ideas about what kinds of patterns or

1014
00:46:24,770 --> 00:46:27,830
features might be associated with so

1015
00:46:27,830 --> 00:46:31,550
long-term survival versus versus dying

1016
00:46:31,550 --> 00:46:34,190
quickly basically and so he came up with

1017
00:46:34,190 --> 00:46:36,320
these ideas like well they came up with

1018
00:46:36,320 --> 00:46:37,760
these ideas like relationship between

1019
00:46:37,760 --> 00:46:39,580
epithelial nuclear neighbors

1020
00:46:39,580 --> 00:46:41,540
relationship between epithelial and

1021
00:46:41,540 --> 00:46:44,060
stromal objects and so forth and so they

1022
00:46:44,060 --> 00:46:45,500
came up with all of these ideas of

1023
00:46:45,500 --> 00:46:47,030
features these are just a few of the

1024
00:46:47,030 --> 00:46:49,430
hundreds that they thought of and then

1025
00:46:49,430 --> 00:46:53,200
lots of smart computer programmers wrote

1026
00:46:53,200 --> 00:46:56,270
specialist algorithms to to calculate

1027
00:46:56,270 --> 00:46:58,790
all these different features and then

1028
00:46:58,790 --> 00:47:01,910
those those features were passed into a

1029
00:47:01,910 --> 00:47:04,940
logistic regression to predict survival

1030
00:47:04,940 --> 00:47:07,790
and it ended up working very well and it

1031
00:47:07,790 --> 00:47:10,100
ended up that the survival predictions

1032
00:47:10,100 --> 00:47:13,700
were more accurate than pathologists own

1033
00:47:13,700 --> 00:47:16,610
survival predictions work and so machine

1034
00:47:16,610 --> 00:47:18,350
learning can work really well but the

1035
00:47:18,350 --> 00:47:21,080
point here is that this was a an

1036
00:47:21,080 --> 00:47:23,840
approach that took lots of domain

1037
00:47:23,840 --> 00:47:27,080
experts and computer experts many years

1038
00:47:27,080 --> 00:47:30,350
of work to actually to build this thing

1039
00:47:30,350 --> 00:47:37,510
right so we really want something

1040
00:47:37,510 --> 00:47:41,390
something better and so specifically I'm

1041
00:47:41,390 --> 00:47:43,550
going to show you something which rather

1042
00:47:43,550 --> 00:47:47,300
than being a very specific function with

1043
00:47:47,300 --> 00:47:51,530
all this very domain-specific feature

1044
00:47:51,530 --> 00:47:53,960
engineering we're going to try and

1045
00:47:53,960 --> 00:47:56,150
create an infinitely flexible function a

1046
00:47:56,150 --> 00:47:58,330
function that could solve any problem

1047
00:47:58,330 --> 00:48:01,280
right it would solve any problem if only

1048
00:48:01,280 --> 00:48:03,020
you set the parameters of that function

1049
00:48:03,020 --> 00:48:05,120
correctly and so then we need

1050
00:48:05,120 --> 00:48:07,250
or purpose way of setting the parameters

1051
00:48:07,250 --> 00:48:10,190
of that function and we would need that

1052
00:48:10,190 --> 00:48:12,770
to be fast and scalable right now if we

1053
00:48:12,770 --> 00:48:14,000
had something that had these three

1054
00:48:14,000 --> 00:48:16,700
things then you wouldn't need to do this

1055
00:48:16,700 --> 00:48:18,770
incredibly time and domain knowledge

1056
00:48:18,770 --> 00:48:21,530
intensive approach anymore instead we

1057
00:48:21,530 --> 00:48:24,730
can learn all of those things with this

1058
00:48:24,730 --> 00:48:28,430
with this algorithm so as you might have

1059
00:48:28,430 --> 00:48:31,700
guessed the algorithm in question which

1060
00:48:31,700 --> 00:48:33,800
has these three properties is called

1061
00:48:33,800 --> 00:48:36,560
deep learning or it's not an algorithm

1062
00:48:36,560 --> 00:48:38,780
then maybe we will call it a class of

1063
00:48:38,780 --> 00:48:41,690
algorithms let's look at each of these

1064
00:48:41,690 --> 00:48:44,630
three things in turn so the underlying

1065
00:48:44,630 --> 00:48:47,750
function that deep learning uses is

1066
00:48:47,750 --> 00:48:50,960
something called the neural network now

1067
00:48:50,960 --> 00:48:53,270
the neural network we're going to learn

1068
00:48:53,270 --> 00:48:54,920
all about it and implemented ourselves

1069
00:48:54,920 --> 00:48:57,350
from scratch later on in the course but

1070
00:48:57,350 --> 00:48:59,270
for now all you need to know about it is

1071
00:48:59,270 --> 00:49:02,210
that it consists of a number of simple

1072
00:49:02,210 --> 00:49:05,330
linear layers interspersed with a number

1073
00:49:05,330 --> 00:49:10,310
of simple nonlinear layers and when you

1074
00:49:10,310 --> 00:49:12,400
in dispersed these layers in this way

1075
00:49:12,400 --> 00:49:15,950
you get something called the universal

1076
00:49:15,950 --> 00:49:18,410
approximation theorem and the universal

1077
00:49:18,410 --> 00:49:20,510
approximation theorem says that this

1078
00:49:20,510 --> 00:49:24,200
kind of function can solve any given

1079
00:49:24,200 --> 00:49:28,340
problem to arbitrarily close accuracy as

1080
00:49:28,340 --> 00:49:32,900
long as you add enough parameters so

1081
00:49:32,900 --> 00:49:35,600
it's actually provably shown to be an

1082
00:49:35,600 --> 00:49:39,740
infinitely flexible function okay so now

1083
00:49:39,740 --> 00:49:41,000
we need some way to fit the parameters

1084
00:49:41,000 --> 00:49:43,400
so that this infinitely flexible neural

1085
00:49:43,400 --> 00:49:46,570
network solves some specific problem and

1086
00:49:46,570 --> 00:49:49,370
so the way we do that is using a

1087
00:49:49,370 --> 00:49:51,890
technique that probably most of you will

1088
00:49:51,890 --> 00:49:53,390
have come across before at some stage

1089
00:49:53,390 --> 00:49:55,910
called gradient descent and with

1090
00:49:55,910 --> 00:49:58,400
gradient descent we basically say okay

1091
00:49:58,400 --> 00:49:59,930
well for the different parameters we

1092
00:49:59,930 --> 00:50:03,620
have how how good are they at solving my

1093
00:50:03,620 --> 00:50:06,590
problem and let's figure out a slightly

1094
00:50:06,590 --> 00:50:09,290
better set of parameters and a slightly

1095
00:50:09,290 --> 00:50:11,210
better set of parameters and j6v follow

1096
00:50:11,210 --> 00:50:14,180
down the the surface of the loss

1097
00:50:14,180 --> 00:50:15,920
function downwards it's kind of like a

1098
00:50:15,920 --> 00:50:18,530
marble going down

1099
00:50:18,530 --> 00:50:21,440
find the minimum and as you can see here

1100
00:50:21,440 --> 00:50:23,780
depending on where you start you end up

1101
00:50:23,780 --> 00:50:27,980
in different places these things a court

1102
00:50:27,980 --> 00:50:31,190
local minima now interestingly it turns

1103
00:50:31,190 --> 00:50:32,390
out that for neural networks

1104
00:50:32,390 --> 00:50:36,830
particularly in particular there aren't

1105
00:50:36,830 --> 00:50:41,480
actually multiple different local minima

1106
00:50:41,480 --> 00:50:44,060
there's basically just there's basically

1107
00:50:44,060 --> 00:50:46,760
just one right or think of it another

1108
00:50:46,760 --> 00:50:48,110
way there are different parts of the

1109
00:50:48,110 --> 00:50:53,320
space which are all equally good so

1110
00:50:53,320 --> 00:50:56,270
gradient descent therefore turns out to

1111
00:50:56,270 --> 00:50:59,330
be actually an excellent way to solve

1112
00:50:59,330 --> 00:51:02,030
this problem of fitting parameters to

1113
00:51:02,030 --> 00:51:06,560
neural networks the problem is though

1114
00:51:06,560 --> 00:51:08,480
that we need to do it in a reasonable

1115
00:51:08,480 --> 00:51:09,440
amount of time

1116
00:51:09,440 --> 00:51:12,920
and it's really only thanks to GPUs that

1117
00:51:12,920 --> 00:51:16,490
that's become possible so GPUs this

1118
00:51:16,490 --> 00:51:19,040
shows over the last few years

1119
00:51:19,040 --> 00:51:22,790
how many gigaflops per second can you

1120
00:51:22,790 --> 00:51:26,600
get out of a GPU that's the red and

1121
00:51:26,600 --> 00:51:29,840
green versus a CPU that's the blue right

1122
00:51:29,840 --> 00:51:32,390
and this is on a log scale so you can

1123
00:51:32,390 --> 00:51:36,040
see that generally speaking the GPUs are

1124
00:51:36,040 --> 00:51:42,040
about 10 times faster than the CPUs and

1125
00:51:42,040 --> 00:51:44,120
what's really interesting is that

1126
00:51:44,120 --> 00:51:48,260
nowadays not only is the Titan X about

1127
00:51:48,260 --> 00:51:51,890
10 times faster than the e5 to $6.99 CPU

1128
00:51:51,890 --> 00:51:56,390
but the Titan X well actually better one

1129
00:51:56,390 --> 00:51:59,810
to look at would be the GTX 1080i GPU

1130
00:51:59,810 --> 00:52:01,690
costs about 700 bucks

1131
00:52:01,690 --> 00:52:04,670
whereas the CPU which is 10 times slower

1132
00:52:04,670 --> 00:52:10,640
costs over $4,000 so GPUs turn out to be

1133
00:52:10,640 --> 00:52:14,300
able to solve these neural network

1134
00:52:14,300 --> 00:52:17,600
parameter fitting problems incredibly

1135
00:52:17,600 --> 00:52:20,570
quickly and also incredibly cheaply so

1136
00:52:20,570 --> 00:52:24,020
they've been absolutely key in bringing

1137
00:52:24,020 --> 00:52:28,490
these three pieces together then there's

1138
00:52:28,490 --> 00:52:31,280
one more piece which is I mentioned that

1139
00:52:31,280 --> 00:52:32,440
these neural network

1140
00:52:32,440 --> 00:52:34,690
so you can intersperse multiple sets of

1141
00:52:34,690 --> 00:52:40,359
linear and then nonlinear layers in the

1142
00:52:40,359 --> 00:52:42,220
particular example that's drawn here

1143
00:52:42,220 --> 00:52:44,619
there's actually only one what we call

1144
00:52:44,619 --> 00:52:46,599
hidden layer one layer in the middle and

1145
00:52:46,599 --> 00:52:48,520
something that we learned in the last

1146
00:52:48,520 --> 00:52:51,520
few years is that these kinds of neural

1147
00:52:51,520 --> 00:52:53,920
networks although they do support the

1148
00:52:53,920 --> 00:52:55,990
universal approximation theorem they can

1149
00:52:55,990 --> 00:52:58,900
solve any given problem arbitrarily

1150
00:52:58,900 --> 00:53:02,170
closely they require an exponentially

1151
00:53:02,170 --> 00:53:04,990
increasing number of parameters to do so

1152
00:53:04,990 --> 00:53:07,660
so they don't actually solve the fast

1153
00:53:07,660 --> 00:53:09,760
and scalable for even reasonable size

1154
00:53:09,760 --> 00:53:14,230
problems but we've since discovered that

1155
00:53:14,230 --> 00:53:16,770
if you create add multiple hidden layers

1156
00:53:16,770 --> 00:53:20,200
then you get super linear scaling so you

1157
00:53:20,200 --> 00:53:23,700
can add a few more hidden layers to get

1158
00:53:23,700 --> 00:53:27,310
multiplicatively more accuracy 2ma

1159
00:53:27,310 --> 00:53:28,990
duplicative lis more complex problems

1160
00:53:28,990 --> 00:53:32,290
and that is where it becomes called deep

1161
00:53:32,290 --> 00:53:34,359
learning so deep learning means a neural

1162
00:53:34,359 --> 00:53:41,619
network with multiple hidden layers so

1163
00:53:41,619 --> 00:53:43,270
when you put all this together there's

1164
00:53:43,270 --> 00:53:46,680
actually really amazing what happens

1165
00:53:46,680 --> 00:53:49,720
Google started investing in deep

1166
00:53:49,720 --> 00:53:54,160
learning in 2012 they actually hired

1167
00:53:54,160 --> 00:53:56,050
Geoffrey Hinton who's kind of the father

1168
00:53:56,050 --> 00:53:59,200
of deep learning and his top student

1169
00:53:59,200 --> 00:54:02,770
Alex Bogusky and they started trying to

1170
00:54:02,770 --> 00:54:05,440
build a team that team became known as

1171
00:54:05,440 --> 00:54:10,540
Google brain and because things with

1172
00:54:10,540 --> 00:54:13,810
these three properties are so incredibly

1173
00:54:13,810 --> 00:54:16,270
powerful and so incredibly flexible you

1174
00:54:16,270 --> 00:54:18,910
can actually see over time how many

1175
00:54:18,910 --> 00:54:23,980
projects at Google use deep learning my

1176
00:54:23,980 --> 00:54:25,990
graph here only goes up through a bit

1177
00:54:25,990 --> 00:54:28,000
over a year ago but it's I know it's

1178
00:54:28,000 --> 00:54:29,650
been continuing to grow exponentially

1179
00:54:29,650 --> 00:54:32,770
since then as well and so what you see

1180
00:54:32,770 --> 00:54:35,140
now is around Google that deep learning

1181
00:54:35,140 --> 00:54:37,300
is used in like every part of the

1182
00:54:37,300 --> 00:54:40,060
business and so it's really interesting

1183
00:54:40,060 --> 00:54:44,590
to see how the

1184
00:54:44,590 --> 00:54:47,440
this kind of simple idea that we can

1185
00:54:47,440 --> 00:54:50,710
solve machine learning problems using a

1186
00:54:50,710 --> 00:54:53,490
an algorithm that has these properties

1187
00:54:53,490 --> 00:54:56,590
when a big company invests heavily in

1188
00:54:56,590 --> 00:54:59,230
actually making that happen you see this

1189
00:54:59,230 --> 00:55:02,700
incredible growth in how much it's used

1190
00:55:02,700 --> 00:55:06,850
so for example if you use the inbox by

1191
00:55:06,850 --> 00:55:09,730
Google software then when you receive an

1192
00:55:09,730 --> 00:55:14,320
email from somebody it will often tell

1193
00:55:14,320 --> 00:55:17,620
you here are some replies that I could

1194
00:55:17,620 --> 00:55:19,900
send for you and so it's actually using

1195
00:55:19,900 --> 00:55:22,030
deep learning here to read the original

1196
00:55:22,030 --> 00:55:25,660
email and to generate some suggested

1197
00:55:25,660 --> 00:55:28,480
replies and so like this is a really

1198
00:55:28,480 --> 00:55:31,110
great example of the kind of stuff that

1199
00:55:31,110 --> 00:55:35,110
previously just wasn't possible another

1200
00:55:35,110 --> 00:55:37,570
great example would be Microsoft is also

1201
00:55:37,570 --> 00:55:40,000
a little bit more recently invested

1202
00:55:40,000 --> 00:55:42,670
heavily in deep learning and so now you

1203
00:55:42,670 --> 00:55:46,420
can use Skype you can speaking to it in

1204
00:55:46,420 --> 00:55:49,900
English and ask it at the other end to

1205
00:55:49,900 --> 00:55:52,690
translate it in real time to Chinese or

1206
00:55:52,690 --> 00:55:54,700
Spanish and then when they talk back to

1207
00:55:54,700 --> 00:55:57,610
you in Chinese or Spanish Scott will in

1208
00:55:57,610 --> 00:56:00,640
real-time translated the speech in in

1209
00:56:00,640 --> 00:56:03,010
their language into English speech in

1210
00:56:03,010 --> 00:56:05,470
real-time and again this is an example

1211
00:56:05,470 --> 00:56:07,870
of stuff which we can only do

1212
00:56:07,870 --> 00:56:13,030
thanks to deep learning and something is

1213
00:56:13,030 --> 00:56:14,410
really interesting to think about how

1214
00:56:14,410 --> 00:56:17,590
deep learning can be combined with human

1215
00:56:17,590 --> 00:56:20,440
expertise so here's an example of low

1216
00:56:20,440 --> 00:56:22,990
drawing something just sketching it out

1217
00:56:22,990 --> 00:56:25,780
and then using a program called neural

1218
00:56:25,780 --> 00:56:27,130
doodle this is from a couple of years

1219
00:56:27,130 --> 00:56:30,040
ago - then say please take that sketch

1220
00:56:30,040 --> 00:56:33,540
and render it in the style of an artist

1221
00:56:33,540 --> 00:56:35,950
and so here's the picture that have been

1222
00:56:35,950 --> 00:56:38,860
created rendering it as you know

1223
00:56:38,860 --> 00:56:41,500
impressionist painting and I think this

1224
00:56:41,500 --> 00:56:44,620
is a really great example of how you can

1225
00:56:44,620 --> 00:56:47,830
use deep learning to help combine human

1226
00:56:47,830 --> 00:56:53,840
expertise and what computers are good at

1227
00:56:53,840 --> 00:56:58,560
so I a few years ago decided to try this

1228
00:56:58,560 --> 00:57:01,670
myself like what would happen if I took

1229
00:57:01,670 --> 00:57:04,140
think learning and tried to use it to

1230
00:57:04,140 --> 00:57:06,420
solve a really important problem and so

1231
00:57:06,420 --> 00:57:09,270
the problem I picked was diagnosing lung

1232
00:57:09,270 --> 00:57:14,220
cancer it turns out if you can find lung

1233
00:57:14,220 --> 00:57:18,360
nodules earlier there's a 10 times

1234
00:57:18,360 --> 00:57:21,570
higher probability of survival so it's a

1235
00:57:21,570 --> 00:57:24,360
really important problem to solve so I

1236
00:57:24,360 --> 00:57:25,860
got together with three other people

1237
00:57:25,860 --> 00:57:27,830
none of us had any medical background

1238
00:57:27,830 --> 00:57:32,730
and we grabbed a data set of CT scans we

1239
00:57:32,730 --> 00:57:34,520
used to compilation or neural network

1240
00:57:34,520 --> 00:57:37,350
much like the dogs vs. cats one we

1241
00:57:37,350 --> 00:57:38,990
trained at the start of today's lesson

1242
00:57:38,990 --> 00:57:43,970
to try and predict which CT scans had

1243
00:57:43,970 --> 00:57:47,700
malignant tumors in them and we ended up

1244
00:57:47,700 --> 00:57:49,860
after a couple of months with something

1245
00:57:49,860 --> 00:57:52,050
with a much lower false negative rate

1246
00:57:52,050 --> 00:57:53,790
and a much lower false positive rate

1247
00:57:53,790 --> 00:57:56,580
than a panel with four radiologists and

1248
00:57:56,580 --> 00:57:59,730
we went on to build this in a start-up

1249
00:57:59,730 --> 00:58:01,590
in just into a company called analytic

1250
00:58:01,590 --> 00:58:02,970
which has really become pretty

1251
00:58:02,970 --> 00:58:06,660
successful and since that time the idea

1252
00:58:06,660 --> 00:58:08,160
of using deep learning for medical

1253
00:58:08,160 --> 00:58:11,610
imaging has become hugely popular and is

1254
00:58:11,610 --> 00:58:14,880
being used all around the world so what

1255
00:58:14,880 --> 00:58:16,560
I've generally noticed is that you know

1256
00:58:16,560 --> 00:58:22,260
the vast majority of of kind of things

1257
00:58:22,260 --> 00:58:23,580
that people do in the world currently

1258
00:58:23,580 --> 00:58:26,280
aren't using deep learning and then each

1259
00:58:26,280 --> 00:58:28,050
time somebody says oh let's try using

1260
00:58:28,050 --> 00:58:30,180
deep learning to improve performance at

1261
00:58:30,180 --> 00:58:32,640
this thing they nearly always get

1262
00:58:32,640 --> 00:58:34,800
fantastic results and then suddenly

1263
00:58:34,800 --> 00:58:36,810
everybody in that industry starts using

1264
00:58:36,810 --> 00:58:38,820
it as well so there's just lots and lots

1265
00:58:38,820 --> 00:58:41,160
of opportunities here at this particular

1266
00:58:41,160 --> 00:58:44,040
time to use deep learning to help with

1267
00:58:44,040 --> 00:58:46,800
all kinds of different stuff so I've

1268
00:58:46,800 --> 00:58:48,990
jotted down a few ideas here these are

1269
00:58:48,990 --> 00:58:52,620
all things which I know you can use deep

1270
00:58:52,620 --> 00:58:54,840
learning for right now to get good

1271
00:58:54,840 --> 00:58:58,800
results from and you know are things

1272
00:58:58,800 --> 00:59:00,990
which people spend a lot of money on or

1273
00:59:00,990 --> 00:59:02,550
have a lot of you know important

1274
00:59:02,550 --> 00:59:04,920
business opportunities there's lots more

1275
00:59:04,920 --> 00:59:06,180
as well

1276
00:59:06,180 --> 00:59:08,099
that these are some examples of things

1277
00:59:08,099 --> 00:59:09,660
that maybe your company you could think

1278
00:59:09,660 --> 00:59:13,769
about applying deep learning for so

1279
00:59:13,769 --> 00:59:15,660
let's talk about what's actually going

1280
00:59:15,660 --> 00:59:19,079
on what actually happened when we

1281
00:59:19,079 --> 00:59:21,779
trained that deep learning model earlier

1282
00:59:21,779 --> 00:59:24,749
and so as I briefly mentioned the thing

1283
00:59:24,749 --> 00:59:26,130
we created is something called a

1284
00:59:26,130 --> 00:59:29,609
convolutional neural network or CNN and

1285
00:59:29,609 --> 00:59:32,670
the key piece of a convolutional neural

1286
00:59:32,670 --> 00:59:36,869
network is the convolution so here's a

1287
00:59:36,869 --> 00:59:40,230
great example from our website

1288
00:59:40,230 --> 00:59:43,230
I've got the URL up here explained

1289
00:59:43,230 --> 00:59:46,680
visually it's called and the explained

1290
00:59:46,680 --> 00:59:49,680
visually website has an example of a

1291
00:59:49,680 --> 00:59:52,589
convolution kind of in fact this over

1292
00:59:52,589 --> 00:59:54,779
here in the bottom left is a very zoomed

1293
00:59:54,779 --> 00:59:57,989
in picture of somebody's face and over

1294
00:59:57,989 --> 01:00:01,170
here on the right is an example of using

1295
01:00:01,170 --> 01:00:05,279
a convolution on that image you can see

1296
01:00:05,279 --> 01:00:07,619
here this particular thing is obviously

1297
01:00:07,619 --> 01:00:14,640
finding edges the edges of his head

1298
01:00:14,640 --> 01:00:16,970
about top and bottom edges in particular

1299
01:00:16,970 --> 01:00:20,519
now how is it doing that well if we look

1300
01:00:20,519 --> 01:00:22,230
at each of these are all three by three

1301
01:00:22,230 --> 01:00:24,119
areas this is moving over it's taking

1302
01:00:24,119 --> 01:00:26,069
each three by three area of pixels and

1303
01:00:26,069 --> 01:00:29,640
here are the pixel values right for each

1304
01:00:29,640 --> 01:00:32,640
thing in that 3x3 area and it's

1305
01:00:32,640 --> 01:00:35,009
multiplying each one of those 3 by 3

1306
01:00:35,009 --> 01:00:39,920
pixels by each one of these 3 by 3

1307
01:00:39,920 --> 01:00:43,349
kernel values in a convolution this

1308
01:00:43,349 --> 01:00:47,009
specific set of 9 values is called a

1309
01:00:47,009 --> 01:00:48,989
kernel it doesn't have to be 9 it could

1310
01:00:48,989 --> 01:00:52,049
be 4 by 4 or 5 by 5 or 3 by 2 or

1311
01:00:52,049 --> 01:00:54,809
whatever right in this case it's a 3 by

1312
01:00:54,809 --> 01:00:56,430
3 kernel and in fact a deep learning

1313
01:00:56,430 --> 01:01:00,480
nearly all of our kernels are 3 by 3 so

1314
01:01:00,480 --> 01:01:03,809
in this case the kernel is 1 - 1 / - 1 -

1315
01:01:03,809 --> 01:01:11,190
2 - 1 so we take each of the black

1316
01:01:11,190 --> 01:01:13,890
through white pixel values and we

1317
01:01:13,890 --> 01:01:16,019
multiply as you can see each of them by

1318
01:01:16,019 --> 01:01:18,510
the corresponding value in the kernel

1319
01:01:18,510 --> 01:01:22,080
and then we add them all together and so

1320
01:01:22,080 --> 01:01:25,320
if you do that for every 3x3 area you

1321
01:01:25,320 --> 01:01:28,200
end up with the values you see over here

1322
01:01:28,200 --> 01:01:31,920
on the right hand side okay so very low

1323
01:01:31,920 --> 01:01:35,460
values become black very high values

1324
01:01:35,460 --> 01:01:38,700
become white and so you can see when

1325
01:01:38,700 --> 01:01:41,400
we're at an edge where it's black at the

1326
01:01:41,400 --> 01:01:44,490
bottom and white at the top we're

1327
01:01:44,490 --> 01:01:46,440
obviously going to get higher numbers

1328
01:01:46,440 --> 01:01:50,310
over here and vice versa okay so that's

1329
01:01:50,310 --> 01:01:53,400
a convolution so as you can see it is a

1330
01:01:53,400 --> 01:01:56,340
linear operation and so based on that

1331
01:01:56,340 --> 01:01:58,290
definition of a neural net I described

1332
01:01:58,290 --> 01:02:01,080
before this can be a layer in our neural

1333
01:02:01,080 --> 01:02:04,230
network it is a simple linear operation

1334
01:02:04,230 --> 01:02:06,390
and we're going to look much more at

1335
01:02:06,390 --> 01:02:08,310
convolutions later including building a

1336
01:02:08,310 --> 01:02:10,710
little spreadsheet that implements them

1337
01:02:10,710 --> 01:02:11,570
ourselves

1338
01:02:11,570 --> 01:02:13,800
so the next thing we're going to do is

1339
01:02:13,800 --> 01:02:17,670
we're going to add a nonlinear layer so

1340
01:02:17,670 --> 01:02:20,180
a non-linearity as it's called is

1341
01:02:20,180 --> 01:02:24,950
something which takes an input value and

1342
01:02:24,950 --> 01:02:27,270
turns it into some different value in a

1343
01:02:27,270 --> 01:02:29,040
nonlinear way and you can see this

1344
01:02:29,040 --> 01:02:31,700
orange picture here is an example of a

1345
01:02:31,700 --> 01:02:34,470
nonlinear function specifically this is

1346
01:02:34,470 --> 01:02:37,380
something called a sigmoid and so a

1347
01:02:37,380 --> 01:02:39,630
sigmoid is something that has this kind

1348
01:02:39,630 --> 01:02:41,940
of S shape and this is what we used to

1349
01:02:41,940 --> 01:02:44,070
use as our nonlinearities in neural

1350
01:02:44,070 --> 01:02:47,310
networks a lot actually nowadays we're

1351
01:02:47,310 --> 01:02:48,720
nearly entirely use something else

1352
01:02:48,720 --> 01:02:51,560
called a rally or rectified linear unit

1353
01:02:51,560 --> 01:02:56,070
a rail u is simply take any negative

1354
01:02:56,070 --> 01:02:58,350
numbers and replace them with 0 and

1355
01:02:58,350 --> 01:03:01,140
leave any positive numbers as they are

1356
01:03:01,140 --> 01:03:05,060
so in other words in code that would be

1357
01:03:05,060 --> 01:03:11,490
y equals max X comma 0 so max X comma 0

1358
01:03:11,490 --> 01:03:18,589
simply says replace the negatives with 0

1359
01:03:18,589 --> 01:03:21,359
now regardless of whether you use a

1360
01:03:21,359 --> 01:03:24,319
sigmoid or a RAL you or something else

1361
01:03:24,319 --> 01:03:27,359
the key point about taking this

1362
01:03:27,359 --> 01:03:29,970
combination of a linear layer followed

1363
01:03:29,970 --> 01:03:33,000
by a element-wise nonlinear function is

1364
01:03:33,000 --> 01:03:35,730
that it allows us to create arbitrarily

1365
01:03:35,730 --> 01:03:37,829
complex shapes as you see in the bottom

1366
01:03:37,829 --> 01:03:41,790
right and the reason why is that and

1367
01:03:41,790 --> 01:03:43,619
this is all from Michael Nelson's

1368
01:03:43,619 --> 01:03:45,750
neural networks and deep learning com

1369
01:03:45,750 --> 01:03:49,380
really fantastic interactive book as you

1370
01:03:49,380 --> 01:03:52,650
change the values of your linear

1371
01:03:52,650 --> 01:03:55,980
functions it basically allows you to

1372
01:03:55,980 --> 01:03:58,619
kind of like build these arbitrarily

1373
01:03:58,619 --> 01:04:01,380
tall or thin blocks and then combine

1374
01:04:01,380 --> 01:04:03,839
those blocks together and this is

1375
01:04:03,839 --> 01:04:06,299
actually the essence of the universal

1376
01:04:06,299 --> 01:04:08,579
approximation theorem this idea that

1377
01:04:08,579 --> 01:04:11,339
when you have a linear layer feeding

1378
01:04:11,339 --> 01:04:13,319
into a non-linearity you can actually

1379
01:04:13,319 --> 01:04:15,680
create these arbitrarily complex shapes

1380
01:04:15,680 --> 01:04:18,780
so this is the key idea behind why

1381
01:04:18,780 --> 01:04:22,170
neural networks can solve any computable

1382
01:04:22,170 --> 01:04:26,790
problem so then we need a way as we

1383
01:04:26,790 --> 01:04:31,559
described to actually set these

1384
01:04:31,559 --> 01:04:33,510
parameters so it's all very well knowing

1385
01:04:33,510 --> 01:04:34,770
that we can move three a meters around

1386
01:04:34,770 --> 01:04:38,430
manually to try to create different

1387
01:04:38,430 --> 01:04:40,829
shapes but we have some specific shape

1388
01:04:40,829 --> 01:04:43,230
we want how do we get to that shape and

1389
01:04:43,230 --> 01:04:45,750
so as we've discussed earlier the basic

1390
01:04:45,750 --> 01:04:47,760
idea is to use something called gradient

1391
01:04:47,760 --> 01:04:50,549
descent this is an extract from a

1392
01:04:50,549 --> 01:04:52,650
notebook actually one of the first AI

1393
01:04:52,650 --> 01:04:56,790
lessons and it shows actually an example

1394
01:04:56,790 --> 01:05:00,150
of using gradient descent to solve a

1395
01:05:00,150 --> 01:05:03,569
simple linear regression problem but I

1396
01:05:03,569 --> 01:05:06,000
can show you the basic idea let's say

1397
01:05:06,000 --> 01:05:11,309
you were just you had a simple quadratic

1398
01:05:11,309 --> 01:05:15,510
all right and so you are trying to find

1399
01:05:15,510 --> 01:05:19,650
the minimum of this quadratic and so in

1400
01:05:19,650 --> 01:05:22,079
order to find the minimum you start out

1401
01:05:22,079 --> 01:05:24,599
by randomly picking some point all right

1402
01:05:24,599 --> 01:05:26,880
so we say okay let's pick let's pick

1403
01:05:26,880 --> 01:05:28,770
here and so you go up there and you

1404
01:05:28,770 --> 01:05:30,790
calculate the value of your quadratic

1405
01:05:30,790 --> 01:05:33,880
at that point so what you now want to do

1406
01:05:33,880 --> 01:05:35,920
is try to find a slightly better point

1407
01:05:35,920 --> 01:05:39,010
so what you could do is you can move a

1408
01:05:39,010 --> 01:05:42,130
little bit to the left and a little bit

1409
01:05:42,130 --> 01:05:44,560
to the right to find out which direction

1410
01:05:44,560 --> 01:05:47,140
is down and what you'll find out is that

1411
01:05:47,140 --> 01:05:48,610
moving a little bit to the left

1412
01:05:48,610 --> 01:05:51,190
decreases the value of the function so

1413
01:05:51,190 --> 01:05:53,350
that looks good right and so in other

1414
01:05:53,350 --> 01:05:56,890
words we're calculating the derivative

1415
01:05:56,890 --> 01:06:02,590
there's a function at that point right

1416
01:06:02,590 --> 01:06:05,080
so that tells you which way is down it's

1417
01:06:05,080 --> 01:06:07,390
the gradient and so now that we know

1418
01:06:07,390 --> 01:06:09,850
that going to the left is down we can

1419
01:06:09,850 --> 01:06:13,330
take a small step in that direction

1420
01:06:13,330 --> 01:06:17,290
to create a new point and then we can

1421
01:06:17,290 --> 01:06:18,880
repeat the process and say okay which

1422
01:06:18,880 --> 01:06:20,770
way is down now and we can now take

1423
01:06:20,770 --> 01:06:23,290
another step and another step and

1424
01:06:23,290 --> 01:06:25,630
another step another step another step

1425
01:06:25,630 --> 01:06:28,510
okay and each time we're getting closer

1426
01:06:28,510 --> 01:06:31,570
and closer so the basic approach here is

1427
01:06:31,570 --> 01:06:34,480
to say okay we start we're at some point

1428
01:06:34,480 --> 01:06:37,180
we've got some value X which is our

1429
01:06:37,180 --> 01:06:40,870
current guess right that's at time step

1430
01:06:40,870 --> 01:06:44,110
n so then our new guest at time step n

1431
01:06:44,110 --> 01:06:47,170
plus 1 is just equal to our previous

1432
01:06:47,170 --> 01:06:56,410
guess plus the derivative

1433
01:06:56,410 --> 01:07:01,870
right times some small number because we

1434
01:07:01,870 --> 01:07:04,540
want to take a small step we need to

1435
01:07:04,540 --> 01:07:06,340
pick a small number because if we picked

1436
01:07:06,340 --> 01:07:09,400
a big number right then we say okay we

1437
01:07:09,400 --> 01:07:11,140
know we want to go to the left let's

1438
01:07:11,140 --> 01:07:13,330
jump a big long way to the left we could

1439
01:07:13,330 --> 01:07:16,180
go all the way over here and we actually

1440
01:07:16,180 --> 01:07:18,280
end up worse all right and then we do it

1441
01:07:18,280 --> 01:07:21,400
again now we're even worse again right

1442
01:07:21,400 --> 01:07:27,910
so if you have too high a step size you

1443
01:07:27,910 --> 01:07:29,710
can actually end up with divergence

1444
01:07:29,710 --> 01:07:32,620
rather than convergence so this number

1445
01:07:32,620 --> 01:07:33,970
here we're going to be talking about it

1446
01:07:33,970 --> 01:07:35,530
a lot during this course and we're going

1447
01:07:35,530 --> 01:07:37,060
to be writing all this stuff out in code

1448
01:07:37,060 --> 01:07:39,010
from scratch ourselves but this number

1449
01:07:39,010 --> 01:07:48,750
here is called the learning rate okay so

1450
01:07:48,750 --> 01:07:52,060
you can see here this is an example of

1451
01:07:52,060 --> 01:07:54,250
basically starting out with some random

1452
01:07:54,250 --> 01:07:56,800
line and then using gradient descent to

1453
01:07:56,800 --> 01:07:58,480
gradually make the line better and

1454
01:07:58,480 --> 01:07:59,460
better and better

1455
01:07:59,460 --> 01:08:03,100
so what happens when you combine these

1456
01:08:03,100 --> 01:08:05,830
ideas right the convolution the

1457
01:08:05,830 --> 01:08:07,960
non-linearity and gradient descent

1458
01:08:07,960 --> 01:08:10,360
because they're all tiny small simple

1459
01:08:10,360 --> 01:08:11,890
little things it doesn't sound that

1460
01:08:11,890 --> 01:08:16,000
exciting but if you have enough of these

1461
01:08:16,000 --> 01:08:19,300
kernels right with enough layers

1462
01:08:19,300 --> 01:08:22,480
something really interesting happens and

1463
01:08:22,480 --> 01:08:29,400
we can actually draw them so here's the

1464
01:08:29,400 --> 01:08:32,440
so this is a really interesting paper by

1465
01:08:32,440 --> 01:08:34,960
Matt Siler and Rob Fergus and what they

1466
01:08:34,960 --> 01:08:38,170
did a few years ago was they figured out

1467
01:08:38,170 --> 01:08:40,720
how to basically draw a picture of what

1468
01:08:40,720 --> 01:08:42,460
each layer in a deep learning net

1469
01:08:42,460 --> 01:08:46,780
Network learned and so they showed that

1470
01:08:46,780 --> 01:08:49,780
layer one of the network here are nine

1471
01:08:49,780 --> 01:08:52,210
examples of convolutional filters from

1472
01:08:52,210 --> 01:08:55,360
layer one of a trained network and they

1473
01:08:55,360 --> 01:08:56,860
found that some of the filters kind of

1474
01:08:56,860 --> 01:08:59,770
learnt these diagonal lines or simple

1475
01:08:59,770 --> 01:09:01,930
dat or grid patterns some of them learnt

1476
01:09:01,930 --> 01:09:04,660
these simple gradients right and so for

1477
01:09:04,660 --> 01:09:07,300
each of these filters they show nine

1478
01:09:07,300 --> 01:09:09,560
examples of little pieces

1479
01:09:09,560 --> 01:09:12,950
of actual photos which activate that

1480
01:09:12,950 --> 01:09:15,950
filter quite highly right so you can see

1481
01:09:15,950 --> 01:09:18,620
layer one these learnt or ever these

1482
01:09:18,620 --> 01:09:20,810
these are learnt using gradient descent

1483
01:09:20,810 --> 01:09:23,180
these filters were not programmed they

1484
01:09:23,180 --> 01:09:26,390
will learnt using gradient descent so in

1485
01:09:26,390 --> 01:09:31,960
other words we were learning these nine

1486
01:09:31,960 --> 01:09:39,560
numbers so layer two then was going to

1487
01:09:39,560 --> 01:09:43,760
take these as inputs and combine them

1488
01:09:43,760 --> 01:09:46,850
together and so layer two had you know

1489
01:09:46,850 --> 01:09:49,490
this is like my in kind of attempts to

1490
01:09:49,490 --> 01:09:51,650
draw one of the examples of the filters

1491
01:09:51,650 --> 01:09:54,140
in layer two they're pretty hard to draw

1492
01:09:54,140 --> 01:09:56,570
but what you can do is say if the each

1493
01:09:56,570 --> 01:09:59,300
filter what are examples of little bits

1494
01:09:59,300 --> 01:10:01,460
of images that activated them and you

1495
01:10:01,460 --> 01:10:04,160
can see by layer two we've got basically

1496
01:10:04,160 --> 01:10:06,410
something that's being activated nearly

1497
01:10:06,410 --> 01:10:09,200
entirely by little bits of sunset some

1498
01:10:09,200 --> 01:10:10,810
things that's being activated by

1499
01:10:10,810 --> 01:10:14,030
circular objects something that's being

1500
01:10:14,030 --> 01:10:17,540
activated by repeating horizontal lines

1501
01:10:17,540 --> 01:10:19,490
something that's being activated by

1502
01:10:19,490 --> 01:10:22,250
corners so you can see how we're

1503
01:10:22,250 --> 01:10:24,440
basically combining layer one features

1504
01:10:24,440 --> 01:10:27,620
together so if we combine those features

1505
01:10:27,620 --> 01:10:29,440
together and again these are all

1506
01:10:29,440 --> 01:10:31,610
convolutional filters won't through

1507
01:10:31,610 --> 01:10:34,670
gradient descent by the third layer it's

1508
01:10:34,670 --> 01:10:37,400
actually learn to recognize the presence

1509
01:10:37,400 --> 01:10:40,580
of text another filter has learnt to

1510
01:10:40,580 --> 01:10:43,580
recognize the presence of petals another

1511
01:10:43,580 --> 01:10:45,170
filter has learnt to recognize the

1512
01:10:45,170 --> 01:10:48,410
presence of human faces right so just

1513
01:10:48,410 --> 01:10:50,180
three layers is enough to get some

1514
01:10:50,180 --> 01:10:53,900
pretty rich behavior so but by the time

1515
01:10:53,900 --> 01:10:56,120
we get to layer five we've got something

1516
01:10:56,120 --> 01:10:58,430
that can recognize the eyeballs of

1517
01:10:58,430 --> 01:11:01,220
insects and birds and something that can

1518
01:11:01,220 --> 01:11:05,570
recognize unicycle wheels alright so so

1519
01:11:05,570 --> 01:11:07,730
this is kind of where we start with

1520
01:11:07,730 --> 01:11:12,650
something incredibly simple right but if

1521
01:11:12,650 --> 01:11:15,530
we use it as a big enough scale thanks

1522
01:11:15,530 --> 01:11:17,210
to the universal approximation theorem

1523
01:11:17,210 --> 01:11:20,420
and the use of multiple hidden layers in

1524
01:11:20,420 --> 01:11:23,090
deep learning we actually get the

1525
01:11:23,090 --> 01:11:28,040
very very rich capabilities so that is

1526
01:11:28,040 --> 01:11:32,260
what we used when we actually trained

1527
01:11:32,260 --> 01:11:40,750
our little dog vs cat recognizer okay so

1528
01:11:40,750 --> 01:11:44,360
let's talk more about this dog vs cat

1529
01:11:44,360 --> 01:11:45,200
recognizer

1530
01:11:45,200 --> 01:11:48,050
so we've learnt the idea of like we can

1531
01:11:48,050 --> 01:11:49,400
look at the pictures that come out of

1532
01:11:49,400 --> 01:11:50,900
the other end to see what the model is

1533
01:11:50,900 --> 01:11:53,690
classifying well like as I find badly or

1534
01:11:53,690 --> 01:11:56,960
which ones it's unsure about but let's

1535
01:11:56,960 --> 01:11:59,570
talk about like this key thing I

1536
01:11:59,570 --> 01:12:01,700
mentioned which is the learning rate so

1537
01:12:01,700 --> 01:12:03,680
I mentioned we have to set this thing I

1538
01:12:03,680 --> 01:12:05,630
just caught it L before the learning

1539
01:12:05,630 --> 01:12:07,520
rate and you might have noticed there's

1540
01:12:07,520 --> 01:12:09,530
a couple of numbers these kind of magic

1541
01:12:09,530 --> 01:12:13,190
numbers here the first one is the

1542
01:12:13,190 --> 01:12:15,620
learning rate right so this number is

1543
01:12:15,620 --> 01:12:17,180
how much do you want to multiply the

1544
01:12:17,180 --> 01:12:19,540
gradient by when you're taking each step

1545
01:12:19,540 --> 01:12:24,230
in your gradient descent we already

1546
01:12:24,230 --> 01:12:25,790
talked about why you wouldn't want it to

1547
01:12:25,790 --> 01:12:29,720
be too high right but probably also it's

1548
01:12:29,720 --> 01:12:31,040
obvious to see why you wouldn't want it

1549
01:12:31,040 --> 01:12:35,330
to be too low if you had it too low you

1550
01:12:35,330 --> 01:12:37,670
would take like a little step and you'd

1551
01:12:37,670 --> 01:12:39,110
be a little bit closer and little bits

1552
01:12:39,110 --> 01:12:41,000
too little step little step and it would

1553
01:12:41,000 --> 01:12:43,730
take lots and lots and lots of steps and

1554
01:12:43,730 --> 01:12:46,520
it would take too long so setting this

1555
01:12:46,520 --> 01:12:49,190
number well is actually really important

1556
01:12:49,190 --> 01:12:52,460
and for the longest time this was

1557
01:12:52,460 --> 01:12:55,460
driving deep learning researchers crazy

1558
01:12:55,460 --> 01:12:59,000
because they didn't really know a good

1559
01:12:59,000 --> 01:13:02,450
way to set this reliably so the good

1560
01:13:02,450 --> 01:13:07,060
news is last year a researcher came up

1561
01:13:07,060 --> 01:13:10,340
with an approach to quite reliably set

1562
01:13:10,340 --> 01:13:13,430
the learning rate unfortunately almost

1563
01:13:13,430 --> 01:13:16,550
nobody noticed so almost no deep

1564
01:13:16,550 --> 01:13:18,080
learning researchers I know about

1565
01:13:18,080 --> 01:13:21,140
actually are aware of this approach but

1566
01:13:21,140 --> 01:13:22,910
it's incredibly successful and it's

1567
01:13:22,910 --> 01:13:25,010
incredibly simple and I'll show you the

1568
01:13:25,010 --> 01:13:27,740
idea right it's built into the fast AI

1569
01:13:27,740 --> 01:13:29,990
library as something called @lr find or

1570
01:13:29,990 --> 01:13:32,480
the learning rate finder and it comes

1571
01:13:32,480 --> 01:13:34,820
from this paper I was actually 2015

1572
01:13:34,820 --> 01:13:36,150
paper sorry

1573
01:13:36,150 --> 01:13:37,830
cyclic or learning rates for training

1574
01:13:37,830 --> 01:13:39,900
neural networks by a terrific researcher

1575
01:13:39,900 --> 01:13:40,199
called

1576
01:13:40,199 --> 01:13:43,530
Leslie Smith and I'll show you Leslie's

1577
01:13:43,530 --> 01:13:47,630
idea

1578
01:13:47,630 --> 01:13:50,760
so Leslie's ideas started out with the

1579
01:13:50,760 --> 01:13:53,340
same basic idea that we've seen before

1580
01:13:53,340 --> 01:13:55,440
which is if we're going to optimize

1581
01:13:55,440 --> 01:13:58,980
something pick some random point take

1582
01:13:58,980 --> 01:14:02,159
its gradient all right and then

1583
01:14:02,159 --> 01:14:05,310
specifically he said take a tiny tiny

1584
01:14:05,310 --> 01:14:10,260
step like tiny step so a learning rate

1585
01:14:10,260 --> 01:14:12,929
of like 10 e next seven all right and

1586
01:14:12,929 --> 01:14:15,480
then do it again again but each time

1587
01:14:15,480 --> 01:14:18,120
increase the learning rate like double

1588
01:14:18,120 --> 01:14:21,510
it so then we try like to wean X 7 14 X

1589
01:14:21,510 --> 01:14:26,310
7 18 X 7 10 in X 6 right and so

1590
01:14:26,310 --> 01:14:30,480
gradually your steps are getting bigger

1591
01:14:30,480 --> 01:14:35,190
and bigger right and so you can see

1592
01:14:35,190 --> 01:14:36,949
what's going to happen it's gonna like

1593
01:14:36,949 --> 01:14:39,630
start doing almost nothing right and

1594
01:14:39,630 --> 01:14:41,460
it's going to then suddenly the loss

1595
01:14:41,460 --> 01:14:43,440
function is going to improve very

1596
01:14:43,440 --> 01:14:45,989
quickly right but then it's going to

1597
01:14:45,989 --> 01:14:50,010
step even further again and then even

1598
01:14:50,010 --> 01:14:53,790
further again all right

1599
01:14:53,790 --> 01:14:55,949
let's draw the rest of that line to be

1600
01:14:55,949 --> 01:15:01,500
clear all right and so suddenly it's

1601
01:15:01,500 --> 01:15:02,969
then going to shoot off and get much

1602
01:15:02,969 --> 01:15:09,060
worse right so the idea then is to go

1603
01:15:09,060 --> 01:15:17,460
back and say okay at what point did we

1604
01:15:17,460 --> 01:15:26,600
see like the best improvement so here

1605
01:15:26,600 --> 01:15:29,610
we've got our best improvement right and

1606
01:15:29,610 --> 01:15:32,000
so I would say ok let's use that

1607
01:15:32,000 --> 01:15:35,219
learning rate right so in other words if

1608
01:15:35,219 --> 01:15:41,040
we were to plot the learning rate over

1609
01:15:41,040 --> 01:15:42,590
time

1610
01:15:42,590 --> 01:15:48,239
it was increasing like so alright and so

1611
01:15:48,239 --> 01:15:50,159
what we then want to do is we want to

1612
01:15:50,159 --> 01:15:54,420
plot the learning rate against the loss

1613
01:15:54,420 --> 01:15:56,909
right so when I say the loss I basically

1614
01:15:56,909 --> 01:15:59,460
mean like how accurate is the model how

1615
01:15:59,460 --> 01:16:02,010
close in this case the loss would be how

1616
01:16:02,010 --> 01:16:05,119
far away is the predictive prediction

1617
01:16:05,119 --> 01:16:09,659
from the from the goal okay and so if we

1618
01:16:09,659 --> 01:16:11,400
plotted the learning rate against the

1619
01:16:11,400 --> 01:16:14,280
loss we'd say like okay initially it

1620
01:16:14,280 --> 01:16:17,369
didn't do very much right for small

1621
01:16:17,369 --> 01:16:18,989
learning rates and then it suddenly

1622
01:16:18,989 --> 01:16:22,020
improved a lot and then it suddenly got

1623
01:16:22,020 --> 01:16:25,380
a lot worse so that's the basic idea and

1624
01:16:25,380 --> 01:16:28,110
so we'd be looking for the point where

1625
01:16:28,110 --> 01:16:31,530
this graph is dropping quickly right

1626
01:16:31,530 --> 01:16:33,030
we're not looking for its minimum point

1627
01:16:33,030 --> 01:16:34,619
we're not saying like where was it the

1628
01:16:34,619 --> 01:16:36,300
lowest because that could actually be

1629
01:16:36,300 --> 01:16:38,460
the point where it's just jumped too far

1630
01:16:38,460 --> 01:16:40,650
we want at what point was it dropping

1631
01:16:40,650 --> 01:16:48,900
the fastest so if you go so if you

1632
01:16:48,900 --> 01:16:50,520
create your learn object in the same way

1633
01:16:50,520 --> 01:16:51,719
that we did before we'll be learning

1634
01:16:51,719 --> 01:16:54,900
more about this these details shortly if

1635
01:16:54,900 --> 01:16:57,780
you then call LR find method on that

1636
01:16:57,780 --> 01:17:00,480
you'll see that it'll start training a

1637
01:17:00,480 --> 01:17:03,600
model like it did before but it'll

1638
01:17:03,600 --> 01:17:06,090
generally stop before it gets to 100%

1639
01:17:06,090 --> 01:17:10,139
okay because if it notices that the loss

1640
01:17:10,139 --> 01:17:14,219
is getting a lot worse then it'll stop

1641
01:17:14,219 --> 01:17:15,929
automatically that's what you can see

1642
01:17:15,929 --> 01:17:18,719
here it stopped at 84% and so then you

1643
01:17:18,719 --> 01:17:21,600
can call one said that gets you the

1644
01:17:21,600 --> 01:17:23,310
learning rate scheduler that's the

1645
01:17:23,310 --> 01:17:25,080
object which actually does this learning

1646
01:17:25,080 --> 01:17:27,210
rate finding and that object has a plot

1647
01:17:27,210 --> 01:17:29,130
learning rate function and so you can

1648
01:17:29,130 --> 01:17:32,100
see here over by iteration you can see

1649
01:17:32,100 --> 01:17:34,020
the learning rate alright so you can see

1650
01:17:34,020 --> 01:17:36,179
each step the learning rate is getting

1651
01:17:36,179 --> 01:17:40,469
bigger and bigger you can do it this way

1652
01:17:40,469 --> 01:17:42,360
we can see it's increasing exponentially

1653
01:17:42,360 --> 01:17:44,460
another way that Leslie Smith the

1654
01:17:44,460 --> 01:17:47,060
researcher suggests is to do it linearly

1655
01:17:47,060 --> 01:17:49,290
so I'm actually currently researching

1656
01:17:49,290 --> 01:17:50,969
with both of these approaches to see

1657
01:17:50,969 --> 01:17:53,400
which works best recently I've been

1658
01:17:53,400 --> 01:17:55,260
mainly using exponential but I'm

1659
01:17:55,260 --> 01:17:56,219
starting to look more

1660
01:17:56,219 --> 01:17:59,070
using Linea at the moment and so if we

1661
01:17:59,070 --> 01:18:02,070
then call shed but plot that does the

1662
01:18:02,070 --> 01:18:04,099
plot that I just described down here

1663
01:18:04,099 --> 01:18:08,429
learning rate versus loss all right and

1664
01:18:08,429 --> 01:18:10,920
so we're looking for the highest

1665
01:18:10,920 --> 01:18:14,099
learning rate we can find where the loss

1666
01:18:14,099 --> 01:18:17,940
is still improving clearly well right

1667
01:18:17,940 --> 01:18:22,019
and so in this case I would say 10 to

1668
01:18:22,019 --> 01:18:23,909
the negative 2x that 10 to the negative

1669
01:18:23,909 --> 01:18:27,150
1 it's not improving right 10 to the

1670
01:18:27,150 --> 01:18:27,780
negative 3

1671
01:18:27,780 --> 01:18:29,400
it is also improving but I'm trying to

1672
01:18:29,400 --> 01:18:32,099
find the highest learning rate I can or

1673
01:18:32,099 --> 01:18:33,599
it's still clearly improving so I'd say

1674
01:18:33,599 --> 01:18:36,210
10 to the negative 2 okay so you might

1675
01:18:36,210 --> 01:18:38,820
have noticed that when we ran our model

1676
01:18:38,820 --> 01:18:44,130
before we had 10 to the negative to 0.01

1677
01:18:44,130 --> 01:18:45,840
so that's why we picked that learning

1678
01:18:45,840 --> 01:18:51,449
rate so there's really only one other

1679
01:18:51,449 --> 01:18:57,059
number that we have to pick and that was

1680
01:18:57,059 --> 01:19:00,239
this number three and so that number

1681
01:19:00,239 --> 01:19:04,739
three controlled how many epochs did we

1682
01:19:04,739 --> 01:19:08,219
run so an epoch means going through our

1683
01:19:08,219 --> 01:19:12,659
entire data set of images and using each

1684
01:19:12,659 --> 01:19:15,869
each time we do a bunch of they're

1685
01:19:15,869 --> 01:19:18,449
called mini batches we grab like 64

1686
01:19:18,449 --> 01:19:21,030
images at a time and use them to try to

1687
01:19:21,030 --> 01:19:22,499
improve the model a little bit using

1688
01:19:22,499 --> 01:19:24,900
gradient descent right and using all of

1689
01:19:24,900 --> 01:19:28,860
the images once is called one epoch and

1690
01:19:28,860 --> 01:19:31,110
so at the end of each epoch we print out

1691
01:19:31,110 --> 01:19:35,070
the accuracy and validation and training

1692
01:19:35,070 --> 01:19:40,380
loss at the end of the epoch so the

1693
01:19:40,380 --> 01:19:43,229
question of how many epochs should be

1694
01:19:43,229 --> 01:19:45,659
run is kind of the one other question

1695
01:19:45,659 --> 01:19:47,489
that you need to answer to run these

1696
01:19:47,489 --> 01:19:50,550
three lines of code and the answer

1697
01:19:50,550 --> 01:19:54,829
really to me is like as many as you like

1698
01:19:54,829 --> 01:19:58,409
what you might find happen is if you run

1699
01:19:58,409 --> 01:20:00,960
it for too long the accuracy you'll

1700
01:20:00,960 --> 01:20:03,300
start getting worse all right and we'll

1701
01:20:03,300 --> 01:20:04,860
learn about that why later it's

1702
01:20:04,860 --> 01:20:06,979
something called overfitting right so

1703
01:20:06,979 --> 01:20:09,449
you can run it for a while run lots of

1704
01:20:09,449 --> 01:20:10,349
epochs

1705
01:20:10,349 --> 01:20:12,780
once you see it getting worse you know

1706
01:20:12,780 --> 01:20:14,820
how many epochs you can run and the

1707
01:20:14,820 --> 01:20:16,199
other thing that might happen is if

1708
01:20:16,199 --> 01:20:18,030
you've got like a really big model or a

1709
01:20:18,030 --> 01:20:19,980
lot lots and lots of data maybe it takes

1710
01:20:19,980 --> 01:20:21,989
so long you don't have time and so you

1711
01:20:21,989 --> 01:20:25,020
just run enough epochs that fit into the

1712
01:20:25,020 --> 01:20:26,969
time you have available so the number of

1713
01:20:26,969 --> 01:20:28,560
epochs you run you know that's a pretty

1714
01:20:28,560 --> 01:20:31,560
easy thing to set so there are the only

1715
01:20:31,560 --> 01:20:33,000
two numbers you're gonna have to see it

1716
01:20:33,000 --> 01:20:37,139
and so the goal this week will be to

1717
01:20:37,139 --> 01:20:40,409
make sure that you can run not only

1718
01:20:40,409 --> 01:20:42,989
these three lines of code on the data

1719
01:20:42,989 --> 01:20:46,980
that I provided but to run it on a set

1720
01:20:46,980 --> 01:20:49,230
of images that you either have on your

1721
01:20:49,230 --> 01:20:52,050
computer or that you get from work well

1722
01:20:52,050 --> 01:20:54,989
that you download from Google and like

1723
01:20:54,989 --> 01:20:57,989
try to get a sense of like which kinds

1724
01:20:57,989 --> 01:21:00,139
of images this is seem to work well for

1725
01:21:00,139 --> 01:21:04,980
which ones doesn't it work well for what

1726
01:21:04,980 --> 01:21:06,780
kind of learning rates do you need for

1727
01:21:06,780 --> 01:21:08,250
different kinds of images how many

1728
01:21:08,250 --> 01:21:10,710
epochs do you need how does the number

1729
01:21:10,710 --> 01:21:12,599
of the learning rate change the accuracy

1730
01:21:12,599 --> 01:21:14,670
you get and so forth like really

1731
01:21:14,670 --> 01:21:18,179
experiment and then you know try to get

1732
01:21:18,179 --> 01:21:20,610
a sense of like what's inside this data

1733
01:21:20,610 --> 01:21:22,619
object you know what are the y-values

1734
01:21:22,619 --> 01:21:25,920
look like what are these places mean if

1735
01:21:25,920 --> 01:21:27,780
you're not familiar with numpy you know

1736
01:21:27,780 --> 01:21:30,270
really practice a lot with numpy so that

1737
01:21:30,270 --> 01:21:31,920
by the time you come back for the next

1738
01:21:31,920 --> 01:21:33,929
lesson

1739
01:21:33,929 --> 01:21:35,219
you know we're going to be digging into

1740
01:21:35,219 --> 01:21:37,349
a lot more detail and so you'll really

1741
01:21:37,349 --> 01:21:40,739
feel ready to do that now one thing

1742
01:21:40,739 --> 01:21:42,360
that's really important to be able to do

1743
01:21:42,360 --> 01:21:44,699
that is that you need to really know how

1744
01:21:44,699 --> 01:21:50,730
to work with numpy the faster a library

1745
01:21:50,730 --> 01:21:52,560
and so forth and so I want to show you

1746
01:21:52,560 --> 01:21:54,840
some tricks in Jupiter notebook to make

1747
01:21:54,840 --> 01:21:58,349
that much easier so one trick to be

1748
01:21:58,349 --> 01:22:00,300
aware of is if you can't quite remember

1749
01:22:00,300 --> 01:22:03,389
how to spell something right so if

1750
01:22:03,389 --> 01:22:06,360
you're not quite sure what the method

1751
01:22:06,360 --> 01:22:08,460
you want is you can always hit tab and

1752
01:22:08,460 --> 01:22:12,000
you'll get a list of methods that start

1753
01:22:12,000 --> 01:22:13,770
with that letter right and so that's a

1754
01:22:13,770 --> 01:22:15,119
quick way to find things

1755
01:22:15,119 --> 01:22:16,949
if you then can't remember what the

1756
01:22:16,949 --> 01:22:21,150
arguments are to a method hit shift tab

1757
01:22:21,150 --> 01:22:24,360
all right so hitting shift tab tells you

1758
01:22:24,360 --> 01:22:27,600
the arguments to the method so shift tab

1759
01:22:27,600 --> 01:22:29,670
is like one of the most helpful things I

1760
01:22:29,670 --> 01:22:36,630
know so let's take in P X P shift tab

1761
01:22:36,630 --> 01:22:39,300
and so now you might be wondering like

1762
01:22:39,300 --> 01:22:41,550
okay well what does this function do and

1763
01:22:41,550 --> 01:22:44,310
how does it work if you press shift tab

1764
01:22:44,310 --> 01:22:47,340
twice then it actually brings up the

1765
01:22:47,340 --> 01:22:49,590
documentation shows you what the

1766
01:22:49,590 --> 01:22:51,990
parameters are and shows you what it

1767
01:22:51,990 --> 01:22:57,000
returns and gives you examples okay if

1768
01:22:57,000 --> 01:23:00,000
you press it three times then it

1769
01:23:00,000 --> 01:23:01,620
actually pops up a whole little separate

1770
01:23:01,620 --> 01:23:03,810
window with that information

1771
01:23:03,810 --> 01:23:07,890
okay so shift tab is super helpful one

1772
01:23:07,890 --> 01:23:09,870
way to grab that window straight away is

1773
01:23:09,870 --> 01:23:12,210
if you just put question mark at the

1774
01:23:12,210 --> 01:23:15,630
start then it just brings up that little

1775
01:23:15,630 --> 01:23:19,890
documentation window now the other thing

1776
01:23:19,890 --> 01:23:22,140
to be aware of is increasingly during

1777
01:23:22,140 --> 01:23:23,610
this course we're going to be looking at

1778
01:23:23,610 --> 01:23:25,950
the actual source code of fast AI itself

1779
01:23:25,950 --> 01:23:28,650
and learning how it's built and why it's

1780
01:23:28,650 --> 01:23:31,080
built that way it's really helpful to

1781
01:23:31,080 --> 01:23:33,980
look at source code in order to you know

1782
01:23:33,980 --> 01:23:36,120
understand what you can do and how you

1783
01:23:36,120 --> 01:23:38,280
can do it so if you for example wanted

1784
01:23:38,280 --> 01:23:40,110
to look at the source code for learned I

1785
01:23:40,110 --> 01:23:42,240
predict you can just put two question

1786
01:23:42,240 --> 01:23:46,890
marks okay and you can see it's popped

1787
01:23:46,890 --> 01:23:49,410
up the source code right and so it's

1788
01:23:49,410 --> 01:23:50,490
just a single line of code

1789
01:23:50,490 --> 01:23:52,800
you're very often find that fast AI

1790
01:23:52,800 --> 01:23:55,710
methods like they they're designed to

1791
01:23:55,710 --> 01:23:59,070
never be more than about half a screen

1792
01:23:59,070 --> 01:24:01,020
full of code and they're often under six

1793
01:24:01,020 --> 01:24:02,880
lines so you can see this case it's

1794
01:24:02,880 --> 01:24:05,220
calling predicted with tags so we could

1795
01:24:05,220 --> 01:24:06,930
then get the source code for that in the

1796
01:24:06,930 --> 01:24:12,060
same way okay and then that's calling a

1797
01:24:12,060 --> 01:24:14,070
function called predicted with tags so

1798
01:24:14,070 --> 01:24:15,420
we could get that documentation for that

1799
01:24:15,420 --> 01:24:17,850
in the same way and then so here yeah

1800
01:24:17,850 --> 01:24:19,650
and then finally that's what it does it

1801
01:24:19,650 --> 01:24:21,660
either rates through a data loader gets

1802
01:24:21,660 --> 01:24:23,310
the predictions and then passes them

1803
01:24:23,310 --> 01:24:27,660
back and so forth okay so question mark

1804
01:24:27,660 --> 01:24:30,090
question mark is how to get source code

1805
01:24:30,090 --> 01:24:32,430
but the single question mark is how to

1806
01:24:32,430 --> 01:24:34,170
get documentation

1807
01:24:34,170 --> 01:24:36,750
and shift-tab is how to bring up

1808
01:24:36,750 --> 01:24:39,630
parameters or press it more times to get

1809
01:24:39,630 --> 01:24:42,810
the docs so that's really helpful

1810
01:24:42,810 --> 01:24:44,580
another really helpful thing to know

1811
01:24:44,580 --> 01:24:47,340
about is how to use Jupiter notebook

1812
01:24:47,340 --> 01:24:49,200
well and the button that you want to

1813
01:24:49,200 --> 01:24:53,190
know is H if you press H it will bring

1814
01:24:53,190 --> 01:24:56,280
up the keyboard shortcuts palette and so

1815
01:24:56,280 --> 01:24:58,620
now you can see exactly what Jupiter

1816
01:24:58,620 --> 01:25:01,080
notebook can do and how to do it I

1817
01:25:01,080 --> 01:25:02,790
personally find all of these functions

1818
01:25:02,790 --> 01:25:05,760
useful so I generally tell students to

1819
01:25:05,760 --> 01:25:07,740
try and learn four or five different

1820
01:25:07,740 --> 01:25:10,680
keyboard shortcuts a day try them out

1821
01:25:10,680 --> 01:25:12,240
see what they do see how they work and

1822
01:25:12,240 --> 01:25:14,520
then you can try practicing in that

1823
01:25:14,520 --> 01:25:17,520
session and one very important thing to

1824
01:25:17,520 --> 01:25:19,950
remember when you're finished with your

1825
01:25:19,950 --> 01:25:21,930
work for the day go back to a paper

1826
01:25:21,930 --> 01:25:23,600
space and click on that little button

1827
01:25:23,600 --> 01:25:26,760
which stops and starts the machine so

1828
01:25:26,760 --> 01:25:27,630
after it's stopped

1829
01:25:27,630 --> 01:25:29,760
you'll see it says connection closed and

1830
01:25:29,760 --> 01:25:31,950
you'll see it's off if you leave it

1831
01:25:31,950 --> 01:25:33,930
running you'll be charged for it

1832
01:25:33,930 --> 01:25:36,300
same thing with Kressel be sure to go to

1833
01:25:36,300 --> 01:25:39,900
your cresol instance and stop it you

1834
01:25:39,900 --> 01:25:42,330
can't just turn your computer off or

1835
01:25:42,330 --> 01:25:44,310
close the browser you actually have to

1836
01:25:44,310 --> 01:25:46,380
stop an increase or or in paper space

1837
01:25:46,380 --> 01:25:48,480
and don't forget to do that or you'll

1838
01:25:48,480 --> 01:25:51,420
end up being charged until you finally

1839
01:25:51,420 --> 01:25:55,350
do remember okay so I think that's all

1840
01:25:55,350 --> 01:25:57,480
the information that you need to get

1841
01:25:57,480 --> 01:25:59,760
started please remember about the

1842
01:25:59,760 --> 01:26:03,150
forum's okay if you get stuck at any

1843
01:26:03,150 --> 01:26:05,760
point check them out but before you do

1844
01:26:05,760 --> 01:26:08,220
make sure you read the information on

1845
01:26:08,220 --> 01:26:10,830
course dot fast at AI for each lesson

1846
01:26:10,830 --> 01:26:14,220
right because that is going to tell you

1847
01:26:14,220 --> 01:26:16,320
about like things that have changed okay

1848
01:26:16,320 --> 01:26:20,390
so if there's been some change - witch

1849
01:26:20,390 --> 01:26:22,710
Cupid a notebook provider we suggest

1850
01:26:22,710 --> 01:26:25,440
using or how to set up paper space or

1851
01:26:25,440 --> 01:26:27,570
anything like that and that'll all be on

1852
01:26:27,570 --> 01:26:30,420
course doc bastard AI okay thanks very

1853
01:26:30,420 --> 01:26:32,310
much for watching and look forward to

1854
01:26:32,310 --> 01:26:35,900
seeing you in the next lesson

