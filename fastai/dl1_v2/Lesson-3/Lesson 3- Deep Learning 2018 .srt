1
00:00:00,149 --> 00:00:01,870
welcome back everybody

2
00:00:01,870 --> 00:00:03,600
[Music]

3
00:00:03,600 --> 00:00:07,649
I'm sure you've noticed but there's been

4
00:00:07,649 --> 00:00:09,599
a lot of cool activity on the forum this

5
00:00:09,599 --> 00:00:10,710
week and one of the things that's been

6
00:00:10,710 --> 00:00:12,780
really great to see is that a lot of you

7
00:00:12,780 --> 00:00:15,630
have started creating really helpful

8
00:00:15,630 --> 00:00:18,240
materials both for your classmates to

9
00:00:18,240 --> 00:00:20,010
better understand stuff and also for you

10
00:00:20,010 --> 00:00:23,279
to better understand stuff by trying to

11
00:00:23,279 --> 00:00:25,920
teach what you've learned I just wanted

12
00:00:25,920 --> 00:00:29,550
to highlight a few i've actually posted

13
00:00:29,550 --> 00:00:32,340
to the wiki thread of a few of these but

14
00:00:32,340 --> 00:00:36,780
there's there's lots more Russian has

15
00:00:36,780 --> 00:00:38,579
posted a whole bunch of nice

16
00:00:38,579 --> 00:00:41,550
introductory tutorials so for example if

17
00:00:41,550 --> 00:00:42,480
you're having any trouble getting

18
00:00:42,480 --> 00:00:46,079
connected with AWS she's got a whole

19
00:00:46,079 --> 00:00:49,649
step-by-step how to go about logging in

20
00:00:49,649 --> 00:00:51,629
and getting everything working which i

21
00:00:51,629 --> 00:00:53,789
think is a really terrific thing and so

22
00:00:53,789 --> 00:00:57,750
it's a kind of thing that if you are

23
00:00:57,750 --> 00:00:59,430
writing some notes for yourself to

24
00:00:59,430 --> 00:01:01,289
remind you how to do it

25
00:01:01,289 --> 00:01:02,940
you may as well post them for others to

26
00:01:02,940 --> 00:01:05,460
do it as well and by using a markdown

27
00:01:05,460 --> 00:01:07,439
file like this and it's actually good

28
00:01:07,439 --> 00:01:08,790
practice if you haven't used github

29
00:01:08,790 --> 00:01:10,549
before if you put it up on github

30
00:01:10,549 --> 00:01:13,950
everybody can now use it or of course

31
00:01:13,950 --> 00:01:17,130
you can just put it in the forum so more

32
00:01:17,130 --> 00:01:20,009
advanced a thing that Reshma wrote up

33
00:01:20,009 --> 00:01:21,930
about is she noticed that I like using

34
00:01:21,930 --> 00:01:25,140
Tmax which is a handy little thing which

35
00:01:25,140 --> 00:01:30,200
lets me lets me basically have a window

36
00:01:30,200 --> 00:01:33,720
I'll show you so as soon as I log into

37
00:01:33,720 --> 00:01:38,009
my computer if I run Tmax you'll see

38
00:01:38,009 --> 00:01:39,869
that all of my windows pop straight up

39
00:01:39,869 --> 00:01:42,150
basically and I can like continue

40
00:01:42,150 --> 00:01:44,250
running stuff in the background and I

41
00:01:44,250 --> 00:01:46,020
can like I've got them over here and I

42
00:01:46,020 --> 00:01:48,210
can kind of zoom into it or I can move

43
00:01:48,210 --> 00:01:50,610
over to the top which is here so Jupiter

44
00:01:50,610 --> 00:01:53,579
colonel running and so forth so if that

45
00:01:53,579 --> 00:01:54,360
sounds interesting

46
00:01:54,360 --> 00:01:57,479
Reshma has a tutorial here on how you

47
00:01:57,479 --> 00:02:00,509
can use two maps and it's actually got a

48
00:02:00,509 --> 00:02:03,060
whole bunch of stuff in her github so

49
00:02:03,060 --> 00:02:07,110
that's that's really cool I built among

50
00:02:07,110 --> 00:02:10,860
has written a very nice kind of summary

51
00:02:10,860 --> 00:02:13,689
basically of our last lesson

52
00:02:13,689 --> 00:02:17,319
which kind of covers what are the key

53
00:02:17,319 --> 00:02:19,120
things we did and why did we do them so

54
00:02:19,120 --> 00:02:21,700
if you are a kind of

55
00:02:21,700 --> 00:02:23,110
wondering like how does it fit together

56
00:02:23,110 --> 00:02:25,959
I think this is a really helpful summary

57
00:02:25,959 --> 00:02:28,810
like what if those couple of hours look

58
00:02:28,810 --> 00:02:30,430
like if we summarize it all into a page

59
00:02:30,430 --> 00:02:38,470
or two I also really like Pavel has dad

60
00:02:38,470 --> 00:02:40,750
kind of done a deep dive on the learning

61
00:02:40,750 --> 00:02:45,069
rate finder which is a topic that a lot

62
00:02:45,069 --> 00:02:46,330
of you have been interested in learning

63
00:02:46,330 --> 00:02:49,330
more about particularly those of you who

64
00:02:49,330 --> 00:02:50,470
have done deep learning before I

65
00:02:50,470 --> 00:02:52,720
realized that this is like a solution to

66
00:02:52,720 --> 00:02:54,160
a problem that you've been having for a

67
00:02:54,160 --> 00:02:55,989
long time and haven't seen before and so

68
00:02:55,989 --> 00:02:57,880
it's kind of something which hasn't

69
00:02:57,880 --> 00:02:59,500
really been blogged about before so this

70
00:02:59,500 --> 00:03:00,940
is the first I've seen it's logged about

71
00:03:00,940 --> 00:03:04,079
so when I put this on Twitter a link to

72
00:03:04,079 --> 00:03:06,519
pebbles post it's been shared you know

73
00:03:06,519 --> 00:03:08,739
hundreds of times it's been really

74
00:03:08,739 --> 00:03:10,810
really popular and viewed many thousands

75
00:03:10,810 --> 00:03:13,560
of times so that's some great content

76
00:03:13,560 --> 00:03:16,630
radec has posted lots of cool stuff I

77
00:03:16,630 --> 00:03:18,579
really like this practitioners guide to

78
00:03:18,579 --> 00:03:21,190
apply torch which again this is more for

79
00:03:21,190 --> 00:03:23,079
more advanced students but it's like

80
00:03:23,079 --> 00:03:25,420
digging into people who have never used

81
00:03:25,420 --> 00:03:28,650
hi torch before but know a bit about

82
00:03:28,650 --> 00:03:30,940
numerical programming in general and

83
00:03:30,940 --> 00:03:32,260
it's a quick introduction to how high

84
00:03:32,260 --> 00:03:35,560
torch is different and then there's been

85
00:03:35,560 --> 00:03:36,819
some interesting little bits of research

86
00:03:36,819 --> 00:03:38,709
like what's the relationship between

87
00:03:38,709 --> 00:03:40,959
learning rate and batch sites so one of

88
00:03:40,959 --> 00:03:42,639
the students actually asked me this

89
00:03:42,639 --> 00:03:44,620
before class and I said oh well one of

90
00:03:44,620 --> 00:03:46,030
the other students has written an

91
00:03:46,030 --> 00:03:49,930
analysis of exactly that so what he's

92
00:03:49,930 --> 00:03:51,489
done is basically looked through and

93
00:03:51,489 --> 00:03:52,930
tried different batch sizes and

94
00:03:52,930 --> 00:03:54,010
different learning rates and tried to

95
00:03:54,010 --> 00:03:55,389
see how they seemed to relate together

96
00:03:55,389 --> 00:03:58,510
and these are all like cool experiments

97
00:03:58,510 --> 00:04:00,299
which you know you can try yourself

98
00:04:00,299 --> 00:04:04,060
I predict again he's written something

99
00:04:04,060 --> 00:04:05,829
again a kind of a research into this

100
00:04:05,829 --> 00:04:10,810
question I made a claim that the the

101
00:04:10,810 --> 00:04:12,099
stochastic gradient descent with

102
00:04:12,099 --> 00:04:14,769
restarts finds more generalizable parts

103
00:04:14,769 --> 00:04:16,479
of the function surface because they're

104
00:04:16,479 --> 00:04:18,340
kind of flatter and he's been trying to

105
00:04:18,340 --> 00:04:19,599
figure out is there a way to measure

106
00:04:19,599 --> 00:04:22,479
that more directly not quite successful

107
00:04:22,479 --> 00:04:23,770
yet but a really interesting piece of

108
00:04:23,770 --> 00:04:27,789
research got some introductions to

109
00:04:27,789 --> 00:04:32,999
convolutional neural networks and then

110
00:04:32,999 --> 00:04:34,199
something that we'll be learning about

111
00:04:34,199 --> 00:04:36,389
towards the end of this course but I'm

112
00:04:36,389 --> 00:04:37,349
sure you've noticed we're using

113
00:04:37,349 --> 00:04:40,499
something called ResNet and a nonce aha

114
00:04:40,499 --> 00:04:43,189
actually posted a pretty impressive

115
00:04:43,189 --> 00:04:45,569
analysis of like watts arrest net and

116
00:04:45,569 --> 00:04:47,399
why is it interesting and this one's

117
00:04:47,399 --> 00:04:49,169
actually being very already shared very

118
00:04:49,169 --> 00:04:50,639
widely around the internet I've seen

119
00:04:50,639 --> 00:04:54,360
also so some more advanced students who

120
00:04:54,360 --> 00:04:57,059
are interested in jumping ahead can look

121
00:04:57,059 --> 00:05:00,360
at that and uphill Tamang also has done

122
00:05:00,360 --> 00:05:04,349
something similar so lots of yeah lots

123
00:05:04,349 --> 00:05:06,059
of stuff going on on the forums

124
00:05:06,059 --> 00:05:07,919
I'm sure you've also noticed we have a

125
00:05:07,919 --> 00:05:11,069
beginner forum now specifically for you

126
00:05:11,069 --> 00:05:15,929
know asking questions which you know it

127
00:05:15,929 --> 00:05:17,479
is always the case that there are no

128
00:05:17,479 --> 00:05:19,860
dumb questions but when there's lots of

129
00:05:19,860 --> 00:05:21,929
people around you talking about advanced

130
00:05:21,929 --> 00:05:23,369
topics it might not feel that way so

131
00:05:23,369 --> 00:05:25,279
hopefully the beginners forum is just a

132
00:05:25,279 --> 00:05:29,819
less intimidating space and if there are

133
00:05:29,819 --> 00:05:32,099
more advanced student who can help

134
00:05:32,099 --> 00:05:34,079
answer those questions please do but

135
00:05:34,079 --> 00:05:35,129
remember when you do answer those

136
00:05:35,129 --> 00:05:37,800
questions try to answer in a way that's

137
00:05:37,800 --> 00:05:39,419
friendly to people that maybe you know

138
00:05:39,419 --> 00:05:41,429
have no more than a year of programming

139
00:05:41,429 --> 00:05:42,989
experience you haven't done any machine

140
00:05:42,989 --> 00:05:50,399
learning before so you know I hope other

141
00:05:50,399 --> 00:05:52,979
people in the class feel like you can

142
00:05:52,979 --> 00:05:54,659
contribute as well and just remember all

143
00:05:54,659 --> 00:05:56,159
of the people we just looked at or many

144
00:05:56,159 --> 00:05:58,979
of them I believe have never hosted

145
00:05:58,979 --> 00:06:00,959
anything to the internet before right I

146
00:06:00,959 --> 00:06:02,429
mean you don't have to be a particular

147
00:06:02,429 --> 00:06:04,769
kind of person to be allowed to block

148
00:06:04,769 --> 00:06:07,079
something you can just jot down your

149
00:06:07,079 --> 00:06:11,399
notes throw it up there and one handy

150
00:06:11,399 --> 00:06:12,959
thing is if you just put it on the forum

151
00:06:12,959 --> 00:06:15,779
and you're not quite sure of some of the

152
00:06:15,779 --> 00:06:18,569
details then then you know you have an

153
00:06:18,569 --> 00:06:20,399
opportunity to get feedback and say like

154
00:06:20,399 --> 00:06:22,049
oh well that's not quite how that works

155
00:06:22,049 --> 00:06:23,579
you know actually it works this way

156
00:06:23,579 --> 00:06:25,559
instead or oh that's a really

157
00:06:25,559 --> 00:06:26,819
interesting insight have you thought

158
00:06:26,819 --> 00:06:29,029
about taking this further and so forth

159
00:06:29,029 --> 00:06:32,159
so what we've done so far is a kind of

160
00:06:32,159 --> 00:06:34,919
an injury an introduction as a just as a

161
00:06:34,919 --> 00:06:38,969
practitioner to convolutional neural

162
00:06:38,969 --> 00:06:41,309
networks for images and we haven't

163
00:06:41,309 --> 00:06:43,389
really talked much at all about

164
00:06:43,389 --> 00:06:46,330
the theory or why they work or the math

165
00:06:46,330 --> 00:06:48,280
of them but on the other hand what we

166
00:06:48,280 --> 00:06:52,659
have done is seen how to build a model

167
00:06:52,659 --> 00:06:55,900
which actually works exceptionally well

168
00:06:55,900 --> 00:06:59,319
compact world-class level models and

169
00:06:59,319 --> 00:07:01,780
we'll kind of review a little bit of

170
00:07:01,780 --> 00:07:06,669
that today and then also today we're

171
00:07:06,669 --> 00:07:08,289
going to dig in a little quite a lot

172
00:07:08,289 --> 00:07:09,819
more actually into the underlying theory

173
00:07:09,819 --> 00:07:12,280
of like what is a what is a CNN what's a

174
00:07:12,280 --> 00:07:14,650
convolution how does this work and then

175
00:07:14,650 --> 00:07:16,000
we're going to kind of go through this

176
00:07:16,000 --> 00:07:18,449
this cycle where we're going to dig

177
00:07:18,449 --> 00:07:21,039
we're going to do a little intro into a

178
00:07:21,039 --> 00:07:23,379
whole bunch of application areas using

179
00:07:23,379 --> 00:07:25,930
neural nets for structured data so kind

180
00:07:25,930 --> 00:07:29,560
of like logistics or forecasting or you

181
00:07:29,560 --> 00:07:31,449
know financial data or that kind of

182
00:07:31,449 --> 00:07:34,360
thing and then looking at language

183
00:07:34,360 --> 00:07:36,669
applications and LP applications using

184
00:07:36,669 --> 00:07:39,210
recurrent neural Nets and then

185
00:07:39,210 --> 00:07:42,689
collaborative filtering for

186
00:07:42,689 --> 00:07:45,550
recommendations and systems and so these

187
00:07:45,550 --> 00:07:47,740
will all be like similar to what we've

188
00:07:47,740 --> 00:07:50,199
done for cnn's for images would be like

189
00:07:50,199 --> 00:07:51,009
here's how you can get a

190
00:07:51,009 --> 00:07:53,020
state-of-the-art result without digging

191
00:07:53,020 --> 00:07:55,150
into the theory but but knowing how to

192
00:07:55,150 --> 00:07:57,849
actually make a work and then we're kind

193
00:07:57,849 --> 00:08:00,400
of going to go back through those almost

194
00:08:00,400 --> 00:08:01,839
in reverse order so then we're going to

195
00:08:01,839 --> 00:08:03,669
dig right into collaborative filtering

196
00:08:03,669 --> 00:08:06,250
in a lot of detail and see how how to

197
00:08:06,250 --> 00:08:08,409
write the code underneath and how the

198
00:08:08,409 --> 00:08:10,479
math works underneath and then we're

199
00:08:10,479 --> 00:08:11,589
going to do the same thing for the

200
00:08:11,589 --> 00:08:13,719
structured data analysis we're going to

201
00:08:13,719 --> 00:08:15,400
do the same thing for comp nets for

202
00:08:15,400 --> 00:08:18,069
images and finally an in depth deep dive

203
00:08:18,069 --> 00:08:20,620
into apparent neural networks so that's

204
00:08:20,620 --> 00:08:24,610
kind of where we're okay so let's start

205
00:08:24,610 --> 00:08:29,020
by doing a little bit of a review and I

206
00:08:29,020 --> 00:08:32,620
want to also provide a bit more detail

207
00:08:32,620 --> 00:08:35,050
on some on some steps that we only

208
00:08:35,050 --> 00:08:36,940
briefly slipped over so I want to make

209
00:08:36,940 --> 00:08:39,209
sure that we're all able to complete

210
00:08:39,209 --> 00:08:42,039
kind of last week's assignment which was

211
00:08:42,039 --> 00:08:45,310
that the dog breeze I mean to basically

212
00:08:45,310 --> 00:08:47,050
apply what you've learned with another

213
00:08:47,050 --> 00:08:48,790
data set and I thought the easiest one

214
00:08:48,790 --> 00:08:50,320
to do with me the dog breeds cattle

215
00:08:50,320 --> 00:08:52,089
competition and so I want to make sure

216
00:08:52,089 --> 00:08:53,709
everybody has everything you need to do

217
00:08:53,709 --> 00:08:55,889
this right now so and the

218
00:08:55,889 --> 00:08:57,269
first thing is to make sure that you

219
00:08:57,269 --> 00:09:01,230
know how to download data and so there's

220
00:09:01,230 --> 00:09:03,059
there's two main places at the moment

221
00:09:03,059 --> 00:09:04,799
we're kind of downloading data from one

222
00:09:04,799 --> 00:09:07,439
is from cattle and the other is from

223
00:09:07,439 --> 00:09:11,339
like anywhere else and so I'll first of

224
00:09:11,339 --> 00:09:15,299
all do the the casual version so to

225
00:09:15,299 --> 00:09:18,569
download from cattle we use something

226
00:09:18,569 --> 00:09:24,809
called cattle CLI which is gear and to

227
00:09:24,809 --> 00:09:28,379
install what I think it's already in the

228
00:09:28,379 --> 00:09:34,980
system will shake yeah so it should

229
00:09:34,980 --> 00:09:39,149
already be in your environment but to

230
00:09:39,149 --> 00:09:41,040
make sure one thing that happens is

231
00:09:41,040 --> 00:09:42,449
because this is downloading from the

232
00:09:42,449 --> 00:09:44,369
cattle website through experience rating

233
00:09:44,369 --> 00:09:45,540
every time cap will change us the

234
00:09:45,540 --> 00:09:48,149
website it breaks so anytime you try to

235
00:09:48,149 --> 00:09:51,569
use it and if the cattles websites

236
00:09:51,569 --> 00:09:53,279
changed recent when you'll need to make

237
00:09:53,279 --> 00:09:55,709
sure you get the most recent version so

238
00:09:55,709 --> 00:10:00,379
you can always go to pip install cable -

239
00:10:00,379 --> 00:10:06,419
CL I - - upgrade and so that'll just

240
00:10:06,419 --> 00:10:07,829
make sure that you've got the latest

241
00:10:07,829 --> 00:10:10,290
version of of it and everything that it

242
00:10:10,290 --> 00:10:14,009
depends on okay and so then having done

243
00:10:14,009 --> 00:10:16,679
that you can follow the instructions

244
00:10:16,679 --> 00:10:18,749
actually I think Reshma was kind enough

245
00:10:18,749 --> 00:10:21,720
to they go there's a cable CLI you know

246
00:10:21,720 --> 00:10:22,919
like everything you need to know can be

247
00:10:22,919 --> 00:10:29,910
under Reshma's github so basically to do

248
00:10:29,910 --> 00:10:35,089
that at the next step you go kg download

249
00:10:35,089 --> 00:10:37,679
and then you provide your username with

250
00:10:37,679 --> 00:10:41,100
- you you provide your password with - P

251
00:10:41,100 --> 00:10:44,399
and then - see it the competition name

252
00:10:44,399 --> 00:10:46,019
and a lot of people in the forum is

253
00:10:46,019 --> 00:10:47,730
being confused about what to enter here

254
00:10:47,730 --> 00:10:49,529
and so the key thing to note is that

255
00:10:49,529 --> 00:10:51,439
when you're at a capital competition

256
00:10:51,439 --> 00:10:55,559
after the /c there's a specific name

257
00:10:55,559 --> 00:10:58,169
planet - understanding - etcetera right

258
00:10:58,169 --> 00:11:02,100
that's the name you need okay the other

259
00:11:02,100 --> 00:11:03,749
thing you'll need to make sure is that

260
00:11:03,749 --> 00:11:06,809
you've on your own computer have

261
00:11:06,809 --> 00:11:08,489
attempted to click download at least

262
00:11:08,489 --> 00:11:09,670
once because when you do

263
00:11:09,670 --> 00:11:12,910
ask you to accept the rules if you've

264
00:11:12,910 --> 00:11:15,730
forgotten to do that kg download will

265
00:11:15,730 --> 00:11:17,170
give you a hint it'll say oh it looks

266
00:11:17,170 --> 00:11:18,850
like you might have forgotten the rules

267
00:11:18,850 --> 00:11:21,700
if you log into cattle with like a

268
00:11:21,700 --> 00:11:24,010
Google account like anything other than

269
00:11:24,010 --> 00:11:26,800
a username password this won't work so

270
00:11:26,800 --> 00:11:28,720
you'll need to click forgot password on

271
00:11:28,720 --> 00:11:30,730
Kaggle and get them to send you a normal

272
00:11:30,730 --> 00:11:33,550
password so that's the cattle version

273
00:11:33,550 --> 00:11:36,220
right and so when you do that you end up

274
00:11:36,220 --> 00:11:38,410
with a whole folder created for you with

275
00:11:38,410 --> 00:11:41,430
all of that competition and data in it

276
00:11:41,430 --> 00:11:44,080
so a couple of reasons you might want to

277
00:11:44,080 --> 00:11:46,480
not use that the first years that you're

278
00:11:46,480 --> 00:11:48,220
using a data set that's not on cattle

279
00:11:48,220 --> 00:11:50,320
the second is that you don't want all of

280
00:11:50,320 --> 00:11:52,600
the data sets in a cattle competition

281
00:11:52,600 --> 00:11:55,330
for example the planet competition that

282
00:11:55,330 --> 00:11:56,800
we've been looking at a little bit we'll

283
00:11:56,800 --> 00:12:00,010
look at again today has data in two

284
00:12:00,010 --> 00:12:03,610
formats TIFF and JPEG the TIFF is 19

285
00:12:03,610 --> 00:12:06,370
gigabytes and the JPEG is 600 megabytes

286
00:12:06,370 --> 00:12:08,380
so you probably don't want to download

287
00:12:08,380 --> 00:12:11,890
both so I'll show you a really cool kit

288
00:12:11,890 --> 00:12:13,510
which actually somebody on the forum

289
00:12:13,510 --> 00:12:14,920
taught me I think was one of the young

290
00:12:14,920 --> 00:12:18,900
MSN students here at USF there's a

291
00:12:18,900 --> 00:12:24,160
Chrome extension cord curl W get so you

292
00:12:24,160 --> 00:12:27,520
can just search for a curl W get and

293
00:12:27,520 --> 00:12:29,590
then you install it by just clicking on

294
00:12:29,590 --> 00:12:31,420
installed and having an extension before

295
00:12:31,420 --> 00:12:35,470
and then from now on every time you try

296
00:12:35,470 --> 00:12:36,970
to download something so I'll try and

297
00:12:36,970 --> 00:12:40,480
download this file

298
00:12:40,480 --> 00:12:42,699
and I'll just go ahead and cancel it

299
00:12:42,699 --> 00:12:45,220
right and now you see this little yellow

300
00:12:45,220 --> 00:12:47,620
button that's added up here there's a

301
00:12:47,620 --> 00:12:51,310
whole command here right so I can copy

302
00:12:51,310 --> 00:12:59,949
that and paste it into my window and hit

303
00:12:59,949 --> 00:13:03,670
go and it's there cuz okay so what that

304
00:13:03,670 --> 00:13:06,040
does is like all of your cookies and

305
00:13:06,040 --> 00:13:08,139
headers and everything else needed to

306
00:13:08,139 --> 00:13:10,449
download that file is like say so this

307
00:13:10,449 --> 00:13:13,959
is not just useful for downloading data

308
00:13:13,959 --> 00:13:16,029
it's also useful if you like trying to

309
00:13:16,029 --> 00:13:18,130
download some I don't know TV show or

310
00:13:18,130 --> 00:13:19,420
something anything where you're it's

311
00:13:19,420 --> 00:13:23,440
hidden behind a login or something you

312
00:13:23,440 --> 00:13:25,120
can you can grab it and actually that is

313
00:13:25,120 --> 00:13:26,800
very useful for data science because

314
00:13:26,800 --> 00:13:28,240
quite often we want to analyze things

315
00:13:28,240 --> 00:13:31,630
like videos on our on our consoles so

316
00:13:31,630 --> 00:13:33,610
this is a good trick alright so there's

317
00:13:33,610 --> 00:13:40,029
two ways to get the data so then having

318
00:13:40,029 --> 00:13:44,019
got the data you then need to build your

319
00:13:44,019 --> 00:13:47,410
model right so what I tend to do like

320
00:13:47,410 --> 00:13:49,209
you'll notice that I tend to assume that

321
00:13:49,209 --> 00:13:51,880
the data is in a directory called data

322
00:13:51,880 --> 00:13:54,250
that's a subdirectory of wherever your

323
00:13:54,250 --> 00:13:57,670
notebook is right now you don't

324
00:13:57,670 --> 00:14:00,399
necessarily actually want to put your

325
00:14:00,399 --> 00:14:01,600
data there you might want to put it

326
00:14:01,600 --> 00:14:03,639
directly in your home directory or you

327
00:14:03,639 --> 00:14:04,990
might wanna put it on another drive or

328
00:14:04,990 --> 00:14:07,959
whatever so what I do is if you look

329
00:14:07,959 --> 00:14:11,620
inside my courses do one folder you'll

330
00:14:11,620 --> 00:14:14,380
see that data is actually a symbolic

331
00:14:14,380 --> 00:14:17,949
link to a different drive alright so you

332
00:14:17,949 --> 00:14:19,600
can put it anywhere you like and then

333
00:14:19,600 --> 00:14:22,269
you can just add a symbolic link or you

334
00:14:22,269 --> 00:14:24,430
can just put it there directly it's up

335
00:14:24,430 --> 00:14:26,050
to you if you haven't used symlinks

336
00:14:26,050 --> 00:14:28,990
before they're like aliases or shortcuts

337
00:14:28,990 --> 00:14:32,019
on the mac or windows very handy and

338
00:14:32,019 --> 00:14:33,760
there's some threads on the forum about

339
00:14:33,760 --> 00:14:35,829
how to use them if you want help with

340
00:14:35,829 --> 00:14:38,529
that that for example is also how we

341
00:14:38,529 --> 00:14:41,290
actually have the fast AI modules

342
00:14:41,290 --> 00:14:43,660
available from the same place as our

343
00:14:43,660 --> 00:14:45,490
notebooks it's just a symlink

344
00:14:45,490 --> 00:14:48,699
to where they come from anytime you want

345
00:14:48,699 --> 00:14:51,790
to see like where things actually point

346
00:14:51,790 --> 00:14:54,250
to in Linux you can just use the

347
00:14:54,250 --> 00:14:57,490
- L flag - listing a directory and

348
00:14:57,490 --> 00:14:59,490
that'll show you where the symlinks

349
00:14:59,490 --> 00:15:01,450
exist still lost I'll show you which

350
00:15:01,450 --> 00:15:05,170
scenes the directories so forth okay so

351
00:15:05,170 --> 00:15:12,340
one thing which may be a little unclear

352
00:15:12,340 --> 00:15:14,740
based on what we've done so far is like

353
00:15:14,740 --> 00:15:17,860
how little code you actually need to do

354
00:15:17,860 --> 00:15:20,410
this end to it so what I've got here is

355
00:15:20,410 --> 00:15:22,870
is in a single window is an entire

356
00:15:22,870 --> 00:15:25,270
end-to-end process to get a

357
00:15:25,270 --> 00:15:27,220
state-of-the-art result put cats versus

358
00:15:27,220 --> 00:15:30,310
dogs all right I've only step I've

359
00:15:30,310 --> 00:15:31,810
skipped is the bit where we've

360
00:15:31,810 --> 00:15:33,970
downloaded it from title and then where

361
00:15:33,970 --> 00:15:37,900
we unzip it all right so these are

362
00:15:37,900 --> 00:15:43,180
literally all the steps and so we import

363
00:15:43,180 --> 00:15:45,730
our libraries and actually if you import

364
00:15:45,730 --> 00:15:47,890
this one Kampf loner that basically

365
00:15:47,890 --> 00:15:51,130
imports everything else so that's that

366
00:15:51,130 --> 00:15:53,020
we need to tell at the path of where

367
00:15:53,020 --> 00:15:55,630
things are the size that we want the

368
00:15:55,630 --> 00:15:59,230
batch size that we want right so then

369
00:15:59,230 --> 00:16:00,190
and we're going to learn a lot more

370
00:16:00,190 --> 00:16:02,500
about what these do very shortly but

371
00:16:02,500 --> 00:16:04,000
basically we say how do we want to

372
00:16:04,000 --> 00:16:06,670
transform our data so we want to

373
00:16:06,670 --> 00:16:08,530
transform it in a way that's suitable to

374
00:16:08,530 --> 00:16:10,780
this particular kind of model and it

375
00:16:10,780 --> 00:16:13,450
assumes that the photos aside on photos

376
00:16:13,450 --> 00:16:15,250
and that we're going to zoom in up to

377
00:16:15,250 --> 00:16:18,490
ten percent each time we say that we

378
00:16:18,490 --> 00:16:21,490
want to get some data based on paths and

379
00:16:21,490 --> 00:16:23,020
so remember this is this idea that

380
00:16:23,020 --> 00:16:24,610
there's a path called cats and the path

381
00:16:24,610 --> 00:16:26,860
called dogs and they're inside a path

382
00:16:26,860 --> 00:16:28,470
called train and a path called valid

383
00:16:28,470 --> 00:16:34,360
note that you can always overwrite these

384
00:16:34,360 --> 00:16:36,280
with other things so if your things are

385
00:16:36,280 --> 00:16:37,360
in different folders

386
00:16:37,360 --> 00:16:39,850
you could either rename them or you can

387
00:16:39,850 --> 00:16:42,160
see here there's like a train name and a

388
00:16:42,160 --> 00:16:44,620
bowel name you can always pick something

389
00:16:44,620 --> 00:16:48,280
else here also notice there's a test

390
00:16:48,280 --> 00:16:51,070
name so if you want to submit some into

391
00:16:51,070 --> 00:16:52,839
cattle you'll need to fill in the name

392
00:16:52,839 --> 00:16:54,430
the name of the folder where the test

393
00:16:54,430 --> 00:16:56,380
sentence and obviously those those won't

394
00:16:56,380 --> 00:16:59,770
be labeled

395
00:16:59,770 --> 00:17:02,750
so then we create a model from a

396
00:17:02,750 --> 00:17:04,910
pre-training model it's from a resonant

397
00:17:04,910 --> 00:17:08,150
50 model using this data and then we

398
00:17:08,150 --> 00:17:11,600
call fit and remember by default that

399
00:17:11,600 --> 00:17:13,640
has all of the layers but the last few

400
00:17:13,640 --> 00:17:15,560
frozen and again we'll learn a lot more

401
00:17:15,560 --> 00:17:17,420
about what that means

402
00:17:17,420 --> 00:17:19,370
and so that's that's what that does so

403
00:17:19,370 --> 00:17:23,110
that that took two and a half minutes

404
00:17:23,110 --> 00:17:26,120
notice here I didn't say pre-compute

405
00:17:26,120 --> 00:17:27,890
equals true again there's been some

406
00:17:27,890 --> 00:17:29,510
confusion on the forums about like what

407
00:17:29,510 --> 00:17:33,320
that means it's only a is only something

408
00:17:33,320 --> 00:17:35,390
that makes it a little faster for this

409
00:17:35,390 --> 00:17:37,490
first step right so you can always skip

410
00:17:37,490 --> 00:17:39,710
it and if you're at all confused about

411
00:17:39,710 --> 00:17:41,480
it or it's causing you any problems just

412
00:17:41,480 --> 00:17:45,040
leave it off right because it's just a

413
00:17:45,040 --> 00:17:48,950
it's just a shortcut which caches some

414
00:17:48,950 --> 00:17:50,660
of the intermediate steps that don't

415
00:17:50,660 --> 00:17:52,880
have to be recalculating each time and

416
00:17:52,880 --> 00:17:54,710
remember that when we are using pre

417
00:17:54,710 --> 00:17:56,930
computed activations data or

418
00:17:56,930 --> 00:17:59,390
augmentation doesn't work right so even

419
00:17:59,390 --> 00:18:01,460
if you ask for a data augmentation if

420
00:18:01,460 --> 00:18:04,070
you've got pre compute equals true it

421
00:18:04,070 --> 00:18:05,090
doesn't actually do any data

422
00:18:05,090 --> 00:18:06,710
augmentation because it's using the

423
00:18:06,710 --> 00:18:11,180
cached non augmented activations so in

424
00:18:11,180 --> 00:18:12,800
this case to keep this as simple as

425
00:18:12,800 --> 00:18:14,780
possible I have no pre computed anything

426
00:18:14,780 --> 00:18:17,870
going on so I do three cycles of length

427
00:18:17,870 --> 00:18:24,110
one and then I can then unfreeze so it's

428
00:18:24,110 --> 00:18:25,870
now going to train the whole thing

429
00:18:25,870 --> 00:18:27,770
something we haven't seen before and

430
00:18:27,770 --> 00:18:30,070
we'll learn about in the second half is

431
00:18:30,070 --> 00:18:33,260
called B and freeze for now all you need

432
00:18:33,260 --> 00:18:36,050
to know is that if you're using a model

433
00:18:36,050 --> 00:18:39,170
like a bigger deeper model like ResNet

434
00:18:39,170 --> 00:18:42,830
50 or rez next 101 on a data set that's

435
00:18:42,830 --> 00:18:46,250
very very similar to imagenet like these

436
00:18:46,250 --> 00:18:48,260
cat sandbox data set sort of words it's

437
00:18:48,260 --> 00:18:51,860
like sidon photos of standard objects

438
00:18:51,860 --> 00:18:54,590
you know of a similar size to image turn

439
00:18:54,590 --> 00:18:56,720
and money somewhere between 200 and 500

440
00:18:56,720 --> 00:18:57,970
pixels

441
00:18:57,970 --> 00:19:01,400
you should probably add this line when

442
00:19:01,400 --> 00:19:03,380
you unfreeze for those of you that are

443
00:19:03,380 --> 00:19:06,080
more advanced what it's doing is it's

444
00:19:06,080 --> 00:19:09,290
causing the batch normalization moving

445
00:19:09,290 --> 00:19:10,840
averages to not be updated

446
00:19:10,840 --> 00:19:12,070
but in the second half of this course

447
00:19:12,070 --> 00:19:14,200
we're gonna learn all about why we do

448
00:19:14,200 --> 00:19:15,610
that it's something that's not supported

449
00:19:15,610 --> 00:19:18,129
by any other library but it turns out to

450
00:19:18,129 --> 00:19:20,409
be super important anyway so we do one

451
00:19:20,409 --> 00:19:21,190
more

452
00:19:21,190 --> 00:19:26,039
epoch with training the whole network

453
00:19:26,039 --> 00:19:28,809
and then at the end we use test time

454
00:19:28,809 --> 00:19:31,990
augmentation to ensure that we get the

455
00:19:31,990 --> 00:19:34,870
best predictions we can and that gives

456
00:19:34,870 --> 00:19:37,620
us ninety nine point four five percent

457
00:19:37,620 --> 00:19:41,049
so that's that's it right so when you

458
00:19:41,049 --> 00:19:44,649
try a new data set they're basically the

459
00:19:44,649 --> 00:19:47,139
minimum set of steps that you would need

460
00:19:47,139 --> 00:19:49,809
to follow you'll notice this is assuming

461
00:19:49,809 --> 00:19:51,820
I already know what learning wrote to

462
00:19:51,820 --> 00:19:53,409
use so you'd use the learning rate

463
00:19:53,409 --> 00:19:56,019
finder for that it's assuming that I

464
00:19:56,019 --> 00:19:59,590
know that the directory layout and so

465
00:19:59,590 --> 00:20:02,799
forth so that's kind of a minimum set

466
00:20:02,799 --> 00:20:04,600
now one of the things that I wanted to

467
00:20:04,600 --> 00:20:07,210
make sure you had an understanding of

468
00:20:07,210 --> 00:20:10,059
how to do is how to use other libraries

469
00:20:10,059 --> 00:20:12,999
other than fast AI and so I feel like

470
00:20:12,999 --> 00:20:15,039
the best thing to to look at is to look

471
00:20:15,039 --> 00:20:18,100
at care us because care us is a library

472
00:20:18,100 --> 00:20:20,590
just like fast AI sits on top of pi

473
00:20:20,590 --> 00:20:23,590
torch care us sits on top of actually a

474
00:20:23,590 --> 00:20:25,450
whole variety of different backends it

475
00:20:25,450 --> 00:20:28,029
fits mainly people nowadays use it with

476
00:20:28,029 --> 00:20:28,769
tensorflow

477
00:20:28,769 --> 00:20:32,259
there's also an MX net version there's

478
00:20:32,259 --> 00:20:37,450
also a Microsoft CNT K version so what

479
00:20:37,450 --> 00:20:39,789
I've got if you do a git pull you'll see

480
00:20:39,789 --> 00:20:43,480
that there's a something called care us

481
00:20:43,480 --> 00:20:46,059
less than one where I've attempted to

482
00:20:46,059 --> 00:20:48,429
replicate at least parts of lesson one

483
00:20:48,429 --> 00:20:52,090
in care us just to give you a sense of

484
00:20:52,090 --> 00:20:59,110
how that works I'm not going to talk

485
00:20:59,110 --> 00:21:00,850
more about batch two norm freeze now

486
00:21:00,850 --> 00:21:04,139
other than to say if you're using

487
00:21:04,139 --> 00:21:07,659
something which has got a number larger

488
00:21:07,659 --> 00:21:09,039
than 34 at the end

489
00:21:09,039 --> 00:21:11,950
so like ResNet 50 or res next 101 and

490
00:21:11,950 --> 00:21:16,090
you're trading a data set that has that

491
00:21:16,090 --> 00:21:18,129
is very similar to image net so it's

492
00:21:18,129 --> 00:21:20,529
like normal photos of normal sizes where

493
00:21:20,529 --> 00:21:22,480
the thing of interest takes up most of

494
00:21:22,480 --> 00:21:24,520
the frame then you probably should

495
00:21:24,520 --> 00:21:27,880
at the end fries true after unfreeze

496
00:21:27,880 --> 00:21:30,400
if in doubt try trading it with and then

497
00:21:30,400 --> 00:21:33,580
try trading it without more advanced

498
00:21:33,580 --> 00:21:35,679
students will certainly talk about it on

499
00:21:35,679 --> 00:21:37,420
the forums this week and we will be

500
00:21:37,420 --> 00:21:39,910
talking about the details of it in the

501
00:21:39,910 --> 00:21:41,710
second half of the course when we come

502
00:21:41,710 --> 00:21:46,720
back to our CNN in death section in the

503
00:21:46,720 --> 00:21:55,660
second last lesson so with care us again

504
00:21:55,660 --> 00:22:01,480
we import a bunch of stuff and remember

505
00:22:01,480 --> 00:22:03,130
I mentioned that this idea that you've

506
00:22:03,130 --> 00:22:04,630
got a thing called train and a thing

507
00:22:04,630 --> 00:22:06,190
called valid and inside that you've got

508
00:22:06,190 --> 00:22:07,540
a thing called dogs and things called

509
00:22:07,540 --> 00:22:10,679
cats is a standard way of providing

510
00:22:10,679 --> 00:22:14,679
image labelled images so Karis does that

511
00:22:14,679 --> 00:22:16,870
too right so it's going to tell it where

512
00:22:16,870 --> 00:22:18,610
the training set and the validation set

513
00:22:18,610 --> 00:22:24,309
are twice what batch size to used now

514
00:22:24,309 --> 00:22:27,100
you'll notice in Karis we need much much

515
00:22:27,100 --> 00:22:32,020
much more code to do the same thing more

516
00:22:32,020 --> 00:22:34,600
importantly each part of that code has

517
00:22:34,600 --> 00:22:36,460
many many many more things you have to

518
00:22:36,460 --> 00:22:38,650
set and if you set them wrong everything

519
00:22:38,650 --> 00:22:43,840
breaks right so I'll give you a summary

520
00:22:43,840 --> 00:22:44,650
of what they are

521
00:22:44,650 --> 00:22:47,050
so you're basically rather than creating

522
00:22:47,050 --> 00:22:51,250
a single data object in chaos we first

523
00:22:51,250 --> 00:22:52,660
of all have to define something called a

524
00:22:52,660 --> 00:22:55,030
data generator to say kind of generate

525
00:22:55,030 --> 00:22:57,550
the data and so a data generator we

526
00:22:57,550 --> 00:23:00,010
basically have to say what kind of data

527
00:23:00,010 --> 00:23:04,780
augmentation we want to do and we also

528
00:23:04,780 --> 00:23:08,190
we actually have to say what kind of

529
00:23:08,190 --> 00:23:11,170
normalization do we want to do

530
00:23:11,170 --> 00:23:13,179
so we're else with fast AI we just say

531
00:23:13,179 --> 00:23:16,240
whatever ResNet 50 requires just do that

532
00:23:16,240 --> 00:23:18,160
for me please we actually have to kind

533
00:23:18,160 --> 00:23:19,929
of know a little bit about what's

534
00:23:19,929 --> 00:23:21,700
expected of us

535
00:23:21,700 --> 00:23:23,320
generally speaking copying and pasting

536
00:23:23,320 --> 00:23:25,300
cos code from the internet is a good way

537
00:23:25,300 --> 00:23:27,160
to make sure you've got the right the

538
00:23:27,160 --> 00:23:30,220
right stuff to make that work and again

539
00:23:30,220 --> 00:23:32,110
it doesn't have a kind of a standard set

540
00:23:32,110 --> 00:23:34,600
of like here the best data augmentation

541
00:23:34,600 --> 00:23:37,280
parameters to use for photos so you know

542
00:23:37,280 --> 00:23:39,020
I've copied and pasted all of this from

543
00:23:39,020 --> 00:23:43,550
the Kaos documentation so I don't know

544
00:23:43,550 --> 00:23:45,320
if it's I don't think it's the best set

545
00:23:45,320 --> 00:23:46,910
to use it all but it's the set that

546
00:23:46,910 --> 00:23:49,220
they're using in their docks so having

547
00:23:49,220 --> 00:23:51,470
said this is how I want to generate data

548
00:23:51,470 --> 00:23:53,840
so horizontally fit sometimes you know

549
00:23:53,840 --> 00:23:56,540
zoom sometimes sheer sometimes we then

550
00:23:56,540 --> 00:23:59,360
create a generator from that by taking

551
00:23:59,360 --> 00:24:01,880
that data generator and saying I want to

552
00:24:01,880 --> 00:24:04,760
generate images by looking from a

553
00:24:04,760 --> 00:24:07,070
directory we pass in the directory which

554
00:24:07,070 --> 00:24:09,440
is of the same directory structure that

555
00:24:09,440 --> 00:24:12,470
fast AI users and you'll see there's

556
00:24:12,470 --> 00:24:14,090
some overlaps with kind of how fast AI

557
00:24:14,090 --> 00:24:16,220
works here you tell it what size images

558
00:24:16,220 --> 00:24:18,290
you want to create you tell at what

559
00:24:18,290 --> 00:24:19,850
batch size you want in your mini batches

560
00:24:19,850 --> 00:24:22,640
and then there's something you not to

561
00:24:22,640 --> 00:24:24,140
worry about too much but basically if

562
00:24:24,140 --> 00:24:26,390
you're just got two possible outcomes

563
00:24:26,390 --> 00:24:28,760
you would generally say binary here if

564
00:24:28,760 --> 00:24:29,960
you've got multiple possible outcomes

565
00:24:29,960 --> 00:24:32,390
would say categorical yeah so we've only

566
00:24:32,390 --> 00:24:33,440
got cats or dogs

567
00:24:33,440 --> 00:24:37,970
so it's binary so an example of like

568
00:24:37,970 --> 00:24:39,710
where things get a little more complex

569
00:24:39,710 --> 00:24:41,510
is you have to do the same thing for the

570
00:24:41,510 --> 00:24:43,580
validation set so it's up to you to

571
00:24:43,580 --> 00:24:45,620
create a data generator that doesn't

572
00:24:45,620 --> 00:24:47,570
have data augmentation because obviously

573
00:24:47,570 --> 00:24:49,280
for the validation set unless you're

574
00:24:49,280 --> 00:24:51,890
using t/ta that's going to start things

575
00:24:51,890 --> 00:24:57,440
up you also when you train you randomly

576
00:24:57,440 --> 00:24:59,090
reorder the images so that they're

577
00:24:59,090 --> 00:25:01,190
always shown in different orders to make

578
00:25:01,190 --> 00:25:03,170
it more random but with a validation

579
00:25:03,170 --> 00:25:06,080
it's vital that you don't do that

580
00:25:06,080 --> 00:25:07,520
because if you shuffle the validation

581
00:25:07,520 --> 00:25:09,770
set you then can't track how well you're

582
00:25:09,770 --> 00:25:11,270
doing it's in a different order for the

583
00:25:11,270 --> 00:25:11,810
labels

584
00:25:11,810 --> 00:25:14,870
that's a basically these are the kind of

585
00:25:14,870 --> 00:25:17,180
steps you have to do every time with

586
00:25:17,180 --> 00:25:22,160
care us so again the reason I was using

587
00:25:22,160 --> 00:25:24,320
ResNet 50 before is chaos doesn't have

588
00:25:24,320 --> 00:25:26,960
ResNet 34 unfortunately so I just wanted

589
00:25:26,960 --> 00:25:28,370
to compare like with Mike so we're going

590
00:25:28,370 --> 00:25:32,750
to use resident 50 here there isn't the

591
00:25:32,750 --> 00:25:34,940
same idea with chaos of saying like

592
00:25:34,940 --> 00:25:37,520
constructor model that is suitable for

593
00:25:37,520 --> 00:25:39,860
this data set for me so you have to do

594
00:25:39,860 --> 00:25:42,290
it by hand right so the way you do it is

595
00:25:42,290 --> 00:25:44,360
to basically say this is my base model

596
00:25:44,360 --> 00:25:47,330
and then you have to construct on top of

597
00:25:47,330 --> 00:25:50,000
that manually the layers that you want

598
00:25:50,000 --> 00:25:50,600
to add

599
00:25:50,600 --> 00:25:52,220
and so by the end of this course you'll

600
00:25:52,220 --> 00:25:53,809
understand a way it is that these

601
00:25:53,809 --> 00:25:56,630
particular three layers other layers

602
00:25:56,630 --> 00:26:01,370
that we add so having done that in chaos

603
00:26:01,370 --> 00:26:03,230
you basically say okay this is my model

604
00:26:03,230 --> 00:26:06,070
and then again there isn't like a

605
00:26:06,070 --> 00:26:08,450
concept to it like automatically

606
00:26:08,450 --> 00:26:10,880
freezing things or an API for that so

607
00:26:10,880 --> 00:26:13,280
you just have to allow look through the

608
00:26:13,280 --> 00:26:16,190
layers that you want to freeze and call

609
00:26:16,190 --> 00:26:20,630
dot trainable equals false on them in

610
00:26:20,630 --> 00:26:22,940
Karis there's a concept we don't have in

611
00:26:22,940 --> 00:26:25,370
fast AI or play a torch of compiling a

612
00:26:25,370 --> 00:26:27,110
model so basically once your model is

613
00:26:27,110 --> 00:26:28,960
ready to use you have to compile it

614
00:26:28,960 --> 00:26:32,000
passing in what kind of optimizer to use

615
00:26:32,000 --> 00:26:33,860
what kind of loss to look for about

616
00:26:33,860 --> 00:26:37,039
metric so again with fast AI you don't

617
00:26:37,039 --> 00:26:38,990
have to pass this in because we know

618
00:26:38,990 --> 00:26:41,720
what loss is the write loss to use you

619
00:26:41,720 --> 00:26:43,039
can always override it but for a

620
00:26:43,039 --> 00:26:44,570
particular model we give you good

621
00:26:44,570 --> 00:26:48,070
defaults okay so having done all that

622
00:26:48,070 --> 00:26:50,179
rather than calling fit you call

623
00:26:50,179 --> 00:26:53,120
generator passing in those two

624
00:26:53,120 --> 00:26:54,679
generators that you saw earlier the

625
00:26:54,679 --> 00:26:55,789
Train generator in the validation

626
00:26:55,789 --> 00:26:59,809
generator for reasons I don't quite

627
00:26:59,809 --> 00:27:01,820
understand chaos expects you to also

628
00:27:01,820 --> 00:27:03,650
tell it how many batches there are per

629
00:27:03,650 --> 00:27:06,409
epoch so the number of batches is a

630
00:27:06,409 --> 00:27:08,620
quarter the size of the generator

631
00:27:08,620 --> 00:27:11,780
divided by the batch size you can tell

632
00:27:11,780 --> 00:27:16,100
it how many epochs just like in fast AI

633
00:27:16,100 --> 00:27:18,950
you can say how many processes or how

634
00:27:18,950 --> 00:27:21,840
many workers to use for pre-processing

635
00:27:21,840 --> 00:27:23,200
[Music]

636
00:27:23,200 --> 00:27:26,330
unlike fast AI the default in chaos is

637
00:27:26,330 --> 00:27:30,830
basically not to use any so to get good

638
00:27:30,830 --> 00:27:32,090
speed you're going to make sure you

639
00:27:32,090 --> 00:27:35,870
include this and so that's basically

640
00:27:35,870 --> 00:27:39,289
enough to start fine tuning the last

641
00:27:39,289 --> 00:27:44,299
layers so as you can see I got to a

642
00:27:44,299 --> 00:27:47,750
validation accuracy of 95% but as you

643
00:27:47,750 --> 00:27:48,919
can also see something really weird

644
00:27:48,919 --> 00:27:50,960
happened we're after one it was like 49

645
00:27:50,960 --> 00:27:53,840
and then it was 69 and then 95 I don't

646
00:27:53,840 --> 00:27:56,750
know why these are so low that's not

647
00:27:56,750 --> 00:27:58,159
normal

648
00:27:58,159 --> 00:28:00,140
I may have there may be a bug and chaos

649
00:28:00,140 --> 00:28:03,290
they may be a bug in my code I reached

650
00:28:03,290 --> 00:28:04,340
out on Twitter to see if

651
00:28:04,340 --> 00:28:05,630
anybody could figure it out but they

652
00:28:05,630 --> 00:28:07,130
couldn't I guess this is one of the

653
00:28:07,130 --> 00:28:09,409
challenges with using something like

654
00:28:09,409 --> 00:28:11,299
this is one of the reasons I wanted to

655
00:28:11,299 --> 00:28:13,700
use fast AI for this course is it's much

656
00:28:13,700 --> 00:28:16,340
harder to screw things up so I don't

657
00:28:16,340 --> 00:28:17,419
know if I screwed something up or

658
00:28:17,419 --> 00:28:18,169
somebody else did

659
00:28:18,169 --> 00:28:23,270
yes you know this is you've seen the

660
00:28:23,270 --> 00:28:25,400
chance to float back end yeah yeah and

661
00:28:25,400 --> 00:28:28,340
if you want to run this to try it out

662
00:28:28,340 --> 00:28:31,850
yourself you just can just go pip

663
00:28:31,850 --> 00:28:39,020
install tensorflow - GPU Kerris okay

664
00:28:39,020 --> 00:28:40,700
because it's not part of the faster I

665
00:28:40,700 --> 00:28:45,049
environment by default but that should

666
00:28:45,049 --> 00:28:47,149
be all you need to do to get that

667
00:28:47,149 --> 00:28:55,610
working so then there isn't a concept of

668
00:28:55,610 --> 00:28:58,130
like layer groups or differential

669
00:28:58,130 --> 00:29:00,230
learning rates or partial unfreezing or

670
00:29:00,230 --> 00:29:02,120
whatever so you have to decide like I

671
00:29:02,120 --> 00:29:03,470
had to print out all of the layers and

672
00:29:03,470 --> 00:29:06,380
decide manually how many I wanted to

673
00:29:06,380 --> 00:29:08,539
fine-tune so I decided to fine-tune

674
00:29:08,539 --> 00:29:10,640
everything from a layer 140 onwards so

675
00:29:10,640 --> 00:29:11,809
that's why I just looked through like

676
00:29:11,809 --> 00:29:14,510
this after you change that you have to

677
00:29:14,510 --> 00:29:17,210
recompile the model and then after that

678
00:29:17,210 --> 00:29:19,610
I then ran another step and again I

679
00:29:19,610 --> 00:29:20,750
don't know what happened here the

680
00:29:20,750 --> 00:29:22,429
accuracy of the training set stayed

681
00:29:22,429 --> 00:29:24,169
about the same but the validation set

682
00:29:24,169 --> 00:29:27,830
totally fill in the hole and I mean the

683
00:29:27,830 --> 00:29:29,779
main thing to notice even if we put

684
00:29:29,779 --> 00:29:33,409
aside the validation set we're getting I

685
00:29:33,409 --> 00:29:35,299
mean I guess the main thing is there's a

686
00:29:35,299 --> 00:29:36,740
hell of a lot more code here which is

687
00:29:36,740 --> 00:29:39,230
kind of annoying but also the

688
00:29:39,230 --> 00:29:40,940
performance is very different so where

689
00:29:40,940 --> 00:29:42,860
else is here even on the training set

690
00:29:42,860 --> 00:29:46,070
we're getting like 97% after four epochs

691
00:29:46,070 --> 00:29:48,529
that took a total of about eight minutes

692
00:29:48,529 --> 00:29:53,899
you know over here we had 99.5% on the

693
00:29:53,899 --> 00:29:57,169
validation set and it ran a lot faster

694
00:29:57,169 --> 00:30:02,380
so I was like four or five minutes right

695
00:30:02,380 --> 00:30:06,870
so

696
00:30:06,870 --> 00:30:09,190
depending on what you do particularly if

697
00:30:09,190 --> 00:30:10,809
you end up wanting to deploy stuff to

698
00:30:10,809 --> 00:30:14,830
mobile devices at the moment the kind of

699
00:30:14,830 --> 00:30:17,620
PI torch on mobile situation is very

700
00:30:17,620 --> 00:30:19,690
early so you may find yourself wanting

701
00:30:19,690 --> 00:30:21,760
to use tensorflow or you may work for a

702
00:30:21,760 --> 00:30:23,770
company that's kind of settled on

703
00:30:23,770 --> 00:30:27,070
tensorflow so if you need to convert

704
00:30:27,070 --> 00:30:29,080
something like redo something you've

705
00:30:29,080 --> 00:30:31,929
learnt here intensive flow you probably

706
00:30:31,929 --> 00:30:34,510
want to do it with care us but just

707
00:30:34,510 --> 00:30:37,000
recognize you know it's got to take a

708
00:30:37,000 --> 00:30:40,630
bit more work to get there and by

709
00:30:40,630 --> 00:30:43,000
default it's much harder to get I mean I

710
00:30:43,000 --> 00:30:45,460
to get the same state of the out results

711
00:30:45,460 --> 00:30:46,929
you get the faster I you'd have to like

712
00:30:46,929 --> 00:30:49,690
replicate all of the state-of-the-art

713
00:30:49,690 --> 00:30:52,090
algorithms that are in first a nice so

714
00:30:52,090 --> 00:30:54,669
it's hard to get the same level of

715
00:30:54,669 --> 00:30:57,130
results but you can see the basic ideas

716
00:30:57,130 --> 00:31:01,600
are similar okay and it's certainly it's

717
00:31:01,600 --> 00:31:03,970
certainly possible you know like there's

718
00:31:03,970 --> 00:31:05,860
nothing I'm doing in fast AI that like

719
00:31:05,860 --> 00:31:08,110
would be impossible but like you'd have

720
00:31:08,110 --> 00:31:09,910
to implement stochastic gradient percent

721
00:31:09,910 --> 00:31:11,640
with restarts you would have to

722
00:31:11,640 --> 00:31:14,169
implement differential learning rates

723
00:31:14,169 --> 00:31:16,360
you would have to implement batch norm

724
00:31:16,360 --> 00:31:18,700
freezing which you probably don't want

725
00:31:18,700 --> 00:31:20,799
to do I know and well that's not quite

726
00:31:20,799 --> 00:31:22,179
true I think somewhat one person at

727
00:31:22,179 --> 00:31:24,160
least on the forum is attempting to

728
00:31:24,160 --> 00:31:26,559
create a chaos compatible version of or

729
00:31:26,559 --> 00:31:28,179
a tensorflow compatible version of fast

730
00:31:28,179 --> 00:31:30,669
AI which I think I hope will get there I

731
00:31:30,669 --> 00:31:32,559
actually spoke to Google about this a

732
00:31:32,559 --> 00:31:33,820
few weeks ago and they're very

733
00:31:33,820 --> 00:31:36,040
interested in getting faster i ported to

734
00:31:36,040 --> 00:31:38,590
tensorflow so maybe by the time you're

735
00:31:38,590 --> 00:31:40,059
looking at this on the mooc maybe that

736
00:31:40,059 --> 00:31:43,270
will exist I certainly hope so we will

737
00:31:43,270 --> 00:31:46,390
see hey wait

738
00:31:46,390 --> 00:31:49,390
so Karis is Karis intensive flow was

739
00:31:49,390 --> 00:31:53,370
certainly not you know

740
00:31:53,370 --> 00:31:55,590
that difficult to handle and so I don't

741
00:31:55,590 --> 00:31:56,820
think you should worry if you're told

742
00:31:56,820 --> 00:31:59,010
you have to learn them after this course

743
00:31:59,010 --> 00:32:00,780
for some reason even let me take you a

744
00:32:00,780 --> 00:32:08,309
couple of days I'm sure so that's kind

745
00:32:08,309 --> 00:32:11,120
of most of the stuff you would need to

746
00:32:11,120 --> 00:32:13,380
kind of complete this this kind of

747
00:32:13,380 --> 00:32:15,030
assignment from last week which was like

748
00:32:15,030 --> 00:32:17,040
try to do everything you've seen already

749
00:32:17,040 --> 00:32:19,890
but on the dog of reinstated said just

750
00:32:19,890 --> 00:32:23,309
to remind you that kind of last few

751
00:32:23,309 --> 00:32:26,600
minutes of last week's lesson I show you

752
00:32:26,600 --> 00:32:30,870
how to do much of that including like

753
00:32:30,870 --> 00:32:32,730
how I actually explored the data to find

754
00:32:32,730 --> 00:32:35,910
out like what the classes were and how

755
00:32:35,910 --> 00:32:38,100
big the images were and stuff like that

756
00:32:38,100 --> 00:32:39,960
right so if you've forgotten that or

757
00:32:39,960 --> 00:32:42,090
didn't quite follow at all last week

758
00:32:42,090 --> 00:32:44,100
check out the video from last week to

759
00:32:44,100 --> 00:32:47,040
see one thing that we didn't talk about

760
00:32:47,040 --> 00:32:49,530
is how do you actually submit to Carol

761
00:32:49,530 --> 00:32:51,260
so how do you actually get predictions

762
00:32:51,260 --> 00:32:53,520
so I just wanted to show you that last

763
00:32:53,520 --> 00:32:56,429
piece as well and on the wiki thread

764
00:32:56,429 --> 00:32:58,230
this week I've already put a little

765
00:32:58,230 --> 00:33:01,500
image of this to show you these days but

766
00:33:01,500 --> 00:33:04,860
if you go to the kaggle website for

767
00:33:04,860 --> 00:33:06,210
every competition there's a section

768
00:33:06,210 --> 00:33:08,220
called evaluation and they tell you what

769
00:33:08,220 --> 00:33:10,230
it's a bit and so I just copied and

770
00:33:10,230 --> 00:33:13,590
pasted these two lines from from there

771
00:33:13,590 --> 00:33:15,780
and so it says we're expected to submit

772
00:33:15,780 --> 00:33:18,559
a file where the first line contains the

773
00:33:18,559 --> 00:33:22,140
the work the word ID and then a comma

774
00:33:22,140 --> 00:33:23,910
separated list of all of the possible

775
00:33:23,910 --> 00:33:26,190
dog breeds and then every line after

776
00:33:26,190 --> 00:33:29,090
that will contain the idea itself

777
00:33:29,090 --> 00:33:31,860
followed by all the probabilities of all

778
00:33:31,860 --> 00:33:35,610
the different dog breeds so how do you

779
00:33:35,610 --> 00:33:39,660
create that so recognize that inside our

780
00:33:39,660 --> 00:33:42,960
data object there's a dot classes which

781
00:33:42,960 --> 00:33:46,140
has got in alphabetical order all of the

782
00:33:46,140 --> 00:33:52,200
four other classes and then so it's got

783
00:33:52,200 --> 00:33:53,850
all of the different classes and then

784
00:33:53,850 --> 00:33:58,380
inside data dot test data set just yes

785
00:33:58,380 --> 00:34:00,240
you can also see there's all the file

786
00:34:00,240 --> 00:34:05,010
names so and just to remind you dogs and

787
00:34:05,010 --> 00:34:05,490
cats

788
00:34:05,490 --> 00:34:06,690
sorry

789
00:34:06,690 --> 00:34:11,579
cats dog breeds was not provided in the

790
00:34:11,579 --> 00:34:13,679
kind of care our style format where the

791
00:34:13,679 --> 00:34:15,929
dogs and cats from different folders but

792
00:34:15,929 --> 00:34:18,270
instead it was provided as a CSV file of

793
00:34:18,270 --> 00:34:21,059
labels right so when you get a CSV file

794
00:34:21,059 --> 00:34:25,109
of labels you use image classifier data

795
00:34:25,109 --> 00:34:27,859
from CSV rather than image classifier

796
00:34:27,859 --> 00:34:31,799
data from parts there isn't an

797
00:34:31,799 --> 00:34:33,720
equivalent in care us so you'll see like

798
00:34:33,720 --> 00:34:36,149
on the cattle forums people share

799
00:34:36,149 --> 00:34:38,129
scripts for how to convert it to a care

800
00:34:38,129 --> 00:34:40,139
our style folders but in our case we

801
00:34:40,139 --> 00:34:41,250
don't have to we just go image

802
00:34:41,250 --> 00:34:44,039
classifier data from CSV passing in that

803
00:34:44,039 --> 00:34:49,020
CSV file and so the CSV file will you

804
00:34:49,020 --> 00:34:51,210
know has automatically told the data you

805
00:34:51,210 --> 00:34:54,569
know what the masses are and then also

806
00:34:54,569 --> 00:34:57,750
we can see from the folder of test

807
00:34:57,750 --> 00:35:00,200
images what the file names of those are

808
00:35:00,200 --> 00:35:03,740
so with those two pieces of information

809
00:35:03,740 --> 00:35:07,020
we're ready to go so I always think it's

810
00:35:07,020 --> 00:35:09,660
a good idea to use TTA as you saw with

811
00:35:09,660 --> 00:35:11,549
that dogs and cats example just now it

812
00:35:11,549 --> 00:35:13,500
can really improve things particularly

813
00:35:13,500 --> 00:35:17,099
when your model is less good so I can

814
00:35:17,099 --> 00:35:25,549
say learn dot t ta and if you pass in

815
00:35:25,549 --> 00:35:29,630
yeah if you pass in is test equals true

816
00:35:29,630 --> 00:35:32,130
then it's going to give you predictions

817
00:35:32,130 --> 00:35:33,599
on the test set rather than the

818
00:35:33,599 --> 00:35:35,369
validation set okay

819
00:35:35,369 --> 00:35:39,480
and now obviously we can't now get an

820
00:35:39,480 --> 00:35:40,980
accuracy or anything because by

821
00:35:40,980 --> 00:35:42,930
definition we don't know the labels for

822
00:35:42,930 --> 00:35:48,650
the test set right so by default most

823
00:35:48,650 --> 00:35:51,240
high-touch models give you back the log

824
00:35:51,240 --> 00:35:54,539
of the predictions so then we just have

825
00:35:54,539 --> 00:35:57,150
to go X of that to get back out

826
00:35:57,150 --> 00:35:59,760
probabilities so in this case the test

827
00:35:59,760 --> 00:36:01,140
set had ten thousand three hundred and

828
00:36:01,140 --> 00:36:03,809
fifty seven images in it and there are

829
00:36:03,809 --> 00:36:06,539
120 possible breeds all right so we get

830
00:36:06,539 --> 00:36:10,529
back a matrix of of that size and so we

831
00:36:10,529 --> 00:36:12,890
now need to turn that into something

832
00:36:12,890 --> 00:36:16,020
that looks like this

833
00:36:16,020 --> 00:36:17,460
and so the easiest way to do that is

834
00:36:17,460 --> 00:36:19,470
with pandas if you're not familiar with

835
00:36:19,470 --> 00:36:21,360
pandas there's lots of information

836
00:36:21,360 --> 00:36:23,340
online about it or check out the machine

837
00:36:23,340 --> 00:36:24,630
learning course intro to machine

838
00:36:24,630 --> 00:36:26,400
learning that we have where we do lots

839
00:36:26,400 --> 00:36:28,320
of stuff with pandas but basically we

840
00:36:28,320 --> 00:36:30,990
can describe PD data frame and pass in

841
00:36:30,990 --> 00:36:33,630
that matrix and then we can say the

842
00:36:33,630 --> 00:36:36,060
names of the columns are equal to data

843
00:36:36,060 --> 00:36:38,640
duck classes and then finally we can

844
00:36:38,640 --> 00:36:41,670
insert a new column at position 0 called

845
00:36:41,670 --> 00:36:45,540
ID that contains the file names but

846
00:36:45,540 --> 00:36:47,300
you'll notice that the file names

847
00:36:47,300 --> 00:36:51,210
contain five letters at the end I start

848
00:36:51,210 --> 00:36:52,800
we don't want and four letters at the

849
00:36:52,800 --> 00:36:56,850
end we don't want so I just subset in

850
00:36:56,850 --> 00:37:02,790
like so right so at that point I've got

851
00:37:02,790 --> 00:37:06,510
a data frame that looks like this which

852
00:37:06,510 --> 00:37:10,410
is what we want so you can now call a

853
00:37:10,410 --> 00:37:13,470
data frame data so social cues dated DF

854
00:37:13,470 --> 00:37:24,000
not des let's fix it now data frame okay

855
00:37:24,000 --> 00:37:26,880
so you can now call data frame to CSV

856
00:37:26,880 --> 00:37:30,900
and quite often you'll find these files

857
00:37:30,900 --> 00:37:33,180
actually get quite big so it's a good

858
00:37:33,180 --> 00:37:35,280
idea to say compression equals gzip and

859
00:37:35,280 --> 00:37:37,080
that'll zip it up on the server for you

860
00:37:37,080 --> 00:37:41,180
and that's going to create a zipped up

861
00:37:41,180 --> 00:37:44,280
CSV file on the server on wherever

862
00:37:44,280 --> 00:37:46,230
you're running is Jupiter notebook so

863
00:37:46,230 --> 00:37:47,790
you need apps that you now need to get

864
00:37:47,790 --> 00:37:49,350
that back to your computer so you can

865
00:37:49,350 --> 00:37:53,250
upload it or you can use carol CLA so

866
00:37:53,250 --> 00:37:55,620
you can type kgs submit and do it that

867
00:37:55,620 --> 00:37:55,980
way

868
00:37:55,980 --> 00:37:58,950
I generally download it to my computer

869
00:37:58,950 --> 00:38:00,660
so like how often back to this lab

870
00:38:00,660 --> 00:38:04,170
double check it all looks ok so to do

871
00:38:04,170 --> 00:38:05,610
that there's a cool little theme called

872
00:38:05,610 --> 00:38:09,300
file link and if you run file link with

873
00:38:09,300 --> 00:38:11,880
a path on your server it gives you back

874
00:38:11,880 --> 00:38:14,970
a URL which you can click on and it will

875
00:38:14,970 --> 00:38:18,390
download that file from the server onto

876
00:38:18,390 --> 00:38:21,390
your computer so if I click on that now

877
00:38:21,390 --> 00:38:25,290
I can go ahead and save it

878
00:38:25,290 --> 00:38:33,490
and then I can see in my downloads

879
00:38:33,490 --> 00:38:40,330
there it is here's my submission file

880
00:38:40,330 --> 00:38:44,560
they want to open their yeah and as you

881
00:38:44,560 --> 00:38:45,820
can see it's exactly what I asked for

882
00:38:45,820 --> 00:38:49,060
there's my ID and 120 different dog

883
00:38:49,060 --> 00:38:51,820
breeds and then here's my first row

884
00:38:51,820 --> 00:38:54,040
containing the file name and the 120

885
00:38:54,040 --> 00:38:56,470
different probabilities okay so then you

886
00:38:56,470 --> 00:38:58,120
can go ahead and submit that to cattle

887
00:38:58,120 --> 00:39:00,700
through their through their regular form

888
00:39:00,700 --> 00:39:03,370
and so this is also a good way you can

889
00:39:03,370 --> 00:39:05,910
see we've now got a good way of both

890
00:39:05,910 --> 00:39:08,380
grabbing any file off the internet and

891
00:39:08,380 --> 00:39:10,990
getting a to our AWS instance or paper

892
00:39:10,990 --> 00:39:15,160
space or whatever by using the cool

893
00:39:15,160 --> 00:39:17,170
little extension in chrome and we've

894
00:39:17,170 --> 00:39:18,850
also got a way of grabbing stuff off our

895
00:39:18,850 --> 00:39:22,200
server easily those of you that are more

896
00:39:22,200 --> 00:39:24,940
command line oriented you can also use

897
00:39:24,940 --> 00:39:28,030
SCP of course but I kind of like doing

898
00:39:28,030 --> 00:39:31,270
everything through the notebook all

899
00:39:31,270 --> 00:39:32,890
right

900
00:39:32,890 --> 00:39:34,960
one other question I had during the week

901
00:39:34,960 --> 00:39:37,810
was like what if I want to just get a

902
00:39:37,810 --> 00:39:42,910
single a single file that I want to you

903
00:39:42,910 --> 00:39:45,610
know get a prediction for so for example

904
00:39:45,610 --> 00:39:47,290
you know maybe I want to get this first

905
00:39:47,290 --> 00:39:49,300
file from my validation set

906
00:39:49,300 --> 00:39:52,390
so there's its name so you can always

907
00:39:52,390 --> 00:39:54,250
look at a file just by calling image dot

908
00:39:54,250 --> 00:39:59,350
open that just uses regular - imaging

909
00:39:59,350 --> 00:40:04,240
library and so what you can do is

910
00:40:04,240 --> 00:40:05,800
there's actually I'll show you the

911
00:40:05,800 --> 00:40:09,400
shortest version you can just call learn

912
00:40:09,400 --> 00:40:16,230
predict array passing in your your image

913
00:40:16,230 --> 00:40:20,010
okay now the image needs to have been

914
00:40:20,010 --> 00:40:21,390
transformed

915
00:40:21,390 --> 00:40:24,360
so you've seen transform Trent

916
00:40:24,360 --> 00:40:27,760
transforms from model before normally we

917
00:40:27,760 --> 00:40:29,890
just put put it all in one variable but

918
00:40:29,890 --> 00:40:31,270
actually behind the scenes it was

919
00:40:31,270 --> 00:40:32,500
returning to things

920
00:40:32,500 --> 00:40:34,480
it was returning training transforms and

921
00:40:34,480 --> 00:40:36,310
validation transforms so I can actually

922
00:40:36,310 --> 00:40:38,770
split them apart and so here you can see

923
00:40:38,770 --> 00:40:41,110
I'm actually applying example my

924
00:40:41,110 --> 00:40:43,180
training transforms or probably more

925
00:40:43,180 --> 00:40:45,280
likely that would apply validation

926
00:40:45,280 --> 00:40:48,820
transforms that gives me back an array

927
00:40:48,820 --> 00:40:51,220
containing the image the transformed

928
00:40:51,220 --> 00:40:55,440
image which I can then past

929
00:40:55,440 --> 00:40:58,210
everything that gets passed to or

930
00:40:58,210 --> 00:41:01,570
returned from bottles is generally

931
00:41:01,570 --> 00:41:03,460
assumed to be a mini batch right it's

932
00:41:03,460 --> 00:41:05,320
generally assumed to be a bunch of

933
00:41:05,320 --> 00:41:07,930
images so we'll talk more about some

934
00:41:07,930 --> 00:41:10,690
numpy tricks later but basically in this

935
00:41:10,690 --> 00:41:13,330
case we only have one image so we have

936
00:41:13,330 --> 00:41:15,250
to turn that into a mini batch of images

937
00:41:15,250 --> 00:41:16,930
so in other words we need to create a

938
00:41:16,930 --> 00:41:21,490
tensor that basically is not just rows

939
00:41:21,490 --> 00:41:24,130
by columns by channels but it's number

940
00:41:24,130 --> 00:41:26,230
of image by rows by columns by channels

941
00:41:26,230 --> 00:41:29,080
and as one image so it's basically

942
00:41:29,080 --> 00:41:31,330
becomes a 4 dimensional tensor so

943
00:41:31,330 --> 00:41:33,430
there's a cool little trick in numpy

944
00:41:33,430 --> 00:41:36,610
that if you index into an array with

945
00:41:36,610 --> 00:41:39,880
none that basically adds additional unit

946
00:41:39,880 --> 00:41:41,350
access to the start so it turns it from

947
00:41:41,350 --> 00:41:44,080
an image into a mini batch of one images

948
00:41:44,080 --> 00:41:46,480
and so that's why we had to do that so

949
00:41:46,480 --> 00:41:48,910
if you basically find you're trying to

950
00:41:48,910 --> 00:41:52,630
do things with a single image with any

951
00:41:52,630 --> 00:41:55,540
kind of Pi torch or fast AI thing this

952
00:41:55,540 --> 00:41:56,980
is just something you might you might

953
00:41:56,980 --> 00:41:59,400
find it says like expecting four

954
00:41:59,400 --> 00:42:02,080
dimensions only got three it probably

955
00:42:02,080 --> 00:42:04,470
means that or if you get back a return

956
00:42:04,470 --> 00:42:07,330
value from something that has like some

957
00:42:07,330 --> 00:42:10,120
weird first access that's probably why

958
00:42:10,120 --> 00:42:11,350
it's probably giving you like back a

959
00:42:11,350 --> 00:42:13,690
mini batch okay and so we'll learn a lot

960
00:42:13,690 --> 00:42:15,040
more about this but it's just something

961
00:42:15,040 --> 00:42:22,080
to be aware of okay so that's kind of

962
00:42:22,080 --> 00:42:28,140
everything you need to do in practice so

963
00:42:28,140 --> 00:42:29,580
now we're going to kind of get into a

964
00:42:29,580 --> 00:42:32,460
little bit of theory what's actually

965
00:42:32,460 --> 00:42:34,500
going on behind the scenes with these

966
00:42:34,500 --> 00:42:36,990
convolutional neural networks and you

967
00:42:36,990 --> 00:42:43,850
might remember it back in Lesson one we

968
00:42:43,850 --> 00:42:48,510
actually saw our first little bit of

969
00:42:48,510 --> 00:42:51,720
theory which we stole from this

970
00:42:51,720 --> 00:42:53,670
fantastic websites a toaster dot IO

971
00:42:53,670 --> 00:42:56,730
either explained visually and we learnt

972
00:42:56,730 --> 00:42:58,890
that a that a convolution is something

973
00:42:58,890 --> 00:43:01,320
where we basically have a little matrix

974
00:43:01,320 --> 00:43:03,870
in deep learning nearly always three by

975
00:43:03,870 --> 00:43:06,600
three a little matrix that we basically

976
00:43:06,600 --> 00:43:09,030
multiply every element of that matrix by

977
00:43:09,030 --> 00:43:10,770
every element of a three by three

978
00:43:10,770 --> 00:43:14,010
section of an image add them all

979
00:43:14,010 --> 00:43:16,440
together to get the result of that

980
00:43:16,440 --> 00:43:19,670
convolution at one point all right now

981
00:43:19,670 --> 00:43:22,260
let's see how that all gets turned

982
00:43:22,260 --> 00:43:26,370
together to create these these various

983
00:43:26,370 --> 00:43:29,370
layers that we saw in the the Zeiler and

984
00:43:29,370 --> 00:43:31,860
burgers paper and to do that again I'm

985
00:43:31,860 --> 00:43:33,300
going to steal off somebody who's much

986
00:43:33,300 --> 00:43:35,010
smarter than I am

987
00:43:35,010 --> 00:43:38,370
we're going to steal from a guy called

988
00:43:38,370 --> 00:43:39,570
Ottavia good

989
00:43:39,570 --> 00:43:42,480
Ottavia oh good was the guy who created

990
00:43:42,480 --> 00:43:45,540
Word Lens which nowadays is part of

991
00:43:45,540 --> 00:43:47,460
Google Translate if I'm Google Translate

992
00:43:47,460 --> 00:43:48,870
you've ever like done that thing where

993
00:43:48,870 --> 00:43:52,740
you point your camera at something at

994
00:43:52,740 --> 00:43:54,600
something which has any kind of foreign

995
00:43:54,600 --> 00:43:56,190
language on it and in real-time it

996
00:43:56,190 --> 00:43:58,080
overlays it with the translation that

997
00:43:58,080 --> 00:44:01,440
was a views company that built that and

998
00:44:01,440 --> 00:44:03,630
so Tokyo was kind enough to share this

999
00:44:03,630 --> 00:44:05,130
fantastic video

1000
00:44:05,130 --> 00:44:09,240
he created he's at Google now and I want

1001
00:44:09,240 --> 00:44:10,350
to kind of step you through works I

1002
00:44:10,350 --> 00:44:12,030
think it explains really really well

1003
00:44:12,030 --> 00:44:14,310
what's going on and then after we look

1004
00:44:14,310 --> 00:44:15,930
at the video we're going to see how to

1005
00:44:15,930 --> 00:44:18,710
implement the whole a whole sequence of

1006
00:44:18,710 --> 00:44:20,940
kintyre set of layers of convolution on

1007
00:44:20,940 --> 00:44:23,400
your network in Microsoft Excel

1008
00:44:23,400 --> 00:44:26,160
so with you're a visual learner or a

1009
00:44:26,160 --> 00:44:27,720
spreadsheet learner hopefully you'll be

1010
00:44:27,720 --> 00:44:29,880
able to understand all this okay so

1011
00:44:29,880 --> 00:44:32,190
we're going to start with an image and

1012
00:44:32,190 --> 00:44:33,540
something that we're going to do later

1013
00:44:33,540 --> 00:44:35,160
in the course is we're going to learn to

1014
00:44:35,160 --> 00:44:36,990
nice digits so we'll do it like end to

1015
00:44:36,990 --> 00:44:39,240
end we'll do the whole thing so this is

1016
00:44:39,240 --> 00:44:41,610
pretty similar so we're going to try and

1017
00:44:41,610 --> 00:44:44,100
recognize in this case letters so here's

1018
00:44:44,100 --> 00:44:46,290
an A which obviously it's actually a

1019
00:44:46,290 --> 00:44:50,010
grid of numbers right and so there's the

1020
00:44:50,010 --> 00:44:51,990
creative numbers and so what we do is we

1021
00:44:51,990 --> 00:44:55,650
take our first convolutional filter so

1022
00:44:55,650 --> 00:44:57,150
we're assuming this is all this is

1023
00:44:57,150 --> 00:44:58,770
assuming that these are already learned

1024
00:44:58,770 --> 00:45:01,020
right and you can see this point it's

1025
00:45:01,020 --> 00:45:03,240
got wiped down the right-hand side right

1026
00:45:03,240 --> 00:45:05,010
and black down the left so it's like

1027
00:45:05,010 --> 00:45:07,410
zero zero zero maybe negative 1 negative

1028
00:45:07,410 --> 00:45:10,350
1 negative 1 0 0 0 1 1 1 and so we're

1029
00:45:10,350 --> 00:45:13,140
taking each 3x3 part of the image and

1030
00:45:13,140 --> 00:45:16,950
multiplying it by that 3x3 matrix not as

1031
00:45:16,950 --> 00:45:18,990
a matrix product that an element-wise

1032
00:45:18,990 --> 00:45:21,330
product and so you can see what happens

1033
00:45:21,330 --> 00:45:24,450
is everywhere where the the white edge

1034
00:45:24,450 --> 00:45:28,140
is matching the edge of the a and the

1035
00:45:28,140 --> 00:45:30,180
black edge isn't we're getting green

1036
00:45:30,180 --> 00:45:32,340
we're getting a positive and everywhere

1037
00:45:32,340 --> 00:45:33,870
where it's the opposite we're getting a

1038
00:45:33,870 --> 00:45:35,970
negative we're getting a red right and

1039
00:45:35,970 --> 00:45:39,180
so that's the first filter creating the

1040
00:45:39,180 --> 00:45:41,850
first that the result of the first

1041
00:45:41,850 --> 00:45:44,760
kernel right and so here's a new kernel

1042
00:45:44,760 --> 00:45:46,620
this one is it's got a white stripe

1043
00:45:46,620 --> 00:45:49,380
along the top right so we literally scan

1044
00:45:49,380 --> 00:45:52,760
through every 3x3 part of the matrix

1045
00:45:52,760 --> 00:45:55,890
multiplying those 3 bits of the a the

1046
00:45:55,890 --> 00:45:57,450
neighbors of the a by the 9 bits as a

1047
00:45:57,450 --> 00:45:59,940
filter to find out whether it's red or

1048
00:45:59,940 --> 00:46:02,610
green and how red or green it is ok and

1049
00:46:02,610 --> 00:46:04,860
so this is assuming we had two filters

1050
00:46:04,860 --> 00:46:07,080
one was a bottom edge one was a left

1051
00:46:07,080 --> 00:46:08,970
edge and you can see here the top edge

1052
00:46:08,970 --> 00:46:10,800
not surprisingly it's red here so a

1053
00:46:10,800 --> 00:46:12,690
bottom edge was red here and green here

1054
00:46:12,690 --> 00:46:15,570
the right edge right here in green here

1055
00:46:15,570 --> 00:46:17,880
and then in the next step we add a

1056
00:46:17,880 --> 00:46:20,460
non-linearity ok the rectified linear

1057
00:46:20,460 --> 00:46:23,040
unit which literally means strongly the

1058
00:46:23,040 --> 00:46:26,520
negatives so here the Reds all gone okay

1059
00:46:26,520 --> 00:46:28,290
so here's layer 1 the input

1060
00:46:28,290 --> 00:46:30,780
here's layup to the result of 2

1061
00:46:30,780 --> 00:46:33,660
convolutional filters here's layer 3

1062
00:46:33,660 --> 00:46:36,180
which is which is throw away all of the

1063
00:46:36,180 --> 00:46:38,190
red stuff and that's called a rectified

1064
00:46:38,190 --> 00:46:41,160
linear unit and then layer 4 is

1065
00:46:41,160 --> 00:46:43,890
something called a max pull on a layer 4

1066
00:46:43,890 --> 00:46:48,090
we replace every 2 by 2 part of this

1067
00:46:48,090 --> 00:46:48,700
grid

1068
00:46:48,700 --> 00:46:51,339
and we replace it with its maximum mat

1069
00:46:51,339 --> 00:46:53,040
so it basically makes it half the size

1070
00:46:53,040 --> 00:46:55,359
it's basically the same thing but half

1071
00:46:55,359 --> 00:46:57,700
the size and then we can go through and

1072
00:46:57,700 --> 00:46:59,619
do exactly the same thing we can have

1073
00:46:59,619 --> 00:47:02,619
some new filter three by three filter

1074
00:47:02,619 --> 00:47:04,510
that we put through each of the two

1075
00:47:04,510 --> 00:47:08,680
results of the previous layer okay and

1076
00:47:08,680 --> 00:47:10,510
again we can throw away the red bits

1077
00:47:10,510 --> 00:47:13,030
right so get rid of all the negatives so

1078
00:47:13,030 --> 00:47:14,349
we just keep the positives that's called

1079
00:47:14,349 --> 00:47:18,299
applying a rectified linear unit and

1080
00:47:18,299 --> 00:47:21,190
that gets us to our next layer of this

1081
00:47:21,190 --> 00:47:23,530
convolutional neural network so you can

1082
00:47:23,530 --> 00:47:26,530
see that by you know at this layer back

1083
00:47:26,530 --> 00:47:29,109
here it was kind of very interpretive

1084
00:47:29,109 --> 00:47:30,490
all it's like we've either got bottom

1085
00:47:30,490 --> 00:47:33,010
edges or left edges but then the next

1086
00:47:33,010 --> 00:47:36,130
layer was combining the results of

1087
00:47:36,130 --> 00:47:37,660
convolutions so it's starting to become

1088
00:47:37,660 --> 00:47:40,089
a lot less clear like intuitively what's

1089
00:47:40,089 --> 00:47:42,130
happening but it's doing the same thing

1090
00:47:42,130 --> 00:47:45,130
and then we do another max pull right so

1091
00:47:45,130 --> 00:47:48,819
we replace every 2x2 or 3x3 section with

1092
00:47:48,819 --> 00:47:52,180
a single digit so here this 2x2 it's all

1093
00:47:52,180 --> 00:47:53,799
black so we replaced it with a black

1094
00:47:53,799 --> 00:47:56,380
right and then we go and we take that

1095
00:47:56,380 --> 00:48:00,430
and we we compare it to basically a kind

1096
00:48:00,430 --> 00:48:03,099
of a template of what we would expect to

1097
00:48:03,099 --> 00:48:05,140
see if it was an a it was a B but the

1098
00:48:05,140 --> 00:48:07,599
see it was d give it an E and we see how

1099
00:48:07,599 --> 00:48:10,690
closely it matches and we can do it in

1100
00:48:10,690 --> 00:48:12,309
exactly the same way we can multiply

1101
00:48:12,309 --> 00:48:15,700
every one of the values in this four by

1102
00:48:15,700 --> 00:48:18,579
eight matrix with every one of the four

1103
00:48:18,579 --> 00:48:20,650
by eight in this one and this one and

1104
00:48:20,650 --> 00:48:22,390
this one and we add we just add them

1105
00:48:22,390 --> 00:48:24,579
together to say like how often does it

1106
00:48:24,579 --> 00:48:26,859
match versus how often does it not match

1107
00:48:26,859 --> 00:48:29,799
and then that could be converted to give

1108
00:48:29,799 --> 00:48:33,910
us a percentage probability that this is

1109
00:48:33,910 --> 00:48:35,530
a no so in this case this particular

1110
00:48:35,530 --> 00:48:39,460
template matched well with a so notice

1111
00:48:39,460 --> 00:48:41,619
we're not doing an each training here

1112
00:48:41,619 --> 00:48:43,540
right this is how it would work if we

1113
00:48:43,540 --> 00:48:46,119
have a pre trained model all right so

1114
00:48:46,119 --> 00:48:48,040
when we download a pre trained imagenet

1115
00:48:48,040 --> 00:48:50,170
model off the internet and isn't on an

1116
00:48:50,170 --> 00:48:52,690
image without any changing to it this is

1117
00:48:52,690 --> 00:48:54,460
what's happening or if we take a model

1118
00:48:54,460 --> 00:48:56,619
that you've trained and you're applying

1119
00:48:56,619 --> 00:48:58,569
it to some test set or for some new

1120
00:48:58,569 --> 00:49:00,790
image this is what it's doing all right

1121
00:49:00,790 --> 00:49:02,500
as it's basically taking it through it

1122
00:49:02,500 --> 00:49:04,870
buying a convolution to each layer to

1123
00:49:04,870 --> 00:49:08,710
each multiple convolutional filters to

1124
00:49:08,710 --> 00:49:12,790
each layer and then doing the rectified

1125
00:49:12,790 --> 00:49:14,680
linear unit so throw away the negatives

1126
00:49:14,680 --> 00:49:18,370
and then do the max pull and then repeat

1127
00:49:18,370 --> 00:49:21,130
that a bunch of times and so then we can

1128
00:49:21,130 --> 00:49:24,520
do it with a new letter A or letter B or

1129
00:49:24,520 --> 00:49:28,030
whatever and keep going through that

1130
00:49:28,030 --> 00:49:31,330
process right so as you can see that's

1131
00:49:31,330 --> 00:49:33,790
far nice the visualization thing and I

1132
00:49:33,790 --> 00:49:35,200
could have created because I'm not at a

1133
00:49:35,200 --> 00:49:37,810
vo so thanks to him for sharing this

1134
00:49:37,810 --> 00:49:40,420
with us because it's totally awesome

1135
00:49:40,420 --> 00:49:42,520
he actually this is not done by hand he

1136
00:49:42,520 --> 00:49:43,480
actually wrote a piece of computer

1137
00:49:43,480 --> 00:49:45,100
software to actually do these

1138
00:49:45,100 --> 00:49:47,410
convolutions this is actually being

1139
00:49:47,410 --> 00:49:49,870
actually being done dynamically which is

1140
00:49:49,870 --> 00:49:52,480
pretty cool so I'm more of a spreadsheet

1141
00:49:52,480 --> 00:49:56,350
guy personally I'm a simple person so

1142
00:49:56,350 --> 00:49:58,060
here is the same thing now in

1143
00:49:58,060 --> 00:50:00,460
spreadsheet all right and so you'll find

1144
00:50:00,460 --> 00:50:03,490
this in the github repo so you can

1145
00:50:03,490 --> 00:50:06,610
either get clone the repo to your own

1146
00:50:06,610 --> 00:50:08,950
computer open up the spreadsheet or you

1147
00:50:08,950 --> 00:50:11,670
can just go to github.com slash / ji and

1148
00:50:11,670 --> 00:50:21,070
click on this it's it's inside if you go

1149
00:50:21,070 --> 00:50:23,830
to our repo and just go to courses as

1150
00:50:23,830 --> 00:50:26,890
usual go to deal 1 as usual you'll see

1151
00:50:26,890 --> 00:50:29,170
there's an Excel section there okay and

1152
00:50:29,170 --> 00:50:30,820
so he lay all that so you can just

1153
00:50:30,820 --> 00:50:32,230
download them by clicking them or you

1154
00:50:32,230 --> 00:50:33,970
can clone the whole repo and we're

1155
00:50:33,970 --> 00:50:36,880
looking at cognitive example convolution

1156
00:50:36,880 --> 00:50:40,690
example all right so you can see I have

1157
00:50:40,690 --> 00:50:44,710
here an input right so in this case the

1158
00:50:44,710 --> 00:50:48,250
input is the number 7 so I grab this

1159
00:50:48,250 --> 00:50:51,310
from a dataset called m-must MN ist

1160
00:50:51,310 --> 00:50:52,600
which we'll be looking at in a lot of

1161
00:50:52,600 --> 00:50:55,290
detail and I just took one of those

1162
00:50:55,290 --> 00:50:58,120
digits at random and I put it into Excel

1163
00:50:58,120 --> 00:51:01,360
and so you can see every Hextall is

1164
00:51:01,360 --> 00:51:05,230
actually just a number between 9 1 okay

1165
00:51:05,230 --> 00:51:09,640
very often actually it'll be a bite

1166
00:51:09,640 --> 00:51:13,060
between Norton 255 or sometimes it might

1167
00:51:13,060 --> 00:51:15,250
be a float between naught and 1 it

1168
00:51:15,250 --> 00:51:16,089
doesn't really matter

1169
00:51:16,089 --> 00:51:18,789
by the time it gets to PI torch we're

1170
00:51:18,789 --> 00:51:22,449
generally dealing with floats so we if

1171
00:51:22,449 --> 00:51:24,519
one of the steps we often will take will

1172
00:51:24,519 --> 00:51:25,900
be to convert it to a number between

1173
00:51:25,900 --> 00:51:29,799
naught 1 so then you can see I've just

1174
00:51:29,799 --> 00:51:32,170
use conditional formatting in Excel to

1175
00:51:32,170 --> 00:51:34,509
kind of make the higher numbers more red

1176
00:51:34,509 --> 00:51:36,489
so you can clearly see that this is a

1177
00:51:36,489 --> 00:51:40,390
red this is a 7 but but it's just a

1178
00:51:40,390 --> 00:51:42,400
bunch of numbers that have been imported

1179
00:51:42,400 --> 00:51:49,479
into Excel okay so here's our input so

1180
00:51:49,479 --> 00:51:52,299
remember what at a via did was he then

1181
00:51:52,299 --> 00:51:55,839
applied two filters right with different

1182
00:51:55,839 --> 00:51:58,299
shapes so here I've created a filter

1183
00:51:58,299 --> 00:52:02,049
which is designed to detect top edges so

1184
00:52:02,049 --> 00:52:05,170
this is a 3 by 3 filter okay and I've

1185
00:52:05,170 --> 00:52:07,269
got ones along the top zeroes in the

1186
00:52:07,269 --> 00:52:10,119
middle minus ones at the bottom right so

1187
00:52:10,119 --> 00:52:11,999
let's take a look at an example that's

1188
00:52:11,999 --> 00:52:16,630
here right and so if I hit that - you

1189
00:52:16,630 --> 00:52:19,479
can see here highlighted this is the 3

1190
00:52:19,479 --> 00:52:22,029
by 3 part of the input that this

1191
00:52:22,029 --> 00:52:24,519
particular thing is calculating right so

1192
00:52:24,519 --> 00:52:28,239
here you can see it's got 1 1 1 are all

1193
00:52:28,239 --> 00:52:32,619
being multiplied by 1 and point 1 0 0

1194
00:52:32,619 --> 00:52:35,249
are all being multiplied by negative 1

1195
00:52:35,249 --> 00:52:37,989
okay so in other words all the positive

1196
00:52:37,989 --> 00:52:39,640
bits are getting a lot of positive the

1197
00:52:39,640 --> 00:52:41,319
negative bits are getting nearly nothing

1198
00:52:41,319 --> 00:52:44,099
at all so we end up with a high number

1199
00:52:44,099 --> 00:52:47,049
okay where else on the other side of

1200
00:52:47,049 --> 00:52:51,029
this bit of the 7 right you can see how

1201
00:52:51,029 --> 00:52:54,670
you know this is basically zeros here or

1202
00:52:54,670 --> 00:52:56,920
perhaps more interestingly on the top of

1203
00:52:56,920 --> 00:53:04,420
it okay here we've got high numbers at

1204
00:53:04,420 --> 00:53:06,039
the top but we've also got high numbers

1205
00:53:06,039 --> 00:53:08,859
at the bottom which are negating it ok

1206
00:53:08,859 --> 00:53:10,719
so you can see that the only place that

1207
00:53:10,719 --> 00:53:14,769
we end up activating is where we're

1208
00:53:14,769 --> 00:53:19,179
actually at an edge so in this case this

1209
00:53:19,179 --> 00:53:22,359
here this number 3 this is called an

1210
00:53:22,359 --> 00:53:25,660
activation ok so when I say an

1211
00:53:25,660 --> 00:53:28,990
activation I mean ah

1212
00:53:28,990 --> 00:53:33,670
at number a number that is calculated

1213
00:53:33,670 --> 00:53:38,500
and it is calculated by taking some

1214
00:53:38,500 --> 00:53:42,880
numbers from the input and applying some

1215
00:53:42,880 --> 00:53:46,000
kind of linear operation in this case a

1216
00:53:46,000 --> 00:53:48,430
convolutional kernel to calculate an

1217
00:53:48,430 --> 00:53:52,240
output right you'll notice that other

1218
00:53:52,240 --> 00:53:56,920
than going inputs multiplied by kernel

1219
00:53:56,920 --> 00:53:59,230
and summing it together

1220
00:53:59,230 --> 00:54:03,210
right so here's my some and here's my x

1221
00:54:03,210 --> 00:54:07,060
then take that and I go max of 0 comma

1222
00:54:07,060 --> 00:54:10,600
that and so that's my rectified linear

1223
00:54:10,600 --> 00:54:13,750
unit so it sounds very fancy rectified

1224
00:54:13,750 --> 00:54:15,340
linear unit but what they actually mean

1225
00:54:15,340 --> 00:54:17,710
is open up Excel and type equals max 0

1226
00:54:17,710 --> 00:54:22,720
comma C ok that's all about then you'll

1227
00:54:22,720 --> 00:54:25,480
see people in the biz so to say value a

1228
00:54:25,480 --> 00:54:28,960
so ral you means rectified linear unit

1229
00:54:28,960 --> 00:54:32,920
means max 0 comma thing and I'm not like

1230
00:54:32,920 --> 00:54:34,690
simplifying it I really mean it like

1231
00:54:34,690 --> 00:54:36,610
when I say like if I'm simplifying I

1232
00:54:36,610 --> 00:54:38,890
always say so I'm simplifying but if I'm

1233
00:54:38,890 --> 00:54:40,930
not saying I'm simplifying that's the

1234
00:54:40,930 --> 00:54:42,970
entirety okay so a rectified linear unit

1235
00:54:42,970 --> 00:54:45,880
in its entirety is this and a

1236
00:54:45,880 --> 00:54:50,490
convolution in its entirety is is this

1237
00:54:50,490 --> 00:54:54,790
okay so a single layer of a

1238
00:54:54,790 --> 00:54:57,370
convolutional neural network is being

1239
00:54:57,370 --> 00:55:00,770
implemented in its entirety

1240
00:55:00,770 --> 00:55:03,560
here in Excel okay and so you can see

1241
00:55:03,560 --> 00:55:06,710
what it's done is it's deleted pretty

1242
00:55:06,710 --> 00:55:09,290
much the vertical edges and highlighted

1243
00:55:09,290 --> 00:55:12,560
the horizontal edges so again this is

1244
00:55:12,560 --> 00:55:15,920
assuming that our network is trained and

1245
00:55:15,920 --> 00:55:18,770
that at the end of training it a created

1246
00:55:18,770 --> 00:55:20,750
a convolutional filter with these

1247
00:55:20,750 --> 00:55:24,500
specific line numbers in and so here is

1248
00:55:24,500 --> 00:55:28,670
a second convolutional filter it's just

1249
00:55:28,670 --> 00:55:32,060
a different line numbers now pi torch

1250
00:55:32,060 --> 00:55:35,210
doesn't store them as two separate nine

1251
00:55:35,210 --> 00:55:38,480
digit arrays it stores it as a tensor

1252
00:55:38,480 --> 00:55:41,540
right remember a tensor just means an

1253
00:55:41,540 --> 00:55:46,010
array with more dimensions okay you can

1254
00:55:46,010 --> 00:55:50,060
use the word array as well it's the same

1255
00:55:50,060 --> 00:55:52,850
thing but in pi torch they always use

1256
00:55:52,850 --> 00:55:54,230
the word tensor so I'm going to say

1257
00:55:54,230 --> 00:55:57,440
cancer okay so it's just a tensor with

1258
00:55:57,440 --> 00:55:59,870
an additional axis which allows us to

1259
00:55:59,870 --> 00:56:02,540
stack each of these filters together

1260
00:56:02,540 --> 00:56:07,220
right filter and kernel pretty much mean

1261
00:56:07,220 --> 00:56:09,350
the same thing yeah right it refers to

1262
00:56:09,350 --> 00:56:13,490
one of these three by three matrices or

1263
00:56:13,490 --> 00:56:17,180
one of these three by three slices of a

1264
00:56:17,180 --> 00:56:19,700
three dimensional tensor so if I take

1265
00:56:19,700 --> 00:56:22,040
this one and here I've literally just

1266
00:56:22,040 --> 00:56:23,990
copied the formulas in Excel from above

1267
00:56:23,990 --> 00:56:27,350
okay and so you can see this one is now

1268
00:56:27,350 --> 00:56:30,680
finding a vertebra which as we would

1269
00:56:30,680 --> 00:56:31,130
expect

1270
00:56:31,130 --> 00:56:40,010
okay so we've now created one layer

1271
00:56:40,010 --> 00:56:42,560
right this here is a layer them

1272
00:56:42,560 --> 00:56:44,150
specifically we'd say it's a hidden

1273
00:56:44,150 --> 00:56:46,520
layer which is it's not an input layer

1274
00:56:46,520 --> 00:56:48,680
and it's not an output layer so

1275
00:56:48,680 --> 00:56:50,810
everything else is a hidden layer okay

1276
00:56:50,810 --> 00:56:55,480
and this particular hidden layer has is

1277
00:56:55,480 --> 00:56:59,210
a size two on this dimension right

1278
00:56:59,210 --> 00:57:02,200
because it has two

1279
00:57:02,200 --> 00:57:07,810
filters right two kernels so what

1280
00:57:07,810 --> 00:57:09,849
happens next

1281
00:57:09,849 --> 00:57:14,500
well let's do another one okay so as we

1282
00:57:14,500 --> 00:57:17,710
kind of go along things can multiply a

1283
00:57:17,710 --> 00:57:20,050
little bit in complexity right because

1284
00:57:20,050 --> 00:57:23,589
my next filter is going to have to

1285
00:57:23,589 --> 00:57:27,010
contain two of these three by threes

1286
00:57:27,010 --> 00:57:28,930
because I'm gonna have to say how do I

1287
00:57:28,930 --> 00:57:31,690
want to bring Adam I want to write these

1288
00:57:31,690 --> 00:57:34,540
three things and at the same time how do

1289
00:57:34,540 --> 00:57:36,369
I want to wait the corresponding three

1290
00:57:36,369 --> 00:57:38,589
things down here right because in pi

1291
00:57:38,589 --> 00:57:41,200
torch this is going to be this whole

1292
00:57:41,200 --> 00:57:44,109
thing here is going to be stored as a

1293
00:57:44,109 --> 00:57:46,660
multi-dimensional tensor right so you

1294
00:57:46,660 --> 00:57:48,430
shouldn't really think of this now as

1295
00:57:48,430 --> 00:57:54,040
two 3x3 kernels but one two by three by

1296
00:57:54,040 --> 00:57:58,599
three eternal okay so to calculate this

1297
00:57:58,599 --> 00:58:02,579
value here I've got the sum product of

1298
00:58:02,579 --> 00:58:09,510
all of that plus the sum product of

1299
00:58:09,510 --> 00:58:17,349
scroll down all of that okay and so the

1300
00:58:17,349 --> 00:58:19,119
top ones are being multiplied by this

1301
00:58:19,119 --> 00:58:21,130
part of the kernel and the bottom ones

1302
00:58:21,130 --> 00:58:22,420
have been multiplied by this part of the

1303
00:58:22,420 --> 00:58:25,869
kernel and so over time you want to

1304
00:58:25,869 --> 00:58:28,000
start to get very comfortable with the

1305
00:58:28,000 --> 00:58:31,170
idea of these like higher dimensional

1306
00:58:31,170 --> 00:58:35,530
linear combinations right like it's it's

1307
00:58:35,530 --> 00:58:37,900
harder to draw it on the screen like I

1308
00:58:37,900 --> 00:58:40,720
had to put one above the other but

1309
00:58:40,720 --> 00:58:42,670
conceptually just stuck it in your mind

1310
00:58:42,670 --> 00:58:44,440
like this that's really how you want to

1311
00:58:44,440 --> 00:58:47,140
think right and actually Geoffrey Hinton

1312
00:58:47,140 --> 00:58:50,400
in his original 2012 neural Nets

1313
00:58:50,400 --> 00:58:54,160
Coursera class has a tip which is how

1314
00:58:54,160 --> 00:58:56,290
all computer scientists deal with like

1315
00:58:56,290 --> 00:58:58,990
very high dimensional spaces which is

1316
00:58:58,990 --> 00:59:01,000
that they basically just visualize the

1317
00:59:01,000 --> 00:59:01,930
two-dimensional space

1318
00:59:01,930 --> 00:59:04,660
and then say like twelve dimensions

1319
00:59:04,660 --> 00:59:06,390
really fast and they had lots of tires

1320
00:59:06,390 --> 00:59:08,109
so that's it

1321
00:59:08,109 --> 00:59:09,760
right we can see two dimensions on the

1322
00:59:09,760 --> 00:59:11,140
screen and then you're just going to try

1323
00:59:11,140 --> 00:59:14,470
to trust that you can have more

1324
00:59:14,470 --> 00:59:15,800
dimensions like the Const

1325
00:59:15,800 --> 00:59:17,660
it's just you know there's there's

1326
00:59:17,660 --> 00:59:19,550
nothing different about them and so you

1327
00:59:19,550 --> 00:59:21,830
can see in Excel you know Excel doesn't

1328
00:59:21,830 --> 00:59:23,090
have the ability to handle

1329
00:59:23,090 --> 00:59:25,100
three-dimensional tensors so I had to

1330
00:59:25,100 --> 00:59:27,730
like say okay take this two-dimensional

1331
00:59:27,730 --> 00:59:31,220
dot product add on this two-dimensional

1332
00:59:31,220 --> 00:59:33,170
dot product right but if there was some

1333
00:59:33,170 --> 00:59:35,660
kind of 3d Excel I could have to stand

1334
00:59:35,660 --> 00:59:38,900
that in a single line all right and then

1335
00:59:38,900 --> 00:59:42,890
again apply max 0 comma otherwise known

1336
00:59:42,890 --> 00:59:44,720
as rectified linear unit otherwise known

1337
00:59:44,720 --> 00:59:48,710
as value okay so here is my second layer

1338
00:59:48,710 --> 00:59:51,970
and so when people create different

1339
00:59:51,970 --> 00:59:54,740
architectures write an architecture

1340
00:59:54,740 --> 00:59:59,300
means like how big is your kernel at

1341
00:59:59,300 --> 00:59:59,960
layer 1

1342
00:59:59,960 --> 01:00:02,540
how many filters are in your kernel at

1343
01:00:02,540 --> 01:00:04,630
layer 1 so here I've got a 3 by 3

1344
01:00:04,630 --> 01:00:08,510
where's number 1 and a 3 by 3 there's

1345
01:00:08,510 --> 01:00:11,330
number 2 so like this architecture I've

1346
01:00:11,330 --> 01:00:15,250
created starts off with 2 3 by 3

1347
01:00:15,250 --> 01:00:20,480
convolutional kernels and then my second

1348
01:00:20,480 --> 01:00:25,100
layer has another two kernels of size 2

1349
01:00:25,100 --> 01:00:27,020
by 3 by 3 so there's the first one

1350
01:00:27,020 --> 01:00:30,650
and then down here here's a second 2 by

1351
01:00:30,650 --> 01:00:34,430
3 by 3 kernel okay and so remember one

1352
01:00:34,430 --> 01:00:35,930
of these specific any one of these

1353
01:00:35,930 --> 01:00:40,640
numbers is an activation okay so this

1354
01:00:40,640 --> 01:00:43,850
activation is being calculated from

1355
01:00:43,850 --> 01:00:45,680
these three things here and other 3

1356
01:00:45,680 --> 01:00:47,780
things up there and we're using these

1357
01:00:47,780 --> 01:00:52,940
this 2 by 3 by 3 kernel okay and so what

1358
01:00:52,940 --> 01:00:55,160
tends to happen is people generally give

1359
01:00:55,160 --> 01:00:57,830
names to their layers so I say okay

1360
01:00:57,830 --> 01:01:01,220
let's call this layer here con 1 and

1361
01:01:01,220 --> 01:01:07,670
this layer here and this and this layer

1362
01:01:07,670 --> 01:01:11,630
here con - all right so that's you know

1363
01:01:11,630 --> 01:01:13,730
but generally you'll just see that like

1364
01:01:13,730 --> 01:01:15,920
when you print out a summary of a

1365
01:01:15,920 --> 01:01:17,930
network every layer will have some kind

1366
01:01:17,930 --> 01:01:22,340
of name okay and so then what happens

1367
01:01:22,340 --> 01:01:26,150
next well part of the architecture is

1368
01:01:26,150 --> 01:01:28,690
like do you have some max pooling where

1369
01:01:28,690 --> 01:01:30,400
bounces up Matt spalling happen so in

1370
01:01:30,400 --> 01:01:32,410
this architecture we're inventing we're

1371
01:01:32,410 --> 01:01:36,520
going to next step is do max fully okay

1372
01:01:36,520 --> 01:01:40,240
Matt spooling is a little hard to kind

1373
01:01:40,240 --> 01:01:43,569
of show in Excel but we've got it so max

1374
01:01:43,569 --> 01:01:45,940
pooling if I do a two by two max pooling

1375
01:01:45,940 --> 01:01:49,180
it's going to have the resolution both

1376
01:01:49,180 --> 01:01:51,579
height and width so you can see here

1377
01:01:51,579 --> 01:01:57,359
that I've replaced these four numbers

1378
01:01:57,359 --> 01:02:00,240
with the maximum of those four numbers

1379
01:02:00,240 --> 01:02:02,560
right and so because I'm having the

1380
01:02:02,560 --> 01:02:04,390
resolution it only makes sense to

1381
01:02:04,390 --> 01:02:06,780
actually have something every two cells

1382
01:02:06,780 --> 01:02:10,359
okay so you can see here the way I've

1383
01:02:10,359 --> 01:02:13,690
got kind of the same looking shape as I

1384
01:02:13,690 --> 01:02:15,940
had back here okay but it's now half the

1385
01:02:15,940 --> 01:02:19,030
resolution so for placed every two by

1386
01:02:19,030 --> 01:02:22,119
two with its max and you'll notice like

1387
01:02:22,119 --> 01:02:24,160
it's not every possible two by two I

1388
01:02:24,160 --> 01:02:26,319
skip over from here so this is like

1389
01:02:26,319 --> 01:02:29,200
starting at beat Hugh and then the next

1390
01:02:29,200 --> 01:02:33,460
one starts at BS right so they're like

1391
01:02:33,460 --> 01:02:35,740
non-overlapping that's why it's

1392
01:02:35,740 --> 01:02:38,050
decreasing the resolution okay

1393
01:02:38,050 --> 01:02:39,940
so anybody who's comfortable with

1394
01:02:39,940 --> 01:02:43,000
spreadsheets you know you can open this

1395
01:02:43,000 --> 01:02:45,369
and have a look and so after our max

1396
01:02:45,369 --> 01:02:52,150
pooling there's a number of different

1397
01:02:52,150 --> 01:02:54,010
things we could do next and I'm going to

1398
01:02:54,010 --> 01:02:58,119
show you a kind of classic old style

1399
01:02:58,119 --> 01:03:00,790
approach nowadays in fact what generally

1400
01:03:00,790 --> 01:03:02,890
happens nowadays is we do a max pool

1401
01:03:02,890 --> 01:03:04,900
where we kind of like max across the

1402
01:03:04,900 --> 01:03:08,260
entire size right but on older

1403
01:03:08,260 --> 01:03:10,180
architectures and also on all the

1404
01:03:10,180 --> 01:03:12,790
structured data stuff we do we actually

1405
01:03:12,790 --> 01:03:14,740
do something called a fully connected

1406
01:03:14,740 --> 01:03:16,930
layer and so here's a fully connected

1407
01:03:16,930 --> 01:03:19,060
layer I'm going to take every single one

1408
01:03:19,060 --> 01:03:22,300
of these activations and I've got to

1409
01:03:22,300 --> 01:03:24,510
give every single one of them or weight

1410
01:03:24,510 --> 01:03:28,150
right and so then I'm going to take over

1411
01:03:28,150 --> 01:03:33,130
here here is the sum product of every

1412
01:03:33,130 --> 01:03:35,650
one of the activations by every one of

1413
01:03:35,650 --> 01:03:40,670
the weights for both of the

1414
01:03:40,670 --> 01:03:43,160
two levels of my three-dimensional

1415
01:03:43,160 --> 01:03:45,619
tensor right and so this is called a

1416
01:03:45,619 --> 01:03:47,599
fully connected layer notice it's

1417
01:03:47,599 --> 01:03:49,490
different to a convolution I'm not going

1418
01:03:49,490 --> 01:03:51,980
through a few at a time right but I'm

1419
01:03:51,980 --> 01:03:54,619
creating a really big weight matrix

1420
01:03:54,619 --> 01:03:57,079
right so rather than having a couple of

1421
01:03:57,079 --> 01:03:59,930
little 3x3 kernels my weight matrix is

1422
01:03:59,930 --> 01:04:03,290
now as big as the entire input and so as

1423
01:04:03,290 --> 01:04:07,760
you can imagine architectures that make

1424
01:04:07,760 --> 01:04:09,920
heavy use of fully convolutional layers

1425
01:04:09,920 --> 01:04:13,640
can have a lot of weights which means

1426
01:04:13,640 --> 01:04:15,049
they can have trouble with overfitting

1427
01:04:15,049 --> 01:04:17,839
and they can also be slow and so you're

1428
01:04:17,839 --> 01:04:20,540
going to see a lot an architecture

1429
01:04:20,540 --> 01:04:23,299
called vgg because it was the first kind

1430
01:04:23,299 --> 01:04:25,280
of successful deeper architecture it has

1431
01:04:25,280 --> 01:04:28,880
up to 19 layers and vgg actually

1432
01:04:28,880 --> 01:04:31,750
contains a fully connected layer with

1433
01:04:31,750 --> 01:04:35,569
4096 weights connected to a hidden layer

1434
01:04:35,569 --> 01:04:39,710
with 4,000 sorry 4096 activations

1435
01:04:39,710 --> 01:04:42,859
connected to a hidden layer with 4096

1436
01:04:42,859 --> 01:04:46,250
activations so you've got like 4096 by

1437
01:04:46,250 --> 01:04:50,359
4096 x remember or apply it by the

1438
01:04:50,359 --> 01:04:53,180
number of kind of kernels that we've

1439
01:04:53,180 --> 01:04:59,839
calculated so in vgg there's this I

1440
01:04:59,839 --> 01:05:03,319
think it's like 300 million weights of

1441
01:05:03,319 --> 01:05:06,319
which something like 250 million of them

1442
01:05:06,319 --> 01:05:09,380
are in these fully connected layers so

1443
01:05:09,380 --> 01:05:11,420
we'll learn later on in the course about

1444
01:05:11,420 --> 01:05:13,700
how we can kind of avoid using these big

1445
01:05:13,700 --> 01:05:15,470
fully connected layers and behind the

1446
01:05:15,470 --> 01:05:17,450
scenes all the stuff that you've seen us

1447
01:05:17,450 --> 01:05:20,059
using like ResNet and res next none of

1448
01:05:20,059 --> 01:05:22,880
them use very large fully connected

1449
01:05:22,880 --> 01:05:26,049
layers you know you had a question

1450
01:05:26,049 --> 01:05:30,319
sorry yeah come on um so could you tell

1451
01:05:30,319 --> 01:05:32,210
us more about for example if we had like

1452
01:05:32,210 --> 01:05:34,819
three channels for the input what would

1453
01:05:34,819 --> 01:05:40,280
be the shape yeah these filters right so

1454
01:05:40,280 --> 01:05:42,619
that's a great question so if we have 3

1455
01:05:42,619 --> 01:05:44,960
channels of input it would look exactly

1456
01:05:44,960 --> 01:05:48,950
like conv one right cons one kind of has

1457
01:05:48,950 --> 01:05:52,160
two channels right and so you can see

1458
01:05:52,160 --> 01:05:54,589
with cons one we had two channels so

1459
01:05:54,589 --> 01:05:57,440
therefore our filters had to have like

1460
01:05:57,440 --> 01:06:00,289
two channels per filter and so you could

1461
01:06:00,289 --> 01:06:02,989
like imagine that this input didn't

1462
01:06:02,989 --> 01:06:05,210
exist you know and actually this was the

1463
01:06:05,210 --> 01:06:06,950
airport alright so when you have a

1464
01:06:06,950 --> 01:06:09,349
multi-channel input it just means that

1465
01:06:09,349 --> 01:06:11,749
your filters look like this and so

1466
01:06:11,749 --> 01:06:15,410
images often full color they have three

1467
01:06:15,410 --> 01:06:17,779
red green and blue sometimes they also

1468
01:06:17,779 --> 01:06:20,180
have an alpha Channel so however many

1469
01:06:20,180 --> 01:06:23,059
you have that's how many inputs you need

1470
01:06:23,059 --> 01:06:25,640
and so something which I know Jeanette

1471
01:06:25,640 --> 01:06:27,529
was playing with recently was like using

1472
01:06:27,529 --> 01:06:32,119
a full color image net model in medical

1473
01:06:32,119 --> 01:06:34,099
imaging for something called bone age

1474
01:06:34,099 --> 01:06:36,170
calculations which has a single channel

1475
01:06:36,170 --> 01:06:38,499
and so what she did was basically take

1476
01:06:38,499 --> 01:06:42,559
the the input the the single channel

1477
01:06:42,559 --> 01:06:45,710
input and make three copies of it so you

1478
01:06:45,710 --> 01:06:48,619
end up with basically like one two three

1479
01:06:48,619 --> 01:06:52,839
versions of the same thing which is like

1480
01:06:52,839 --> 01:06:55,460
it's kind of a small idea like it's kind

1481
01:06:55,460 --> 01:06:57,680
of redundant information that we don't

1482
01:06:57,680 --> 01:06:59,630
quite want but it does mean that then if

1483
01:06:59,630 --> 01:07:01,969
you had a something that expected a

1484
01:07:01,969 --> 01:07:05,960
three channel convolutional filter you

1485
01:07:05,960 --> 01:07:08,660
can use it right and so at the moment

1486
01:07:08,660 --> 01:07:11,420
there's a cable competition for iceberg

1487
01:07:11,420 --> 01:07:15,680
detection using a some funky satellite

1488
01:07:15,680 --> 01:07:17,660
specific data format that has two

1489
01:07:17,660 --> 01:07:20,900
channels so here's how you could do that

1490
01:07:20,900 --> 01:07:23,719
you could either copy one of those two

1491
01:07:23,719 --> 01:07:25,369
channels into the third channel or I

1492
01:07:25,369 --> 01:07:27,380
think what people in Carroll are doing

1493
01:07:27,380 --> 01:07:30,920
is to take the average of the two again

1494
01:07:30,920 --> 01:07:32,779
it's not ideal but it's a way that you

1495
01:07:32,779 --> 01:07:37,460
can use pre-trained networks yeah I've

1496
01:07:37,460 --> 01:07:40,910
done a lot of fiddling around like that

1497
01:07:40,910 --> 01:07:42,650
you can also actually I've actually done

1498
01:07:42,650 --> 01:07:45,440
things where I wanted to use a three

1499
01:07:45,440 --> 01:07:47,569
channel image net Network on four

1500
01:07:47,569 --> 01:07:48,410
channel data

1501
01:07:48,410 --> 01:07:50,329
I had a satellite data where the fourth

1502
01:07:50,329 --> 01:07:53,029
channel was near-infrared and so

1503
01:07:53,029 --> 01:07:59,269
basically I added an extra kind of level

1504
01:07:59,269 --> 01:08:02,239
to my convolutional kernels that were

1505
01:08:02,239 --> 01:08:04,910
all zeros and so basically like started

1506
01:08:04,910 --> 01:08:06,890
off by ignoring the near-infrared band

1507
01:08:06,890 --> 01:08:08,320
and

1508
01:08:08,320 --> 01:08:10,300
so what happens it basically and you'll

1509
01:08:10,300 --> 01:08:13,480
see this next week is that rather than

1510
01:08:13,480 --> 01:08:15,940
having these like carefully trained

1511
01:08:15,940 --> 01:08:17,770
filters when you're actually training

1512
01:08:17,770 --> 01:08:19,870
something from scratch we're actually

1513
01:08:19,870 --> 01:08:21,450
going to start with random numbers

1514
01:08:21,450 --> 01:08:23,230
that's actually what we do we actually

1515
01:08:23,230 --> 01:08:25,180
start with random numbers and then we

1516
01:08:25,180 --> 01:08:26,470
use this thing called stochastic

1517
01:08:26,470 --> 01:08:27,910
gradient descent which we've kind of

1518
01:08:27,910 --> 01:08:29,050
seen conceptually

1519
01:08:29,050 --> 01:08:31,480
to slightly improve those random numbers

1520
01:08:31,480 --> 01:08:33,070
to make them less random and we

1521
01:08:33,070 --> 01:08:35,350
basically do that again and again and

1522
01:08:35,350 --> 01:08:37,600
again okay great

1523
01:08:37,600 --> 01:08:39,760
let's take a seven minute break and

1524
01:08:39,760 --> 01:08:48,700
we'll come back at 7:50 all right so

1525
01:08:48,700 --> 01:08:50,310
what happens next

1526
01:08:50,310 --> 01:08:57,580
so we've got as far as doing a fully

1527
01:08:57,580 --> 01:09:00,339
connected layer right so we had our the

1528
01:09:00,339 --> 01:09:02,290
results of our max pooling layer got fed

1529
01:09:02,290 --> 01:09:04,240
to a fully connected layer and he might

1530
01:09:04,240 --> 01:09:07,180
notice those of you that remember your

1531
01:09:07,180 --> 01:09:09,339
linear algebra the fully connected layer

1532
01:09:09,339 --> 01:09:11,770
is actually doing a classic traditional

1533
01:09:11,770 --> 01:09:14,950
matrix product okay so it's basically

1534
01:09:14,950 --> 01:09:17,920
just going through each pair in turn

1535
01:09:17,920 --> 01:09:19,480
multiplying them together and then

1536
01:09:19,480 --> 01:09:23,970
adding them up to do a matrix product

1537
01:09:23,970 --> 01:09:33,839
now in practice if we want to calculate

1538
01:09:33,839 --> 01:09:36,460
which one of the ten digits we're

1539
01:09:36,460 --> 01:09:40,870
looking at their single number we've

1540
01:09:40,870 --> 01:09:44,740
calculated isn't enough we would

1541
01:09:44,740 --> 01:09:48,130
actually calculate ten numbers so what

1542
01:09:48,130 --> 01:09:50,370
we will have is rather than just having

1543
01:09:50,370 --> 01:09:55,240
one set of fully connected weights like

1544
01:09:55,240 --> 01:09:57,130
this and I say set because remember

1545
01:09:57,130 --> 01:10:00,370
there's like a whole 3d kind of tensor

1546
01:10:00,370 --> 01:10:04,030
of them we would actually need ten of

1547
01:10:04,030 --> 01:10:06,790
those right so you can see that these

1548
01:10:06,790 --> 01:10:09,790
tensors start to get a little bit high

1549
01:10:09,790 --> 01:10:12,250
dimensional right and so this is where

1550
01:10:12,250 --> 01:10:14,680
my patients we're doing it next cell ran

1551
01:10:14,680 --> 01:10:17,140
out but imagine that I had done this ten

1552
01:10:17,140 --> 01:10:19,810
times I could now have ten different

1553
01:10:19,810 --> 01:10:21,499
numbers or being calculated

1554
01:10:21,499 --> 01:10:23,150
yeah using exactly the same process

1555
01:10:23,150 --> 01:10:27,110
right we'll just be ten of these fully

1556
01:10:27,110 --> 01:10:35,679
connected to by m-by-n erased basically

1557
01:10:35,679 --> 01:10:40,190
and so then we would have ten numbers

1558
01:10:40,190 --> 01:10:44,360
being spat out so what happens next so

1559
01:10:44,360 --> 01:10:48,190
next up we can open up a different Excel

1560
01:10:48,190 --> 01:10:52,070
worksheet entropy example dot XLS that's

1561
01:10:52,070 --> 01:10:56,539
got two different worksheets one of them

1562
01:10:56,539 --> 01:10:59,210
is called soft mass and what happens

1563
01:10:59,210 --> 01:11:02,030
here sorry I've changed domains rather

1564
01:11:02,030 --> 01:11:04,400
than predicting whether it's the number

1565
01:11:04,400 --> 01:11:06,199
from one not to nine I'm going to

1566
01:11:06,199 --> 01:11:07,639
predict whether something is a cat a dog

1567
01:11:07,639 --> 01:11:10,820
a plane of Fisher Building okay so out

1568
01:11:10,820 --> 01:11:14,179
of our that fully connected layer we've

1569
01:11:14,179 --> 01:11:16,989
got this case we'd have five numbers and

1570
01:11:16,989 --> 01:11:20,389
notice at this point there's no rail you

1571
01:11:20,389 --> 01:11:23,150
okay in the last layer there's no rail

1572
01:11:23,150 --> 01:11:27,499
you okay so I can have negatives so I

1573
01:11:27,499 --> 01:11:32,300
want to turn these five numbers H into a

1574
01:11:32,300 --> 01:11:34,429
probability I want to turn it into a

1575
01:11:34,429 --> 01:11:37,039
probability from naught to one that it's

1576
01:11:37,039 --> 01:11:37,699
a cat

1577
01:11:37,699 --> 01:11:40,340
that's a dog there's a plane that it's a

1578
01:11:40,340 --> 01:11:42,949
fish that it's a building and I want

1579
01:11:42,949 --> 01:11:44,479
those probabilities to have a couple of

1580
01:11:44,479 --> 01:11:45,979
characteristics first is that each of

1581
01:11:45,979 --> 01:11:48,249
them should be between zero and one and

1582
01:11:48,249 --> 01:11:50,510
the second is that this state together

1583
01:11:50,510 --> 01:11:52,550
should add up to one right it's

1584
01:11:52,550 --> 01:11:55,969
definitely one of these five things okay

1585
01:11:55,969 --> 01:11:58,219
so to do that we use a different kind of

1586
01:11:58,219 --> 01:12:01,400
activation function what's an activation

1587
01:12:01,400 --> 01:12:04,460
function an activation function is a

1588
01:12:04,460 --> 01:12:07,449
function that is applied to activations

1589
01:12:07,449 --> 01:12:13,400
so for example max 0 comma something is

1590
01:12:13,400 --> 01:12:16,340
a function that I applied to an

1591
01:12:16,340 --> 01:12:18,889
activation so an activation function

1592
01:12:18,889 --> 01:12:23,630
always takes in one number and spits out

1593
01:12:23,630 --> 01:12:27,800
one number so max of 0 comma X takes in

1594
01:12:27,800 --> 01:12:29,780
a number X and spits out some different

1595
01:12:29,780 --> 01:12:33,070
number value of s

1596
01:12:33,070 --> 01:12:35,780
that's all an activation function is and

1597
01:12:35,780 --> 01:12:39,260
if you remember back to that PowerPoint

1598
01:12:39,260 --> 01:12:45,590
we saw in Lesson one

1599
01:12:45,590 --> 01:12:50,540
each of our layers was just a linear

1600
01:12:50,540 --> 01:12:55,230
function and then after every layer we

1601
01:12:55,230 --> 01:12:58,619
said we needed some non-linearity act as

1602
01:12:58,619 --> 01:13:01,560
if you stack a bunch of linear layers

1603
01:13:01,560 --> 01:13:04,500
together right then all you end up with

1604
01:13:04,500 --> 01:13:06,810
is a linear layer okay

1605
01:13:06,810 --> 01:13:09,840
so somebody's talking can can you not a

1606
01:13:09,840 --> 01:13:13,829
slow just acting thank you

1607
01:13:13,829 --> 01:13:15,979
if you stack a number of linear

1608
01:13:15,979 --> 01:13:18,210
functions together you just end up with

1609
01:13:18,210 --> 01:13:20,729
a linear function and nobody does any

1610
01:13:20,729 --> 01:13:22,139
cool deep learning with displaying your

1611
01:13:22,139 --> 01:13:24,119
functions right but remember we also

1612
01:13:24,119 --> 01:13:29,179
learnt that by stacking linear functions

1613
01:13:29,179 --> 01:13:33,090
with between each one a non-linearity we

1614
01:13:33,090 --> 01:13:34,769
could create like arbitrarily complex

1615
01:13:34,769 --> 01:13:37,349
shapes and so the non-linearity that

1616
01:13:37,349 --> 01:13:39,599
we're using after every hidden layer is

1617
01:13:39,599 --> 01:13:42,979
a rally rectified linear unit a

1618
01:13:42,979 --> 01:13:45,570
non-linearity is an activation function

1619
01:13:45,570 --> 01:13:48,510
an activation function is a

1620
01:13:48,510 --> 01:13:51,269
non-linearity in with in deep way

1621
01:13:51,269 --> 01:13:52,619
obviously there's lots of other

1622
01:13:52,619 --> 01:13:54,749
nonlinearities and in the world but in

1623
01:13:54,749 --> 01:13:58,110
deep learning this is what we mean so an

1624
01:13:58,110 --> 01:14:00,239
activation function is any function that

1625
01:14:00,239 --> 01:14:02,999
takes some activation in as a single

1626
01:14:02,999 --> 01:14:04,979
number and spits out some new activation

1627
01:14:04,979 --> 01:14:08,969
like max of 0 comma so I'm now going to

1628
01:14:08,969 --> 01:14:10,320
tell you about a different activation

1629
01:14:10,320 --> 01:14:12,300
function it's slightly more complicated

1630
01:14:12,300 --> 01:14:16,170
than value but not too much it's called

1631
01:14:16,170 --> 01:14:19,289
soft max soft max only ever occurs in

1632
01:14:19,289 --> 01:14:22,139
the final layer at the very end and the

1633
01:14:22,139 --> 01:14:24,869
reason why is that soft max always spits

1634
01:14:24,869 --> 01:14:27,900
out numbers as an activation function

1635
01:14:27,900 --> 01:14:29,550
that always spits out a number between

1636
01:14:29,550 --> 01:14:32,849
Norton 1 and it always spits out a bunch

1637
01:14:32,849 --> 01:14:35,729
of numbers that add to 1 so a soft max

1638
01:14:35,729 --> 01:14:39,889
gives us what we want right in theory

1639
01:14:39,889 --> 01:14:43,289
this isn't strictly necessary right like

1640
01:14:43,289 --> 01:14:46,139
we could ask our neural net to learn a

1641
01:14:46,139 --> 01:14:50,579
set of kernels which have you know which

1642
01:14:50,579 --> 01:14:53,489
which give probabilities that line up as

1643
01:14:53,489 --> 01:14:54,960
closely as possible with what we want

1644
01:14:54,960 --> 01:14:57,599
but in general with deep learning if you

1645
01:14:57,599 --> 01:14:59,940
can construct your architecture so that

1646
01:14:59,940 --> 01:15:03,090
the desired characteristics are as easy

1647
01:15:03,090 --> 01:15:06,090
to express as possible you'll end up

1648
01:15:06,090 --> 01:15:07,739
with better models like they'll learn

1649
01:15:07,739 --> 01:15:10,170
more quickly with less parameters so in

1650
01:15:10,170 --> 01:15:13,139
this case we know that our probabilities

1651
01:15:13,139 --> 01:15:16,199
should end up being between 9 1 we know

1652
01:15:16,199 --> 01:15:18,630
that they should end up adding to 1 so

1653
01:15:18,630 --> 01:15:20,190
if we construct an activation function

1654
01:15:20,190 --> 01:15:23,280
which always has those features then

1655
01:15:23,280 --> 01:15:24,869
we're going to make our neural network

1656
01:15:24,869 --> 01:15:26,670
do a better job

1657
01:15:26,670 --> 01:15:27,930
it's gonna make it easier for it it

1658
01:15:27,930 --> 01:15:29,610
doesn't have to learn to do those things

1659
01:15:29,610 --> 01:15:32,400
because it all happen automatically okay

1660
01:15:32,400 --> 01:15:37,470
so in order to make this work we first

1661
01:15:37,470 --> 01:15:39,000
of all have to get rid of all of the

1662
01:15:39,000 --> 01:15:41,760
negatives right like we can't have

1663
01:15:41,760 --> 01:15:44,670
negative probabilities so to make things

1664
01:15:44,670 --> 01:15:46,500
not being negative one way we could do

1665
01:15:46,500 --> 01:15:49,680
it is just go into the pair of right so

1666
01:15:49,680 --> 01:15:51,420
here you can see my first step is to go

1667
01:15:51,420 --> 01:15:55,950
X of the previous one right and I think

1668
01:15:55,950 --> 01:15:58,650
I've mentioned this before but of all

1669
01:15:58,650 --> 01:16:01,080
the math that you just need to be super

1670
01:16:01,080 --> 01:16:03,450
familiar with to do deep learning the

1671
01:16:03,450 --> 01:16:05,490
one you really need is logarithms and

1672
01:16:05,490 --> 01:16:08,670
asks write all of deep learning and all

1673
01:16:08,670 --> 01:16:10,710
of machine learning they appear all the

1674
01:16:10,710 --> 01:16:19,530
time right so for example you absolutely

1675
01:16:19,530 --> 01:16:24,930
need to know that log of x times y

1676
01:16:24,930 --> 01:16:33,090
equals log of X plus log of Y all right

1677
01:16:33,090 --> 01:16:35,070
and like not just know that that's a

1678
01:16:35,070 --> 01:16:37,260
formula that exists but have a sense of

1679
01:16:37,260 --> 01:16:39,240
like what does that mean why is that

1680
01:16:39,240 --> 01:16:41,010
interesting oh I can turn

1681
01:16:41,010 --> 01:16:42,930
multiplications into additions that

1682
01:16:42,930 --> 01:16:45,360
could be really handy right and

1683
01:16:45,360 --> 01:16:50,820
therefore log of x over y equals log of

1684
01:16:50,820 --> 01:16:56,160
X minus log of Y again that's going to

1685
01:16:56,160 --> 01:16:57,480
come in pretty handy you know rather

1686
01:16:57,480 --> 01:16:59,850
than dividing I can just subtract things

1687
01:16:59,850 --> 01:17:03,390
right and also remember that if I've got

1688
01:17:03,390 --> 01:17:08,040
log of x equals y then that means a to

1689
01:17:08,040 --> 01:17:14,370
the y equals x in other words log log

1690
01:17:14,370 --> 01:17:17,250
and E to the for the inverse of each

1691
01:17:17,250 --> 01:17:21,450
other okay again you just you need to

1692
01:17:21,450 --> 01:17:23,550
really really understand these things

1693
01:17:23,550 --> 01:17:25,140
and like so if you if you haven't spent

1694
01:17:25,140 --> 01:17:27,830
much time with logs and X for a while

1695
01:17:27,830 --> 01:17:31,650
try plotting them in Excel or a notebook

1696
01:17:31,650 --> 01:17:33,780
have a sense of what shape they are how

1697
01:17:33,780 --> 01:17:35,670
they combine together just make sure

1698
01:17:35,670 --> 01:17:38,260
you're really comfortable with them so

1699
01:17:38,260 --> 01:17:42,640
we're using it here right we're using it

1700
01:17:42,640 --> 01:17:44,590
here so one of the things that we know

1701
01:17:44,590 --> 01:17:46,750
is a to the power of something is

1702
01:17:46,750 --> 01:17:49,450
positive okay so that's great

1703
01:17:49,450 --> 01:17:50,890
the other thing you'll notice about e to

1704
01:17:50,890 --> 01:17:52,630
the power of something is because it's a

1705
01:17:52,630 --> 01:17:56,290
power numbers that are slightly bigger

1706
01:17:56,290 --> 01:17:58,210
than other numbers like four is a little

1707
01:17:58,210 --> 01:18:00,640
bit bigger than 2.8 when you go e to the

1708
01:18:00,640 --> 01:18:02,710
power of it really accentuates that

1709
01:18:02,710 --> 01:18:04,120
difference okay

1710
01:18:04,120 --> 01:18:05,320
so we're going to take advantage of both

1711
01:18:05,320 --> 01:18:06,850
of these features for the purpose of

1712
01:18:06,850 --> 01:18:10,480
deep learning okay so we take our the

1713
01:18:10,480 --> 01:18:13,330
results of this fully connected layer we

1714
01:18:13,330 --> 01:18:15,850
go e to the power of for each of them

1715
01:18:15,850 --> 01:18:24,130
and then we're gonna yeah and then we're

1716
01:18:24,130 --> 01:18:28,120
going to add them up okay so here is the

1717
01:18:28,120 --> 01:18:32,340
sum of e to the power of so then here

1718
01:18:32,340 --> 01:18:35,590
we're going to take e to the power of

1719
01:18:35,590 --> 01:18:37,830
divided by the sum of e to the power of

1720
01:18:37,830 --> 01:18:41,080
so if you take all of these things

1721
01:18:41,080 --> 01:18:43,860
divided by their sum then by definition

1722
01:18:43,860 --> 01:18:49,140
all of those things must add up to 1 and

1723
01:18:49,140 --> 01:18:51,460
furthermore since we're dividing by

1724
01:18:51,460 --> 01:18:54,580
their sum they must always vary between

1725
01:18:54,580 --> 01:18:56,890
0 and 1 because they were always

1726
01:18:56,890 --> 01:18:59,590
positive alright and that's it so that's

1727
01:18:59,590 --> 01:19:04,840
what softmax is ok so I've got this kind

1728
01:19:04,840 --> 01:19:08,470
of doing random numbers each time right

1729
01:19:08,470 --> 01:19:11,050
and so you can see like as I as I look

1730
01:19:11,050 --> 01:19:14,770
through my softmax generally has quite a

1731
01:19:14,770 --> 01:19:16,510
few things that are so close to zero

1732
01:19:16,510 --> 01:19:19,090
that they round down to zero and you

1733
01:19:19,090 --> 01:19:20,739
know maybe one thing that's nearly 1

1734
01:19:20,739 --> 01:19:23,110
right and the reason for that is what we

1735
01:19:23,110 --> 01:19:25,320
just talked about that is with the X

1736
01:19:25,320 --> 01:19:28,420
just having one number a bit bigger than

1737
01:19:28,420 --> 01:19:30,310
the others tends to like push it out

1738
01:19:30,310 --> 01:19:33,100
further right so even though my inputs

1739
01:19:33,100 --> 01:19:35,440
here around on numbers between negative

1740
01:19:35,440 --> 01:19:39,190
5 and 5 right my outputs from the

1741
01:19:39,190 --> 01:19:41,140
softmax don't really look that random at

1742
01:19:41,140 --> 01:19:44,770
all in the sense that they tend to have

1743
01:19:44,770 --> 01:19:47,140
one big number and a bunch of small

1744
01:19:47,140 --> 01:19:50,650
numbers and now that's what we want

1745
01:19:50,650 --> 01:19:53,680
right we want to say like in terms of

1746
01:19:53,680 --> 01:19:55,600
like is this a cat a dog a plane a fish

1747
01:19:55,600 --> 01:19:57,700
or a building we really want it to say

1748
01:19:57,700 --> 01:20:00,100
like it's it's that you know it's it's a

1749
01:20:00,100 --> 01:20:03,880
dog or it's a plane not like I don't

1750
01:20:03,880 --> 01:20:07,510
know okay so softmax has lots of these

1751
01:20:07,510 --> 01:20:10,030
cool properties right it's going to

1752
01:20:10,030 --> 01:20:12,610
return a probability that adds up to 1

1753
01:20:12,610 --> 01:20:14,980
and it's going to tend to want to pick

1754
01:20:14,980 --> 01:20:20,260
one thing particularly strongly okay so

1755
01:20:20,260 --> 01:20:22,600
that's soft mess your net could you pass

1756
01:20:22,600 --> 01:20:29,230
actually bust me up we how would we do

1757
01:20:29,230 --> 01:20:31,180
something that as let's say you have any

1758
01:20:31,180 --> 01:20:32,440
imaging you want to count in categorize

1759
01:20:32,440 --> 01:20:34,780
I was like cat and the dog or like has

1760
01:20:34,780 --> 01:20:37,600
multiple things but what kind of

1761
01:20:37,600 --> 01:20:40,150
function will we try to use so happens

1762
01:20:40,150 --> 01:20:45,419
we're going to do that right now so

1763
01:20:45,419 --> 01:20:47,099
so hope you think about why we might

1764
01:20:47,099 --> 01:20:48,840
want to do that and so runways where you

1765
01:20:48,840 --> 01:20:50,070
might want to do that is to do

1766
01:20:50,070 --> 01:20:53,010
multi-label classification so we're

1767
01:20:53,010 --> 01:20:54,989
looking now at listen to image models

1768
01:20:54,989 --> 01:20:57,119
and specifically we're going to take a

1769
01:20:57,119 --> 01:21:00,419
look at the planet competition satellite

1770
01:21:00,419 --> 01:21:04,139
imaging competition now the satellite

1771
01:21:04,139 --> 01:21:08,189
imaging competition has some

1772
01:21:08,189 --> 01:21:09,840
similarities to stuff we've seen before

1773
01:21:09,840 --> 01:21:13,229
right so before we've seen cat versus

1774
01:21:13,229 --> 01:21:15,860
dog and these images are a cat or a dog

1775
01:21:15,860 --> 01:21:19,769
they're not Maya they're not both right

1776
01:21:19,769 --> 01:21:22,550
but the satellite imaging competition

1777
01:21:22,550 --> 01:21:25,559
has stayed as images that look like this

1778
01:21:25,559 --> 01:21:27,869
and in fact every single one of the

1779
01:21:27,869 --> 01:21:30,389
images is classified by whether there's

1780
01:21:30,389 --> 01:21:31,530
four kinds of weather

1781
01:21:31,530 --> 01:21:34,260
one of which is haze and another of

1782
01:21:34,260 --> 01:21:37,289
which is clear in addition to which

1783
01:21:37,289 --> 01:21:39,719
there is a list of features that may be

1784
01:21:39,719 --> 01:21:42,479
present including agriculture which is

1785
01:21:42,479 --> 01:21:44,880
like some some cleared area used for

1786
01:21:44,880 --> 01:21:48,269
agriculture primary which means primary

1787
01:21:48,269 --> 01:21:50,999
rainforest and water which means a river

1788
01:21:50,999 --> 01:21:54,019
or a creek so here is a clear day

1789
01:21:54,019 --> 01:21:56,969
satellite image showing some agriculture

1790
01:21:56,969 --> 01:21:59,579
some primary rainforest and some water

1791
01:21:59,579 --> 01:22:02,550
features and here's one which is in haze

1792
01:22:02,550 --> 01:22:07,079
and is entirely primary rainforest so in

1793
01:22:07,079 --> 01:22:09,840
this case we're going to want to be able

1794
01:22:09,840 --> 01:22:13,229
to show we're going to predict multiple

1795
01:22:13,229 --> 01:22:15,689
things and so softmax wouldn't be good

1796
01:22:15,689 --> 01:22:19,439
because softmax doesn't like predicting

1797
01:22:19,439 --> 01:22:21,749
multiple things and like I would

1798
01:22:21,749 --> 01:22:23,610
definitely recommend anthropomorphizing

1799
01:22:23,610 --> 01:22:25,860
your activation functions right they

1800
01:22:25,860 --> 01:22:28,079
have personalities okay and the

1801
01:22:28,079 --> 01:22:30,869
personality of the softmax is it wants

1802
01:22:30,869 --> 01:22:34,559
to pick a thing and people forget this

1803
01:22:34,559 --> 01:22:36,679
all the time I've seen many people even

1804
01:22:36,679 --> 01:22:39,419
well-regarded researchers in famous

1805
01:22:39,419 --> 01:22:44,189
academic papers using like soft maps for

1806
01:22:44,189 --> 01:22:46,229
multi-label classification it happens

1807
01:22:46,229 --> 01:22:46,860
all the time

1808
01:22:46,860 --> 01:22:49,590
right and it's kind of ridiculous

1809
01:22:49,590 --> 01:22:52,409
because they're not understanding the

1810
01:22:52,409 --> 01:22:55,249
personality of their activation function

1811
01:22:55,249 --> 01:22:58,000
so for multi

1812
01:22:58,000 --> 01:23:00,550
classification where each sample can

1813
01:23:00,550 --> 01:23:03,190
belong to one or more classes we have to

1814
01:23:03,190 --> 01:23:05,590
change a few things but here's the good

1815
01:23:05,590 --> 01:23:05,920
news

1816
01:23:05,920 --> 01:23:08,560
in fast AI we don't have to change

1817
01:23:08,560 --> 01:23:12,280
anything right so fast AI will look at

1818
01:23:12,280 --> 01:23:15,940
the labels in the CSV and if there is

1819
01:23:15,940 --> 01:23:21,880
more than one label ever for any item it

1820
01:23:21,880 --> 01:23:23,710
will automatically switch into like

1821
01:23:23,710 --> 01:23:25,600
multi-label mode so I'm going to show

1822
01:23:25,600 --> 01:23:27,670
you how it works behind the scenes but

1823
01:23:27,670 --> 01:23:29,770
the good news is you don't actually have

1824
01:23:29,770 --> 01:23:35,950
to care it happens anywhere so if you

1825
01:23:35,950 --> 01:23:40,690
have multi label images multi label

1826
01:23:40,690 --> 01:23:43,150
objects you obviously can't use the

1827
01:23:43,150 --> 01:23:46,000
classic Kerris style approach where

1828
01:23:46,000 --> 01:23:48,370
things are in folders because something

1829
01:23:48,370 --> 01:23:51,430
can't conveniently be in multiple

1830
01:23:51,430 --> 01:23:55,180
folders at the same time right so that's

1831
01:23:55,180 --> 01:23:57,610
why we you basically have to use the

1832
01:23:57,610 --> 01:24:04,050
from CSV approach right so if we look at

1833
01:24:04,050 --> 01:24:06,180
[Music]

1834
01:24:06,180 --> 01:24:12,160
an example actually I'll show you I tend

1835
01:24:12,160 --> 01:24:14,350
to take you through it right so we can

1836
01:24:14,350 --> 01:24:16,420
say okay this is the CSV file containing

1837
01:24:16,420 --> 01:24:18,970
our labels this looks exactly the same

1838
01:24:18,970 --> 01:24:21,610
as I did before but rather than side on

1839
01:24:21,610 --> 01:24:24,700
its top down alright and top down I've

1840
01:24:24,700 --> 01:24:27,220
mentioned before that can do our

1841
01:24:27,220 --> 01:24:28,810
vertical flips it actually does more

1842
01:24:28,810 --> 01:24:29,800
than that there's actually eight

1843
01:24:29,800 --> 01:24:32,110
possible symmetries for a square which

1844
01:24:32,110 --> 01:24:35,140
is it can be rotated through 90 180 270

1845
01:24:35,140 --> 01:24:37,660
or 0 degrees and for each of those it

1846
01:24:37,660 --> 01:24:39,670
can be flipped and if you think about it

1847
01:24:39,670 --> 01:24:42,130
for awhile you'll realize that that's a

1848
01:24:42,130 --> 01:24:45,070
complete enumeration of everything that

1849
01:24:45,070 --> 01:24:48,430
you can do in terms of symmetries to a

1850
01:24:48,430 --> 01:24:50,620
square so they're called it's called the

1851
01:24:50,620 --> 01:24:53,530
dihedral group of eight so if you see in

1852
01:24:53,530 --> 01:24:55,630
the code there's actually a transform or

1853
01:24:55,630 --> 01:24:59,620
dihedral that's why it's called that so

1854
01:24:59,620 --> 01:25:01,690
this transforms will basically do the

1855
01:25:01,690 --> 01:25:05,670
full set of eight symmetric dihedral

1856
01:25:05,670 --> 01:25:09,280
rotations and flips plus everything

1857
01:25:09,280 --> 01:25:11,130
which we can do to dogs and cats

1858
01:25:11,130 --> 01:25:13,860
you know small clinical rotations a

1859
01:25:13,860 --> 01:25:16,290
little bit of zooming a little bit of

1860
01:25:16,290 --> 01:25:18,810
contrast and brightness adjustment so

1861
01:25:18,810 --> 01:25:23,340
these images are a size 256 by 256 so I

1862
01:25:23,340 --> 01:25:24,870
just create a little function here to

1863
01:25:24,870 --> 01:25:28,440
let me quickly grab you know data loader

1864
01:25:28,440 --> 01:25:32,429
of any size so here's a 256 by 256 once

1865
01:25:32,429 --> 01:25:36,179
you've got a data object inside it we've

1866
01:25:36,179 --> 01:25:37,650
already seen that there's things called

1867
01:25:37,650 --> 01:25:41,400
Val D s test D s train D s there are

1868
01:25:41,400 --> 01:25:43,230
things that you can just index into and

1869
01:25:43,230 --> 01:25:45,510
grab a particular image so you just use

1870
01:25:45,510 --> 01:25:48,360
square brackets 0 you'll also see that

1871
01:25:48,360 --> 01:25:50,429
all of those things have a DL that's a

1872
01:25:50,429 --> 01:25:53,310
data loader so des is data set DL is

1873
01:25:53,310 --> 01:25:55,440
data motor these are concepts from PI

1874
01:25:55,440 --> 01:25:58,080
watch so if you Google PI torch data set

1875
01:25:58,080 --> 01:26:00,300
or pipe watch data loader you can

1876
01:26:00,300 --> 01:26:02,489
basically see what it means but the

1877
01:26:02,489 --> 01:26:04,650
basic idea is a data set gives you a

1878
01:26:04,650 --> 01:26:08,429
single image or a single object back a

1879
01:26:08,429 --> 01:26:10,350
data loader gives you back a mini batch

1880
01:26:10,350 --> 01:26:12,900
and specifically it gives you back a

1881
01:26:12,900 --> 01:26:15,989
transformed mini - so that's why when we

1882
01:26:15,989 --> 01:26:20,219
create our data object we can pass in

1883
01:26:20,219 --> 01:26:24,030
num workers and transforms it's like how

1884
01:26:24,030 --> 01:26:25,560
many processes do you want to use what

1885
01:26:25,560 --> 01:26:28,199
transforms do you want and so with with

1886
01:26:28,199 --> 01:26:30,480
a data loader you can't ask for an

1887
01:26:30,480 --> 01:26:32,159
individual image you can only get back

1888
01:26:32,159 --> 01:26:35,159
at a mini batch and you can't get back a

1889
01:26:35,159 --> 01:26:37,260
particular mini batch you can only get

1890
01:26:37,260 --> 01:26:39,750
back the next mini - so something

1891
01:26:39,750 --> 01:26:42,719
reverses look through grabbing a mini

1892
01:26:42,719 --> 01:26:46,560
batch at a time and so in Python the

1893
01:26:46,560 --> 01:26:47,880
thing that does that is called a

1894
01:26:47,880 --> 01:26:50,760
generator right or an iterator this

1895
01:26:50,760 --> 01:26:51,989
slightly different versions are the same

1896
01:26:51,989 --> 01:26:54,480
thing so to turn a data loader into an

1897
01:26:54,480 --> 01:26:56,489
iterator you use the standard Python

1898
01:26:56,489 --> 01:26:57,480
function cordetta

1899
01:26:57,480 --> 01:27:00,030
that's a Python function just a regular

1900
01:27:00,030 --> 01:27:02,550
part of the Python basic language that

1901
01:27:02,550 --> 01:27:04,710
returns you an iterator and an iterator

1902
01:27:04,710 --> 01:27:07,350
is something that takes you can pass the

1903
01:27:07,350 --> 01:27:08,909
static give pass it to the standard

1904
01:27:08,909 --> 01:27:13,290
Python function or statement next and

1905
01:27:13,290 --> 01:27:15,840
that just says give me another batch

1906
01:27:15,840 --> 01:27:18,800
from this iterator

1907
01:27:18,800 --> 01:27:21,000
so we're basically this is one of the

1908
01:27:21,000 --> 01:27:23,070
things I really like about PI torch is

1909
01:27:23,070 --> 01:27:24,829
it really leverages

1910
01:27:24,829 --> 01:27:28,760
modern pythons kind of stuff you know in

1911
01:27:28,760 --> 01:27:31,489
in tensorflow they invent their whole

1912
01:27:31,489 --> 01:27:35,090
new world earth ways of doing things and

1913
01:27:35,090 --> 01:27:38,329
so it's kind of more in a sense it's

1914
01:27:38,329 --> 01:27:40,040
more like cross-platform but in another

1915
01:27:40,040 --> 01:27:42,349
sense like it's not a good fit to any

1916
01:27:42,349 --> 01:27:46,429
platform so it's nice if you if you know

1917
01:27:46,429 --> 01:27:49,880
Python well PI torch comes very

1918
01:27:49,880 --> 01:27:52,070
naturally if you don't know Python well

1919
01:27:52,070 --> 01:27:53,960
PI torches are good reason to learn

1920
01:27:53,960 --> 01:27:58,369
Python well a PI torch your module

1921
01:27:58,369 --> 01:28:00,559
neural network module is a standard

1922
01:28:00,559 --> 01:28:03,739
Python bus for example so any work you

1923
01:28:03,739 --> 01:28:05,690
put into learning Python better will pay

1924
01:28:05,690 --> 01:28:08,090
off with paid watch so here I am using

1925
01:28:08,090 --> 01:28:12,050
standard Python iterators and next to

1926
01:28:12,050 --> 01:28:15,619
grab my next mini batch from the

1927
01:28:15,619 --> 01:28:17,780
validation sets data loader and that's

1928
01:28:17,780 --> 01:28:19,159
going to return two things it's going to

1929
01:28:19,159 --> 01:28:20,960
return the images in the mini batch and

1930
01:28:20,960 --> 01:28:23,780
the labels of the mini - so standard

1931
01:28:23,780 --> 01:28:25,360
Python approach I can pull them apart

1932
01:28:25,360 --> 01:28:30,739
like so and so here is one mini batch of

1933
01:28:30,739 --> 01:28:34,099
labels and so not surprisingly since I

1934
01:28:34,099 --> 01:28:37,670
said that my batch size let's go ahead

1935
01:28:37,670 --> 01:28:41,570
and find it

1936
01:28:41,570 --> 01:28:44,389
Oh actually it's the batch size by

1937
01:28:44,389 --> 01:28:47,840
default is 64 so I didn't pass in a

1938
01:28:47,840 --> 01:28:50,000
batch size and so just remember shift

1939
01:28:50,000 --> 01:28:52,099
tab to see like what are the things you

1940
01:28:52,099 --> 01:28:54,440
can pass and what are the defaults so by

1941
01:28:54,440 --> 01:28:57,710
default my batch size is 64 so I've got

1942
01:28:57,710 --> 01:29:01,820
that something of size 64 by 17 so there

1943
01:29:01,820 --> 01:29:07,040
are 17 of the possible classes right so

1944
01:29:07,040 --> 01:29:12,500
let's take a look at the zeroth set of

1945
01:29:12,500 --> 01:29:15,829
labels so the zeroth images labels so I

1946
01:29:15,829 --> 01:29:18,349
can zip again standard Python things it

1947
01:29:18,349 --> 01:29:21,559
takes two lists and combines it so you

1948
01:29:21,559 --> 01:29:23,150
get the zero theme from the first list

1949
01:29:23,150 --> 01:29:25,489
as you're asking for the second list and

1950
01:29:25,489 --> 01:29:27,440
the first thing for the first first this

1951
01:29:27,440 --> 01:29:29,210
first thing from the second list and so

1952
01:29:29,210 --> 01:29:31,190
forth so I can zip them together and

1953
01:29:31,190 --> 01:29:34,250
that way I can find out for the zeroth

1954
01:29:34,250 --> 01:29:36,790
image and the validation set is

1955
01:29:36,790 --> 01:29:38,369
agriculture

1956
01:29:38,369 --> 01:29:42,599
it's clear its primary rainforest its

1957
01:29:42,599 --> 01:29:47,280
slash-and-burn its water okay so as you

1958
01:29:47,280 --> 01:29:51,869
can see here this is a MOLLE label you

1959
01:29:51,869 --> 01:29:53,219
see here's a way to do multi-label

1960
01:29:53,219 --> 01:29:57,560
classification so by the same token

1961
01:29:57,560 --> 01:30:01,080
right if we go back to our single label

1962
01:30:01,080 --> 01:30:03,179
classification it's a cat dog playing

1963
01:30:03,179 --> 01:30:07,230
official building behind the scenes we

1964
01:30:07,230 --> 01:30:08,580
haven't actually looked at it but behind

1965
01:30:08,580 --> 01:30:12,599
the scenes fast AI imply torch are

1966
01:30:12,599 --> 01:30:15,389
turning our labels into something called

1967
01:30:15,389 --> 01:30:18,869
one hot encoded labels and so if it was

1968
01:30:18,869 --> 01:30:22,940
actually a dog than the actual values

1969
01:30:22,940 --> 01:30:25,980
would be like that right so these are

1970
01:30:25,980 --> 01:30:29,699
like the actuals okay so do you remember

1971
01:30:29,699 --> 01:30:32,219
at the very end of at AV o--'s video he

1972
01:30:32,219 --> 01:30:34,139
showed how like the template had to

1973
01:30:34,139 --> 01:30:36,929
match to one of the like five ABCDE

1974
01:30:36,929 --> 01:30:39,119
templates and so what it's actually

1975
01:30:39,119 --> 01:30:42,989
doing is it's comparing when I said it's

1976
01:30:42,989 --> 01:30:44,489
basically doing a dot product it's

1977
01:30:44,489 --> 01:30:46,080
actually a fully connected layer at the

1978
01:30:46,080 --> 01:30:50,040
end right that calculates an output

1979
01:30:50,040 --> 01:30:52,889
activation that goes through a soft Max

1980
01:30:52,889 --> 01:30:56,940
and then the soft max is compared to the

1981
01:30:56,940 --> 01:30:59,730
one hot encoded label right so if it was

1982
01:30:59,730 --> 01:31:03,540
a dog there would be a one here and then

1983
01:31:03,540 --> 01:31:05,250
we take the difference between the

1984
01:31:05,250 --> 01:31:08,670
actuals and the softmax activation is to

1985
01:31:08,670 --> 01:31:10,170
say and add those add up those

1986
01:31:10,170 --> 01:31:12,179
differences to say how much error is

1987
01:31:12,179 --> 01:31:14,489
there essentially we're skipping over

1988
01:31:14,489 --> 01:31:16,110
something called a loss function that

1989
01:31:16,110 --> 01:31:17,489
we'll learn about next week but

1990
01:31:17,489 --> 01:31:21,680
essentially we're basically doing that

1991
01:31:21,680 --> 01:31:25,610
now if it's one hot encoded like there's

1992
01:31:25,610 --> 01:31:28,250
only one thing which have a 1 in it then

1993
01:31:28,250 --> 01:31:32,410
actually storing it as 0 1 0 0 0 is

1994
01:31:32,410 --> 01:31:36,530
terribly inefficient right like we could

1995
01:31:36,530 --> 01:31:38,270
basically say what are the index of each

1996
01:31:38,270 --> 01:31:40,700
of these things right so we can say it's

1997
01:31:40,700 --> 01:31:44,890
like 0 1 2 3 4 like so right and so

1998
01:31:44,890 --> 01:31:48,710
rather than storing it is 0 1 0 0 0

1999
01:31:48,710 --> 01:31:52,360
we actually just store the index value

2000
01:31:52,360 --> 01:31:56,420
right so if you look at the the Y values

2001
01:31:56,420 --> 01:31:58,850
for the cats and dogs competition or the

2002
01:31:58,850 --> 01:32:01,160
dog breeds competition you won't

2003
01:32:01,160 --> 01:32:03,140
actually see a big lists of ones and

2004
01:32:03,140 --> 01:32:04,850
zeros like this you'll see a single

2005
01:32:04,850 --> 01:32:07,520
integer right which is like what what

2006
01:32:07,520 --> 01:32:11,680
class index is it right and internally

2007
01:32:11,680 --> 01:32:15,140
inside pipe arch it will actually turn

2008
01:32:15,140 --> 01:32:17,750
that into a one hot encoded vector but

2009
01:32:17,750 --> 01:32:19,360
like you will literally never see it

2010
01:32:19,360 --> 01:32:23,270
okay and and pi torch has different loss

2011
01:32:23,270 --> 01:32:26,090
functions where you basically say this

2012
01:32:26,090 --> 01:32:27,860
thing's won this thing is one hot

2013
01:32:27,860 --> 01:32:30,050
encoder door this thing is not and it

2014
01:32:30,050 --> 01:32:31,810
uses different bus functions

2015
01:32:31,810 --> 01:32:34,160
that's all hidden by the faster I

2016
01:32:34,160 --> 01:32:36,500
library right so like you don't have to

2017
01:32:36,500 --> 01:32:40,040
worry about it but is but the the cool

2018
01:32:40,040 --> 01:32:42,640
thing to realize is that this approach

2019
01:32:42,640 --> 01:32:45,350
for multi-label encoding with these ones

2020
01:32:45,350 --> 01:32:48,620
and zeros behind the scenes the exact

2021
01:32:48,620 --> 01:32:51,080
same thing happens for single level

2022
01:32:51,080 --> 01:32:56,210
classification does it make sense to

2023
01:32:56,210 --> 01:32:59,000
change the beginners of the sigmoid of

2024
01:32:59,000 --> 01:33:00,920
the softmax function by changing the

2025
01:33:00,920 --> 01:33:07,940
base no because when you change the more

2026
01:33:07,940 --> 01:33:19,450
math log base a of B equals log B over

2027
01:33:19,450 --> 01:33:24,200
log a so changing the base is just a

2028
01:33:24,200 --> 01:33:27,080
linear scaling and linear scaling is

2029
01:33:27,080 --> 01:33:29,670
something which the neural net can

2030
01:33:29,670 --> 01:33:35,450
with very easily

2031
01:33:35,450 --> 01:33:42,140
good question okay so here is that image

2032
01:33:42,140 --> 01:33:44,250
right here is the image with

2033
01:33:44,250 --> 01:33:47,850
slash-and-burn water etc etc one of the

2034
01:33:47,850 --> 01:33:49,650
things to notice here is like when I

2035
01:33:49,650 --> 01:33:54,150
first displayed this image it was so

2036
01:33:54,150 --> 01:33:56,070
washed out I really couldn't see it

2037
01:33:56,070 --> 01:34:00,420
right but remember images now you know

2038
01:34:00,420 --> 01:34:02,970
we know images are just matrices of

2039
01:34:02,970 --> 01:34:04,890
numbers and so you can see here I just

2040
01:34:04,890 --> 01:34:08,670
said times 1.4 just to make it more

2041
01:34:08,670 --> 01:34:11,580
visible right so like now that you kind

2042
01:34:11,580 --> 01:34:12,930
of it's the kind of thing I want you to

2043
01:34:12,930 --> 01:34:14,820
get familiar with is the idea that this

2044
01:34:14,820 --> 01:34:15,840
stuff you're dealing with

2045
01:34:15,840 --> 01:34:18,060
they're just matrices of numbers and you

2046
01:34:18,060 --> 01:34:19,710
can fiddle around with them so if you're

2047
01:34:19,710 --> 01:34:21,120
looking at something like guys a bit

2048
01:34:21,120 --> 01:34:22,830
washed out you can just multiply it by

2049
01:34:22,830 --> 01:34:25,920
something to brighten it up a bit okay

2050
01:34:25,920 --> 01:34:27,990
so here we can see I guess this is the

2051
01:34:27,990 --> 01:34:30,510
slash-and-burn here's the river that's

2052
01:34:30,510 --> 01:34:33,300
the water here's the primary rainforest

2053
01:34:33,300 --> 01:34:35,400
maybe that's the agriculture so forth

2054
01:34:35,400 --> 01:34:41,010
okay so so you know with all that

2055
01:34:41,010 --> 01:34:44,300
background how do we actually use this

2056
01:34:44,300 --> 01:34:46,350
exactly the same way as everything we've

2057
01:34:46,350 --> 01:34:49,530
done before right so you know size and

2058
01:34:49,530 --> 01:34:52,170
and the interesting thing about playing

2059
01:34:52,170 --> 01:34:54,270
around with this planet competition is

2060
01:34:54,270 --> 01:34:56,370
that these images are not at all like

2061
01:34:56,370 --> 01:35:00,480
image there and I would guess that the

2062
01:35:00,480 --> 01:35:02,580
vast majority is of stuff that the vast

2063
01:35:02,580 --> 01:35:05,040
majority of you do involving

2064
01:35:05,040 --> 01:35:07,770
convolutional neural Nets won't actually

2065
01:35:07,770 --> 01:35:10,110
be anything like image net you know

2066
01:35:10,110 --> 01:35:14,130
it'll be it'll be medical imaging it'll

2067
01:35:14,130 --> 01:35:15,870
be like classifying different kinds of

2068
01:35:15,870 --> 01:35:18,530
steel tube or figuring out whether a

2069
01:35:18,530 --> 01:35:21,300
world you know is going to break or not

2070
01:35:21,300 --> 01:35:25,650
or or looking at satellite images or you

2071
01:35:25,650 --> 01:35:30,540
know whatever right so it's it's good to

2072
01:35:30,540 --> 01:35:32,690
experiment with stuff like this planet

2073
01:35:32,690 --> 01:35:35,190
competition to get a sense of kind of

2074
01:35:35,190 --> 01:35:37,170
what you want to do and so you'll see

2075
01:35:37,170 --> 01:35:40,440
here I start out by resizing my data to

2076
01:35:40,440 --> 01:35:44,910
64 by 64 it starts out at 256 by 256

2077
01:35:44,910 --> 01:35:46,410
right now

2078
01:35:46,410 --> 01:35:48,270
I wouldn't want to do this for the cats

2079
01:35:48,270 --> 01:35:50,280
and dogs competition because it cats end

2080
01:35:50,280 --> 01:35:52,620
on competition we start with a pre

2081
01:35:52,620 --> 01:35:54,360
trained imagenet Network it's it's

2082
01:35:54,360 --> 01:35:57,030
nearly isn't it starts off nearly

2083
01:35:57,030 --> 01:36:00,060
perfect right so if we resized

2084
01:36:00,060 --> 01:36:02,070
everything to 64 by 64 and then

2085
01:36:02,070 --> 01:36:04,980
retrained the whole set regular it we'd

2086
01:36:04,980 --> 01:36:07,440
basically destroy the weights that are

2087
01:36:07,440 --> 01:36:09,150
already pre trained to be very good

2088
01:36:09,150 --> 01:36:12,000
remember imagenet most imagenet models

2089
01:36:12,000 --> 01:36:14,400
are trained at either 224 by 224 or

2090
01:36:14,400 --> 01:36:17,760
$2.99 by 299 all right so if we like

2091
01:36:17,760 --> 01:36:20,220
retrain them at 64 by 64 we're going to

2092
01:36:20,220 --> 01:36:23,480
we're going to kill it on the other hand

2093
01:36:23,480 --> 01:36:25,920
there's nothing in image net that looks

2094
01:36:25,920 --> 01:36:28,050
anything like this you know there's no

2095
01:36:28,050 --> 01:36:31,260
satellite images so the only useful bits

2096
01:36:31,260 --> 01:36:36,030
of the image net Network for us are kind

2097
01:36:36,030 --> 01:36:39,870
of layers like this one you know finding

2098
01:36:39,870 --> 01:36:42,810
edges and gradients and this one you

2099
01:36:42,810 --> 01:36:44,460
know finding kind of textures and

2100
01:36:44,460 --> 01:36:48,090
repeating patterns and maybe these ones

2101
01:36:48,090 --> 01:36:49,890
are kind of finding more complex

2102
01:36:49,890 --> 01:36:52,550
textures but that's probably about it

2103
01:36:52,550 --> 01:36:57,530
right so so in other words you know

2104
01:36:57,530 --> 01:37:00,210
starting out by training very small

2105
01:37:00,210 --> 01:37:02,850
images works pretty well when you're

2106
01:37:02,850 --> 01:37:04,920
using stuff like satellites so in this

2107
01:37:04,920 --> 01:37:07,460
case I started right back at 64 by 64

2108
01:37:07,460 --> 01:37:13,410
grab some data built my model found out

2109
01:37:13,410 --> 01:37:15,780
what learning rate to use and

2110
01:37:15,780 --> 01:37:17,280
interestingly it turned out to be quite

2111
01:37:17,280 --> 01:37:22,110
high it seems that because like it's so

2112
01:37:22,110 --> 01:37:25,920
unlike imagenet I needed to do quite a

2113
01:37:25,920 --> 01:37:28,140
bit more fitting with just that last

2114
01:37:28,140 --> 01:37:30,390
layer before it started to flatten out

2115
01:37:30,390 --> 01:37:33,510
then I unfreeze dit and again this is

2116
01:37:33,510 --> 01:37:38,090
the difference to image net like

2117
01:37:38,090 --> 01:37:41,070
datasets is my learning rate in the

2118
01:37:41,070 --> 01:37:44,190
initial layer i set 2/9 the middle

2119
01:37:44,190 --> 01:37:47,190
layers I said 2/3 where else for stuff

2120
01:37:47,190 --> 01:37:48,930
like it's like image net I had a

2121
01:37:48,930 --> 01:37:52,410
multiple of 10 each of those you know

2122
01:37:52,410 --> 01:37:54,780
again the idea being that that earlier

2123
01:37:54,780 --> 01:37:58,140
layers probably and not as close to what

2124
01:37:58,140 --> 01:38:00,500
they need to be compared to the

2125
01:38:00,500 --> 01:38:04,620
like dances again

2126
01:38:04,620 --> 01:38:08,160
unfreeze train for a while and you can

2127
01:38:08,160 --> 01:38:09,120
kind of see here

2128
01:38:09,120 --> 01:38:11,790
you know there's cycle one there's cycle

2129
01:38:11,790 --> 01:38:14,820
- there's cycle three and then I kind of

2130
01:38:14,820 --> 01:38:18,440
increased double the size with my images

2131
01:38:18,440 --> 01:38:21,840
fit for a while and freeze fit for a

2132
01:38:21,840 --> 01:38:23,220
while double the size of the images

2133
01:38:23,220 --> 01:38:25,950
again fit for a while I'm freeze for a

2134
01:38:25,950 --> 01:38:28,740
while and then add TTA and so as I

2135
01:38:28,740 --> 01:38:30,090
mentioned last time we looked at this

2136
01:38:30,090 --> 01:38:32,940
this process ends up you know getting us

2137
01:38:32,940 --> 01:38:35,300
about 30th place in this competition

2138
01:38:35,300 --> 01:38:38,100
which is really cool because people you

2139
01:38:38,100 --> 01:38:39,440
know a lot of very very smart people

2140
01:38:39,440 --> 01:38:42,150
just a few months ago worked very very

2141
01:38:42,150 --> 01:38:46,830
hard on this competition a couple of

2142
01:38:46,830 --> 01:38:51,200
things people have asked about one is

2143
01:38:51,200 --> 01:38:57,420
what is this data dot resize do so a

2144
01:38:57,420 --> 01:38:59,160
couple of different pieces here the

2145
01:38:59,160 --> 01:39:05,520
first is that when we say back here what

2146
01:39:05,520 --> 01:39:07,650
transforms do we apply and here's our

2147
01:39:07,650 --> 01:39:10,080
transforms we actually pass in a size

2148
01:39:10,080 --> 01:39:12,810
right so one of the things that that one

2149
01:39:12,810 --> 01:39:14,520
of the things that data loaded does is

2150
01:39:14,520 --> 01:39:17,130
to resize the images like on-demand

2151
01:39:17,130 --> 01:39:21,510
every time it sees them

2152
01:39:21,510 --> 01:39:22,980
it's got nothing to do with that dot

2153
01:39:22,980 --> 01:39:25,710
resize method right so this is this is

2154
01:39:25,710 --> 01:39:27,239
the thing that happens at the end like

2155
01:39:27,239 --> 01:39:29,699
whatever's passed in before it hits out

2156
01:39:29,699 --> 01:39:31,380
that before our data loader spits it out

2157
01:39:31,380 --> 01:39:35,099
it's going to resize it to this size if

2158
01:39:35,099 --> 01:39:38,550
the initial input is like a thousand by

2159
01:39:38,550 --> 01:39:41,659
a thousand reading that JPEG and

2160
01:39:41,659 --> 01:39:45,989
resizing it to 64 by 64 turns out to

2161
01:39:45,989 --> 01:39:48,420
actually take more time than training

2162
01:39:48,420 --> 01:39:51,300
the content that's for each batch all

2163
01:39:51,300 --> 01:39:55,079
right so basically all resize does is it

2164
01:39:55,079 --> 01:39:57,150
says hey I'm not going to be using any

2165
01:39:57,150 --> 01:40:01,320
images bigger than size times 1.3 so

2166
01:40:01,320 --> 01:40:03,389
just grow through once and create new

2167
01:40:03,389 --> 01:40:08,400
JPEGs of this size right and they're

2168
01:40:08,400 --> 01:40:10,590
rectangular right so new JPEGs where the

2169
01:40:10,590 --> 01:40:14,070
smallest edges of this size and again

2170
01:40:14,070 --> 01:40:16,170
it's like you never have to do this

2171
01:40:16,170 --> 01:40:19,079
there's no reason to ever use it if you

2172
01:40:19,079 --> 01:40:21,389
don't want to it's just a speed-up okay

2173
01:40:21,389 --> 01:40:23,280
but if you've got really big images

2174
01:40:23,280 --> 01:40:25,289
coming in it saves you a lot of time and

2175
01:40:25,289 --> 01:40:27,300
you'll often see on like Carol kernels

2176
01:40:27,300 --> 01:40:30,389
or forum posts or whatever people will

2177
01:40:30,389 --> 01:40:34,409
have like bash script stuff like that -

2178
01:40:34,409 --> 01:40:36,809
like loop through and resize images to

2179
01:40:36,809 --> 01:40:38,519
save time you never have to do that

2180
01:40:38,519 --> 01:40:40,440
right just you can just say dot resize

2181
01:40:40,440 --> 01:40:44,219
and it'll just create you know once-off

2182
01:40:44,219 --> 01:40:46,139
it'll go through and create that if it's

2183
01:40:46,139 --> 01:40:48,389
already there and it'll use the

2184
01:40:48,389 --> 01:40:51,000
criticized ones for you okay so it's

2185
01:40:51,000 --> 01:40:53,880
just it's just a speed up convenience

2186
01:40:53,880 --> 01:41:00,420
function no more okay so for those of

2187
01:41:00,420 --> 01:41:05,119
you that are kind of past dog breeds I

2188
01:41:05,119 --> 01:41:09,210
would be looking at planet next you know

2189
01:41:09,210 --> 01:41:13,980
like try it like play around with with

2190
01:41:13,980 --> 01:41:15,840
trying to get a sense of like how can

2191
01:41:15,840 --> 01:41:19,170
you get this as an accurate model one

2192
01:41:19,170 --> 01:41:20,429
thing to mention and I'm not really

2193
01:41:20,429 --> 01:41:21,840
going to go into it in details there's

2194
01:41:21,840 --> 01:41:22,800
nothing to do with deep learning

2195
01:41:22,800 --> 01:41:24,900
particularly is that I'm using a

2196
01:41:24,900 --> 01:41:26,880
different metric I didn't use metrics

2197
01:41:26,880 --> 01:41:29,940
equals accuracy but I said metrics

2198
01:41:29,940 --> 01:41:33,980
equals f2

2199
01:41:33,980 --> 01:41:37,160
remember from last week that confusion

2200
01:41:37,160 --> 01:41:39,400
matrix that like two by two you know

2201
01:41:39,400 --> 01:41:41,930
correct incorrect for each of dogs and

2202
01:41:41,930 --> 01:41:47,090
cats there's a lot of different ways you

2203
01:41:47,090 --> 01:41:48,770
could turn that confusion matrix into a

2204
01:41:48,770 --> 01:41:51,140
score you know do you care more about

2205
01:41:51,140 --> 01:41:52,730
false negatives or do you care more

2206
01:41:52,730 --> 01:41:54,260
about false positives and how do you

2207
01:41:54,260 --> 01:41:55,730
weight them and how do you combine them

2208
01:41:55,730 --> 01:41:59,390
together right there's a basic there's

2209
01:41:59,390 --> 01:42:02,720
basically a function called F beta where

2210
01:42:02,720 --> 01:42:04,370
the beta says how much do you weight

2211
01:42:04,370 --> 01:42:06,170
false negatives versus false positives

2212
01:42:06,170 --> 01:42:11,540
and so f 2 is f beta with beta equals 2

2213
01:42:11,540 --> 01:42:13,730
and it's basically as particular way of

2214
01:42:13,730 --> 01:42:15,140
weighting false negatives and false

2215
01:42:15,140 --> 01:42:17,239
positives and the reason we use it is

2216
01:42:17,239 --> 01:42:19,700
because cattle told us that planet who

2217
01:42:19,700 --> 01:42:21,739
are running this competition wanted to

2218
01:42:21,739 --> 01:42:26,090
use this particular F beta metric the

2219
01:42:26,090 --> 01:42:28,730
important thing for you to know is that

2220
01:42:28,730 --> 01:42:32,060
you can create custom metrics so in this

2221
01:42:32,060 --> 01:42:33,110
case you can see here it says from

2222
01:42:33,110 --> 01:42:35,930
Planet import f2 and really I've got

2223
01:42:35,930 --> 01:42:38,120
this here so that you can see how to do

2224
01:42:38,120 --> 01:42:43,790
it right so if you look inside courses

2225
01:42:43,790 --> 01:42:47,270
DL 1 you can see there's something

2226
01:42:47,270 --> 01:42:51,800
called planet py right and so if I look

2227
01:42:51,800 --> 01:42:55,540
at planet py you'll see there's a

2228
01:42:55,540 --> 01:42:57,320
function there called

2229
01:42:57,320 --> 01:43:03,800
f2 right and so f2 simply calls F beta

2230
01:43:03,800 --> 01:43:08,690
score from psychic or side PI and patent

2231
01:43:08,690 --> 01:43:12,290
where it came from and does a couple

2232
01:43:12,290 --> 01:43:13,400
little tweets that are particularly

2233
01:43:13,400 --> 01:43:17,239
important but the important thing is

2234
01:43:17,239 --> 01:43:19,340
like you can write any metric you like

2235
01:43:19,340 --> 01:43:23,030
right as long as it takes in set of

2236
01:43:23,030 --> 01:43:25,970
predictions and a set of targets and

2237
01:43:25,970 --> 01:43:27,980
they're both going to be numpy arrays

2238
01:43:27,980 --> 01:43:30,530
one dimensional non pyros and then you

2239
01:43:30,530 --> 01:43:33,380
return back a number okay and so as long

2240
01:43:33,380 --> 01:43:35,260
as you put a function that takes two

2241
01:43:35,260 --> 01:43:38,930
vectors and returns at number you can

2242
01:43:38,930 --> 01:43:41,810
call it as a metric and so then when we

2243
01:43:41,810 --> 01:43:46,710
said

2244
01:43:46,710 --> 01:43:52,510
see here learn metrics equals and then

2245
01:43:52,510 --> 01:43:54,490
past in that array which just contains a

2246
01:43:54,490 --> 01:43:57,070
single function f2 then it's just going

2247
01:43:57,070 --> 01:44:00,580
to be printed out after every for you

2248
01:44:00,580 --> 01:44:03,430
okay so in general like the the faster I

2249
01:44:03,430 --> 01:44:06,970
library everything is customizable so

2250
01:44:06,970 --> 01:44:11,580
kind of the idea is that everything is

2251
01:44:11,580 --> 01:44:15,940
everything is kind of gives you what you

2252
01:44:15,940 --> 01:44:18,520
might want by default but also

2253
01:44:18,520 --> 01:44:23,170
everything can be changed as well yes

2254
01:44:23,170 --> 01:44:26,470
you know um we have a little confusion

2255
01:44:26,470 --> 01:44:30,010
about the difference between multi-label

2256
01:44:30,010 --> 01:44:32,890
and a single label uh-huh

2257
01:44:32,890 --> 01:44:34,930
the vanish as an example in which

2258
01:44:34,930 --> 01:44:38,140
compared like similarly the example they

2259
01:44:38,140 --> 01:44:42,970
just show us ah activation function yeah

2260
01:44:42,970 --> 01:44:47,080
so so I'm so sorry I said I'd do that

2261
01:44:47,080 --> 01:44:49,690
then I didn't so the activation the

2262
01:44:49,690 --> 01:44:52,180
output activation function for a single

2263
01:44:52,180 --> 01:44:55,330
label classification is softmax but all

2264
01:44:55,330 --> 01:44:58,390
the reasons that we talked today but if

2265
01:44:58,390 --> 01:45:00,100
we were trying to predict something that

2266
01:45:00,100 --> 01:45:05,350
was like 0 0 1 1 0 then softmax would be

2267
01:45:05,350 --> 01:45:07,060
a terrible choice because it's very hard

2268
01:45:07,060 --> 01:45:09,100
to come up with something where both of

2269
01:45:09,100 --> 01:45:11,470
these are high in fact it's impossible

2270
01:45:11,470 --> 01:45:13,510
because they have to add up to 1 so the

2271
01:45:13,510 --> 01:45:15,450
closest they could be would be point 5

2272
01:45:15,450 --> 01:45:19,690
so for multi-label classification our

2273
01:45:19,690 --> 01:45:23,650
activation function is called sigmoid ok

2274
01:45:23,650 --> 01:45:26,530
and again the faster library does this

2275
01:45:26,530 --> 01:45:28,690
automatically for you if it notices you

2276
01:45:28,690 --> 01:45:32,170
have a multi label problem and it does

2277
01:45:32,170 --> 01:45:34,210
that by checking your data tip to see if

2278
01:45:34,210 --> 01:45:36,430
anything has more than one label applied

2279
01:45:36,430 --> 01:45:40,870
to it and so sigmoid is a function which

2280
01:45:40,870 --> 01:45:44,350
is equal to it's basically the same

2281
01:45:44,350 --> 01:45:48,240
thing except rather than we never add up

2282
01:45:48,240 --> 01:45:52,060
all of these X but instead we just take

2283
01:45:52,060 --> 01:45:55,170
this X when we say it's just equal to it

2284
01:45:55,170 --> 01:45:59,829
divided by one plus

2285
01:45:59,829 --> 01:46:05,119
it and so the nice thing about that is

2286
01:46:05,119 --> 01:46:09,189
that now like multiple things can be

2287
01:46:09,189 --> 01:46:15,439
high at once right and so generally then

2288
01:46:15,439 --> 01:46:18,589
if something is less than zero its

2289
01:46:18,589 --> 01:46:21,229
sigmoid is going to be less than 0.5 if

2290
01:46:21,229 --> 01:46:23,089
it's greater than zero is signal it's

2291
01:46:23,089 --> 01:46:26,300
going to be greater than 0.5 and so the

2292
01:46:26,300 --> 01:46:27,949
important thing to know about a sigmoid

2293
01:46:27,949 --> 01:46:36,050
function is that its shape is

2294
01:46:36,050 --> 01:46:39,710
something which asymptotes at the top to

2295
01:46:39,710 --> 01:46:45,869
one and asymptotes drew

2296
01:46:45,869 --> 01:46:52,210
asymptotes at the bottom to zero and so

2297
01:46:52,210 --> 01:46:53,440
therefore it's a good thing to model a

2298
01:46:53,440 --> 01:46:57,070
probability with anybody who has done

2299
01:46:57,070 --> 01:47:01,960
any logistic regression will be familiar

2300
01:47:01,960 --> 01:47:03,219
with this is what we do in logistic

2301
01:47:03,219 --> 01:47:05,440
regression so it kind of appears

2302
01:47:05,440 --> 01:47:06,820
everywhere in machine learning and

2303
01:47:06,820 --> 01:47:08,949
you'll see that kind of a sigmoid and a

2304
01:47:08,949 --> 01:47:12,840
softmax they're very close to each other

2305
01:47:12,840 --> 01:47:15,880
conceptually but this is what we want is

2306
01:47:15,880 --> 01:47:18,280
our activation function for multi-label

2307
01:47:18,280 --> 01:47:19,869
and this is what we want the single

2308
01:47:19,869 --> 01:47:22,300
label and again first AI does it all for

2309
01:47:22,300 --> 01:47:22,659
you

2310
01:47:22,659 --> 01:47:31,119
there was a question over here yes I

2311
01:47:31,119 --> 01:47:34,059
have a question about the initial

2312
01:47:34,059 --> 01:47:35,979
training that you do if I understand

2313
01:47:35,979 --> 01:47:39,659
correctly you have we have frozen the

2314
01:47:39,659 --> 01:47:42,989
the premium model and you only need

2315
01:47:42,989 --> 01:47:46,679
initially try to train the latest

2316
01:47:46,679 --> 01:47:50,920
playwright right but from the other hand

2317
01:47:50,920 --> 01:47:53,889
we said that only the initial layer so

2318
01:47:53,889 --> 01:47:55,960
let's last probably the first layer is

2319
01:47:55,960 --> 01:47:58,809
like important to us and the other two

2320
01:47:58,809 --> 01:48:01,599
are more like features that are you must

2321
01:48:01,599 --> 01:48:03,400
not related and we then apply in this

2322
01:48:03,400 --> 01:48:07,119
case what that they the lie is a very

2323
01:48:07,119 --> 01:48:10,840
important but the pre-trained weights in

2324
01:48:10,840 --> 01:48:13,420
them aren't so it's the later layers

2325
01:48:13,420 --> 01:48:16,210
that we really want to train the most so

2326
01:48:16,210 --> 01:48:20,159
earlier layers likely to be like already

2327
01:48:20,159 --> 01:48:23,769
closer to what we want okay so you

2328
01:48:23,769 --> 01:48:25,449
started with the latest one and then you

2329
01:48:25,449 --> 01:48:27,519
go right so if you go back to our quick

2330
01:48:27,519 --> 01:48:31,630
dogs and cats right when we create a

2331
01:48:31,630 --> 01:48:33,820
model from pre train from a pre train

2332
01:48:33,820 --> 01:48:36,190
model it returns something where all of

2333
01:48:36,190 --> 01:48:38,280
the convolutional layers are frozen and

2334
01:48:38,280 --> 01:48:42,969
some randomly set fully connected layers

2335
01:48:42,969 --> 01:48:46,329
we add to the end our unfrozen and so

2336
01:48:46,329 --> 01:48:50,309
when we go fit at first it just trains

2337
01:48:50,309 --> 01:48:53,920
the randomly set a randomly initialized

2338
01:48:53,920 --> 01:48:56,920
fully connected letters right

2339
01:48:56,920 --> 01:48:59,739
and if something is like really close to

2340
01:48:59,739 --> 01:49:02,530
imagenet that's often all we need

2341
01:49:02,530 --> 01:49:04,330
but because the early early layers are

2342
01:49:04,330 --> 01:49:08,380
already good at finding edges gradients

2343
01:49:08,380 --> 01:49:11,949
repeating patterns for ears and dogs

2344
01:49:11,949 --> 01:49:16,830
heads you know so then when we unfreeze

2345
01:49:16,830 --> 01:49:19,570
we set the learning rates for the early

2346
01:49:19,570 --> 01:49:22,960
layers to be really low because we don't

2347
01:49:22,960 --> 01:49:25,030
want to change the mesh for us the later

2348
01:49:25,030 --> 01:49:28,270
ones we set them to be higher where else

2349
01:49:28,270 --> 01:49:32,830
for satellite data right this is no

2350
01:49:32,830 --> 01:49:34,690
longer true you know the early layers

2351
01:49:34,690 --> 01:49:37,420
are still like better than the later

2352
01:49:37,420 --> 01:49:40,420
layers but we still probably need to

2353
01:49:40,420 --> 01:49:42,730
change them quite a bit so that's right

2354
01:49:42,730 --> 01:49:45,460
this learning rate is nine times smaller

2355
01:49:45,460 --> 01:49:48,280
than the final learning rate rather than

2356
01:49:48,280 --> 01:49:52,150
a thousand times smaller the final loan

2357
01:49:52,150 --> 01:49:55,870
rate okay you play with with the weights

2358
01:49:55,870 --> 01:50:00,219
of the layers yeah normally most of the

2359
01:50:00,219 --> 01:50:01,810
stuff you see online if they talk about

2360
01:50:01,810 --> 01:50:03,989
this at all they'll talk about

2361
01:50:03,989 --> 01:50:07,300
unfreezing different subsets of layers

2362
01:50:07,300 --> 01:50:10,780
and indeed we do unfreeze our randomly

2363
01:50:10,780 --> 01:50:13,960
generated runs but what I found is

2364
01:50:13,960 --> 01:50:16,030
although the first layer library you can

2365
01:50:16,030 --> 01:50:18,250
type learn dot freeze too and just

2366
01:50:18,250 --> 01:50:20,949
freeze a subset of layers this approach

2367
01:50:20,949 --> 01:50:22,449
of using differential learning rates

2368
01:50:22,449 --> 01:50:26,350
seems to be like more flexible to the

2369
01:50:26,350 --> 01:50:28,179
point that I never find myself I'm

2370
01:50:28,179 --> 01:50:32,380
freezing subsets of layers that I would

2371
01:50:32,380 --> 01:50:34,960
expect you to start with that with a

2372
01:50:34,960 --> 01:50:37,900
different cell the different learning

2373
01:50:37,900 --> 01:50:40,270
rates rather than trying to learn the

2374
01:50:40,270 --> 01:50:43,090
last layer so the reason okay so you

2375
01:50:43,090 --> 01:50:48,190
could skip this training just the last

2376
01:50:48,190 --> 01:50:49,719
layers and just go straight to

2377
01:50:49,719 --> 01:50:52,090
differential learning rates but you

2378
01:50:52,090 --> 01:50:53,890
probably don't want to and the reason

2379
01:50:53,890 --> 01:50:55,510
you probably don't want to is that

2380
01:50:55,510 --> 01:50:57,610
there's a difference the convolutional

2381
01:50:57,610 --> 01:51:00,910
layers all contain pre-trained weights

2382
01:51:00,910 --> 01:51:03,910
so they're like they're not random for

2383
01:51:03,910 --> 01:51:05,290
things that are close to imagenet

2384
01:51:05,290 --> 01:51:07,060
they're actually really good for things

2385
01:51:07,060 --> 01:51:09,190
that are not close to imagenet they're

2386
01:51:09,190 --> 01:51:10,840
better than that

2387
01:51:10,840 --> 01:51:13,250
all of our fully connected layers

2388
01:51:13,250 --> 01:51:17,420
however are totally random so therefore

2389
01:51:17,420 --> 01:51:19,969
you would always want to make the fully

2390
01:51:19,969 --> 01:51:21,980
connected weights better than random by

2391
01:51:21,980 --> 01:51:24,050
training them a bit first because

2392
01:51:24,050 --> 01:51:26,650
otherwise if you go straight to unfreeze

2393
01:51:26,650 --> 01:51:28,840
then you're actually going to be like

2394
01:51:28,840 --> 01:51:31,699
fiddling around of those early early can

2395
01:51:31,699 --> 01:51:34,369
early layer weights when the later ones

2396
01:51:34,369 --> 01:51:35,809
are still random that's probably not

2397
01:51:35,809 --> 01:51:37,849
what you want I think that's another

2398
01:51:37,849 --> 01:51:41,900
question here any possible so when we

2399
01:51:41,900 --> 01:51:46,909
unfreeze what are the things we're

2400
01:51:46,909 --> 01:51:50,119
trying to change there will it change

2401
01:51:50,119 --> 01:51:54,349
the Colonel's themselves that that's

2402
01:51:54,349 --> 01:51:57,860
always what SGD does yeah so the only

2403
01:51:57,860 --> 01:52:02,869
thing what training means is setting

2404
01:52:02,869 --> 01:52:09,619
these numbers right and these numbers

2405
01:52:09,619 --> 01:52:18,139
and these numbers the weights so the

2406
01:52:18,139 --> 01:52:19,699
weights are the weights of the fully

2407
01:52:19,699 --> 01:52:22,460
connected layers and the weights in

2408
01:52:22,460 --> 01:52:24,320
those kernels and the convolutions so

2409
01:52:24,320 --> 01:52:27,469
that's what training means it's and

2410
01:52:27,469 --> 01:52:29,750
we'll learn about how to do it with SGD

2411
01:52:29,750 --> 01:52:32,059
but training literally is setting those

2412
01:52:32,059 --> 01:52:34,730
numbers these numbers on the other hand

2413
01:52:34,730 --> 01:52:38,059
are activations they're calculated

2414
01:52:38,059 --> 01:52:41,329
they're calculated from the weights and

2415
01:52:41,329 --> 01:52:44,449
the previous layers activations or

2416
01:52:44,449 --> 01:52:49,250
amounts of questions so you can lift it

2417
01:52:49,250 --> 01:52:51,230
up higher and speak badly so in your

2418
01:52:51,230 --> 01:52:52,670
example of a cheerleader set of that

2419
01:52:52,670 --> 01:52:55,880
English example so you start with very

2420
01:52:55,880 --> 01:52:58,610
small size existed for yeah so does it

2421
01:52:58,610 --> 01:53:00,739
literally mean you know the model takes

2422
01:53:00,739 --> 01:53:03,559
a small area from the entire image that

2423
01:53:03,559 --> 01:53:08,150
is 64 bytes so how do we get that 64 by

2424
01:53:08,150 --> 01:53:13,489
64 depends on the transforms by default

2425
01:53:13,489 --> 01:53:18,019
our transform takes the smallest edge

2426
01:53:18,019 --> 01:53:22,269
and recites the whole thing out

2427
01:53:22,269 --> 01:53:24,070
samples it so the smallest edge is

2428
01:53:24,070 --> 01:53:27,010
societics t4 and then it takes a Center

2429
01:53:27,010 --> 01:53:29,829
crop of that okay

2430
01:53:29,829 --> 01:53:33,099
although when we're using data

2431
01:53:33,099 --> 01:53:35,559
augmentation it actually takes a

2432
01:53:35,559 --> 01:53:40,539
randomly chosen prop ie the case where

2433
01:53:40,539 --> 01:53:43,030
the image ties to multiple objects don't

2434
01:53:43,030 --> 01:53:46,780
in this case like would it be possible

2435
01:53:46,780 --> 01:53:48,159
that you would just lose the other

2436
01:53:48,159 --> 01:53:50,170
things that they try to predict yeah

2437
01:53:50,170 --> 01:53:51,969
which is why data augmentation is

2438
01:53:51,969 --> 01:53:54,070
important so by by and particularly

2439
01:53:54,070 --> 01:53:57,130
their test time augmentation is going to

2440
01:53:57,130 --> 01:53:59,289
be particularly important because you

2441
01:53:59,289 --> 01:54:00,940
would you wouldn't want to you know that

2442
01:54:00,940 --> 01:54:03,880
there may be a artisanal mine out in the

2443
01:54:03,880 --> 01:54:06,130
corner which if you take a center crop

2444
01:54:06,130 --> 01:54:08,860
you you don't see so data augmentation

2445
01:54:08,860 --> 01:54:15,820
becomes very important yeah so when we

2446
01:54:15,820 --> 01:54:17,499
talk on their tributaries are he

2447
01:54:17,499 --> 01:54:21,039
receiver up to that's not really what a

2448
01:54:21,039 --> 01:54:23,230
model choice Delton that's a great point

2449
01:54:23,230 --> 01:54:25,179
that's not the loss function yeah right

2450
01:54:25,179 --> 01:54:27,039
the loss function is something we'll be

2451
01:54:27,039 --> 01:54:30,249
learning about next week and it uses

2452
01:54:30,249 --> 01:54:33,429
cross entropy or otherwise known as like

2453
01:54:33,429 --> 01:54:37,179
negative log likelihood the metric is

2454
01:54:37,179 --> 01:54:39,249
just this thing that's printed so we can

2455
01:54:39,249 --> 01:54:43,960
see what's going on just next to that so

2456
01:54:43,960 --> 01:54:47,079
in the context of my deep pass modeling

2457
01:54:47,079 --> 01:54:49,150
cannot change data does it trading it

2458
01:54:49,150 --> 01:54:51,280
also have to be multiplied so can I

2459
01:54:51,280 --> 01:54:53,110
train on just like images of pure cats

2460
01:54:53,110 --> 01:54:54,909
and dogs and expect it at prediction

2461
01:54:54,909 --> 01:54:57,610
time to predict if I give it a picture

2462
01:54:57,610 --> 01:55:04,030
of both having cat eye on it over I've

2463
01:55:04,030 --> 01:55:06,309
never tried that and I've never seen an

2464
01:55:06,309 --> 01:55:08,409
example of something that needed it

2465
01:55:08,409 --> 01:55:11,800
I guess conceptually there's no reason

2466
01:55:11,800 --> 01:55:14,949
it wouldn't work but it's kind of out

2467
01:55:14,949 --> 01:55:18,219
there and you still use a sigmoid you

2468
01:55:18,219 --> 01:55:19,389
would have to make sure you're using a

2469
01:55:19,389 --> 01:55:20,860
sigmoid loss function so in this case

2470
01:55:20,860 --> 01:55:22,659
faster eyes default would not work

2471
01:55:22,659 --> 01:55:24,579
because by default first day I would say

2472
01:55:24,579 --> 01:55:26,619
your training data knitter has both a

2473
01:55:26,619 --> 01:55:28,150
cat and the dog so you would have to

2474
01:55:28,150 --> 01:55:35,349
override the loss function

2475
01:55:35,349 --> 01:55:37,849
when you use the differential learning

2476
01:55:37,849 --> 01:55:40,729
rates those three learning rates do they

2477
01:55:40,729 --> 01:55:42,919
just kind of spread evenly across the

2478
01:55:42,919 --> 01:55:46,550
layers yeah we'll talk more about this

2479
01:55:46,550 --> 01:55:48,289
later in the course but I mean the

2480
01:55:48,289 --> 01:55:50,689
faster I library there's a concept of

2481
01:55:50,689 --> 01:55:53,570
layer groups so in something like a

2482
01:55:53,570 --> 01:55:55,579
resonant 50 you know there's hundreds of

2483
01:55:55,579 --> 01:55:57,889
layers and I think it you don't want to

2484
01:55:57,889 --> 01:56:00,709
write down hundreds of learning rates so

2485
01:56:00,709 --> 01:56:03,679
I've basically decided for you how to

2486
01:56:03,679 --> 01:56:07,489
split them and the the last one always

2487
01:56:07,489 --> 01:56:10,219
refers just to the fully connected

2488
01:56:10,219 --> 01:56:12,050
layers that we've randomly initialized

2489
01:56:12,050 --> 01:56:14,929
and edit to the end and then these ones

2490
01:56:14,929 --> 01:56:16,579
are split generally about halfway

2491
01:56:16,579 --> 01:56:19,669
through basically I've tried to make it

2492
01:56:19,669 --> 01:56:22,429
so that these you know these ones are

2493
01:56:22,429 --> 01:56:24,019
kind of the ones which you hardly want

2494
01:56:24,019 --> 01:56:25,400
to change at all and these are the ones

2495
01:56:25,400 --> 01:56:26,630
you might want to change a little bit

2496
01:56:26,630 --> 01:56:29,239
and I don't think we're covered in the

2497
01:56:29,239 --> 01:56:30,469
course but if you're interested we can

2498
01:56:30,469 --> 01:56:31,939
talk about in the forum there are ways

2499
01:56:31,939 --> 01:56:33,860
you can override this behavior to define

2500
01:56:33,860 --> 01:56:36,159
your own layer groups if you want to and

2501
01:56:36,159 --> 01:56:38,869
is there any way to visualize the model

2502
01:56:38,869 --> 01:56:41,419
easily or like don't dump the layers of

2503
01:56:41,419 --> 01:56:43,999
the model yeah absolutely

2504
01:56:43,999 --> 01:56:47,239
you can let's make sure we've got one

2505
01:56:47,239 --> 01:56:54,229
here okay so if you just type learn it

2506
01:56:54,229 --> 01:56:55,519
doesn't tell you much at all but what

2507
01:56:55,519 --> 01:57:02,479
you can do is go learn summary and that

2508
01:57:02,479 --> 01:57:07,579
spits out basically everything there's

2509
01:57:07,579 --> 01:57:09,590
all the letters and so you can see in

2510
01:57:09,590 --> 01:57:13,099
this case these are the names I

2511
01:57:13,099 --> 01:57:14,419
mentioned how they look up names right

2512
01:57:14,419 --> 01:57:17,469
so the first layer is called con 2 d 1

2513
01:57:17,469 --> 01:57:21,919
and it's going to take as input this is

2514
01:57:21,919 --> 01:57:24,050
useful to actually look at it's taking

2515
01:57:24,050 --> 01:57:27,110
64 by 64 images which is what we told it

2516
01:57:27,110 --> 01:57:28,999
we're going to transform things to this

2517
01:57:28,999 --> 01:57:33,260
is three channels high torch like most

2518
01:57:33,260 --> 01:57:35,209
things have channels at the end would

2519
01:57:35,209 --> 01:57:38,449
say 64 by 64 by 3 ply torch music to the

2520
01:57:38,449 --> 01:57:41,869
front so it's 3 by 64 by 64 that's

2521
01:57:41,869 --> 01:57:44,030
because it turns out that some of the

2522
01:57:44,030 --> 01:57:46,820
GPU computations run faster when it

2523
01:57:46,820 --> 01:57:49,070
in that order okay but that happens all

2524
01:57:49,070 --> 01:57:50,900
behind-the-scenes automatic plays a part

2525
01:57:50,900 --> 01:57:53,510
of that transformation stuff that's kind

2526
01:57:53,510 --> 01:57:56,260
of all done automatically is to do that

2527
01:57:56,260 --> 01:58:00,830
minus one means however however big the

2528
01:58:00,830 --> 01:58:04,280
batch size is in care us they use the

2529
01:58:04,280 --> 01:58:07,580
number they use a special number none in

2530
01:58:07,580 --> 01:58:09,740
pile types that used minus one so this

2531
01:58:09,740 --> 01:58:13,130
is a four dimensional mini batch the

2532
01:58:13,130 --> 01:58:15,920
number of elements in the amount of

2533
01:58:15,920 --> 01:58:18,290
images in the mini batch is dynamic you

2534
01:58:18,290 --> 01:58:20,270
can change that the number of channels

2535
01:58:20,270 --> 01:58:23,660
is three number which is a 64 by 64 okay

2536
01:58:23,660 --> 01:58:26,360
and so then you can basically see that

2537
01:58:26,360 --> 01:58:29,200
this particular convolutional kernel

2538
01:58:29,200 --> 01:58:32,960
apparently has 64 kernels in it and it's

2539
01:58:32,960 --> 01:58:35,240
also having we haven't talked about this

2540
01:58:35,240 --> 01:58:36,500
but convolutions can have something

2541
01:58:36,500 --> 01:58:39,200
called a stride that is like Matt pullin

2542
01:58:39,200 --> 01:58:41,390
it changes the size so it's returning a

2543
01:58:41,390 --> 01:58:47,870
32 by 32 564 kernel tenza and so on and

2544
01:58:47,870 --> 01:58:51,950
so forth so that's summary and we'll

2545
01:58:51,950 --> 01:58:54,080
learn all about that's doing in detail

2546
01:58:54,080 --> 01:58:57,620
on in the second half of the course one

2547
01:58:57,620 --> 01:59:00,770
where I clicked in my own data set and I

2548
01:59:00,770 --> 01:59:02,720
try to use the in as a really small data

2549
01:59:02,720 --> 01:59:06,260
set these currencies from Google Images

2550
01:59:06,260 --> 01:59:09,470
and I tried to do a learning rate find

2551
01:59:09,470 --> 01:59:12,650
and then the plot and it just it gave me

2552
01:59:12,650 --> 01:59:14,030
some numbers which I didn't understand

2553
01:59:14,030 --> 01:59:15,980
and the learning rate font yeah and then

2554
01:59:15,980 --> 01:59:18,260
the plot was empty so yeah I mean let's

2555
01:59:18,260 --> 01:59:20,030
let's talk about that on the forum but

2556
01:59:20,030 --> 01:59:22,910
basically the learning rate finder is

2557
01:59:22,910 --> 01:59:24,290
going to go through a mini batch at a

2558
01:59:24,290 --> 01:59:26,450
time if you've got a tiny data set

2559
01:59:26,450 --> 01:59:28,370
there's just not enough mini batches so

2560
01:59:28,370 --> 01:59:30,560
the trick is to make your mini bit make

2561
01:59:30,560 --> 01:59:32,720
your batch size really small like try

2562
01:59:32,720 --> 01:59:38,250
making it like four

2563
01:59:38,250 --> 01:59:40,480
okay they were great questions it's not

2564
01:59:40,480 --> 01:59:44,440
nothing online to add you know they were

2565
01:59:44,440 --> 01:59:45,580
great questions we've got a little bit

2566
01:59:45,580 --> 01:59:48,160
past where I hope to but let's let's

2567
01:59:48,160 --> 01:59:51,430
quickly talk about structured data so we

2568
01:59:51,430 --> 01:59:52,690
can start thinking about it for next

2569
01:59:52,690 --> 01:59:59,770
week so this is really weird right to me

2570
01:59:59,770 --> 02:00:02,500
there's basically two types of data set

2571
02:00:02,500 --> 02:00:04,090
we use in machine learning there's a

2572
02:00:04,090 --> 02:00:10,300
type of data like audio images natural

2573
02:00:10,300 --> 02:00:14,320
language text where all of the all of

2574
02:00:14,320 --> 02:00:16,600
the things inside an object like all of

2575
02:00:16,600 --> 02:00:18,940
the pixels inside an image are all the

2576
02:00:18,940 --> 02:00:21,850
same kind of thing they're all pixels or

2577
02:00:21,850 --> 02:00:26,410
they're all apertures of a waveform or

2578
02:00:26,410 --> 02:00:30,130
they're all words I call this kind of

2579
02:00:30,130 --> 02:00:33,040
data unstructured and then there's data

2580
02:00:33,040 --> 02:00:37,199
sets like a profit and loss statement or

2581
02:00:37,199 --> 02:00:39,960
the information about a Facebook user

2582
02:00:39,960 --> 02:00:43,960
where each column is like structurally

2583
02:00:43,960 --> 02:00:45,520
quite different you know one thing is

2584
02:00:45,520 --> 02:00:47,260
representing like how many page views

2585
02:00:47,260 --> 02:00:49,449
last month another one is their sex

2586
02:00:49,449 --> 02:00:51,550
another one is what zip code they're in

2587
02:00:51,550 --> 02:00:55,719
and I call this structure there that

2588
02:00:55,719 --> 02:00:57,940
particular terminology is not unusual

2589
02:00:57,940 --> 02:01:00,480
like lots of people use that terminology

2590
02:01:00,480 --> 02:01:02,410
but lots of people don't

2591
02:01:02,410 --> 02:01:05,489
there's no particularly agreed-upon

2592
02:01:05,489 --> 02:01:08,800
terminology so when I say structured

2593
02:01:08,800 --> 02:01:11,770
data I'm referring to kind of columnar

2594
02:01:11,770 --> 02:01:14,620
data as you might find in a database or

2595
02:01:14,620 --> 02:01:16,210
a spreadsheet where different columns

2596
02:01:16,210 --> 02:01:19,000
represent different kinds of things and

2597
02:01:19,000 --> 02:01:21,840
each row represents an observation and

2598
02:01:21,840 --> 02:01:25,630
so structured data is probably what most

2599
02:01:25,630 --> 02:01:31,199
of you are analyzing most of the time

2600
02:01:31,199 --> 02:01:34,810
funnily enough you know academics in the

2601
02:01:34,810 --> 02:01:37,180
deep learning world don't really give a

2602
02:01:37,180 --> 02:01:39,730
 about structured data because it's

2603
02:01:39,730 --> 02:01:41,590
pretty hard to get published in fancy

2604
02:01:41,590 --> 02:01:44,260
conference proceedings if you're like if

2605
02:01:44,260 --> 02:01:46,179
you've got a better logistics model you

2606
02:01:46,179 --> 02:01:47,830
know it's the thing that makes the world

2607
02:01:47,830 --> 02:01:49,719
goes round it's a thing that makes

2608
02:01:49,719 --> 02:01:51,139
everybody you know

2609
02:01:51,139 --> 02:01:55,880
and efficiency and make stuff work but

2610
02:01:55,880 --> 02:02:00,320
it's largely ignored sadly so we're not

2611
02:02:00,320 --> 02:02:01,309
going to ignore it because we're a

2612
02:02:01,309 --> 02:02:04,189
practical deep learning and cackled

2613
02:02:04,189 --> 02:02:05,749
doesn't ignore it either because people

2614
02:02:05,749 --> 02:02:08,090
put prize money up on Cagle to solve

2615
02:02:08,090 --> 02:02:10,070
real-world problems so there are some

2616
02:02:10,070 --> 02:02:11,899
great capable competitions we can look

2617
02:02:11,899 --> 02:02:14,780
at there's one running right now which

2618
02:02:14,780 --> 02:02:16,519
is the grocery sales forecasting

2619
02:02:16,519 --> 02:02:21,399
competition for Ecuador's largest chain

2620
02:02:21,399 --> 02:02:24,559
it's always a little I've got to be a

2621
02:02:24,559 --> 02:02:26,360
little careful about how much I show you

2622
02:02:26,360 --> 02:02:28,099
about currently running competitions

2623
02:02:28,099 --> 02:02:29,840
because I don't want to you know help

2624
02:02:29,840 --> 02:02:32,659
you cheat but it so happens there was a

2625
02:02:32,659 --> 02:02:35,989
competition a year or two ago for one of

2626
02:02:35,989 --> 02:02:38,269
Germany's magistrate chains which is

2627
02:02:38,269 --> 02:02:40,099
almost identical so I'm going to show

2628
02:02:40,099 --> 02:02:45,380
you how to do that so that was called

2629
02:02:45,380 --> 02:02:50,539
the Rossman stores data and so I would

2630
02:02:50,539 --> 02:02:52,489
suggest you know first of all try

2631
02:02:52,489 --> 02:02:53,780
practicing what we're learning on

2632
02:02:53,780 --> 02:02:56,119
Russman right but then see if you can

2633
02:02:56,119 --> 02:02:58,579
get it working on on grocery because

2634
02:02:58,579 --> 02:03:02,119
currently on the leaderboard no one

2635
02:03:02,119 --> 02:03:03,320
seems to basically know what they're

2636
02:03:03,320 --> 02:03:05,149
doing in the groceries competition if

2637
02:03:05,149 --> 02:03:10,130
you look at the leaderboard the let's

2638
02:03:10,130 --> 02:03:13,280
see here yeah these ones around 5 to 9 v

2639
02:03:13,280 --> 02:03:15,769
3o are people that are literally finding

2640
02:03:15,769 --> 02:03:18,019
like group averages and submitting those

2641
02:03:18,019 --> 02:03:19,849
I know because they're kernels that

2642
02:03:19,849 --> 02:03:22,369
they're using so you know the basically

2643
02:03:22,369 --> 02:03:24,369
the people around 20th place are not

2644
02:03:24,369 --> 02:03:29,059
actually doing any machine learning so

2645
02:03:29,059 --> 02:03:31,749
yeah let's see if we can improve things

2646
02:03:31,749 --> 02:03:34,639
so you'll see there's a less than 3

2647
02:03:34,639 --> 02:03:38,030
rossmann notebook sure you get pull ok

2648
02:03:38,030 --> 02:03:40,189
in fact you know just reminder you know

2649
02:03:40,189 --> 02:03:42,800
before you start working get pull in

2650
02:03:42,800 --> 02:03:45,289
you're faster a repo and from time to

2651
02:03:45,289 --> 02:03:48,889
time Condor and update for you guys

2652
02:03:48,889 --> 02:03:51,019
during the in-person course the Condor

2653
02:03:51,019 --> 02:03:52,849
and update you should do it more often

2654
02:03:52,849 --> 02:03:55,309
because we're kind of changing things a

2655
02:03:55,309 --> 02:03:57,769
little bit um folks in the MOOC you know

2656
02:03:57,769 --> 02:04:01,670
more like once a month should be fine

2657
02:04:01,670 --> 02:04:04,010
so anyway I just just changed this a

2658
02:04:04,010 --> 02:04:05,210
little bit so make sure you get Paul to

2659
02:04:05,210 --> 02:04:09,560
get lesson 3 Rossman and there's a

2660
02:04:09,560 --> 02:04:11,390
couple of new libraries here one is fast

2661
02:04:11,390 --> 02:04:14,600
AI dot structure faster guided

2662
02:04:14,600 --> 02:04:16,370
structured contain stuff which is

2663
02:04:16,370 --> 02:04:18,950
actually not at all high torch specific

2664
02:04:18,950 --> 02:04:20,600
and we actually use that in the machine

2665
02:04:20,600 --> 02:04:23,210
learning course as well for doing random

2666
02:04:23,210 --> 02:04:25,000
forests with no tie torch at all I

2667
02:04:25,000 --> 02:04:27,440
mentioned that because you can use that

2668
02:04:27,440 --> 02:04:30,230
particular library without any of the

2669
02:04:30,230 --> 02:04:33,530
other parts of fast AI so that can be

2670
02:04:33,530 --> 02:04:36,080
handy and then we're also going to use

2671
02:04:36,080 --> 02:04:39,020
faster column data which is basically

2672
02:04:39,020 --> 02:04:41,630
some stuff that allows us to do fast a

2673
02:04:41,630 --> 02:04:44,540
type a torch stuff with columnar

2674
02:04:44,540 --> 02:04:49,100
structured data for structured data we

2675
02:04:49,100 --> 02:04:52,730
need to use pandas a lot anybody who's

2676
02:04:52,730 --> 02:04:54,710
used our data frames will be very

2677
02:04:54,710 --> 02:04:56,720
familiar with pandas pandas is basically

2678
02:04:56,720 --> 02:04:59,240
an attempt to kind of replicate data

2679
02:04:59,240 --> 02:05:03,200
friends in Python you know and a bit

2680
02:05:03,200 --> 02:05:08,240
more if you're not entirely familiar

2681
02:05:08,240 --> 02:05:14,730
with pandas there's a great book

2682
02:05:14,730 --> 02:05:16,849
[Music]

2683
02:05:16,849 --> 02:05:18,020
which I think I might have mentioned

2684
02:05:18,020 --> 02:05:23,150
before - for data analysis by Wes

2685
02:05:23,150 --> 02:05:24,920
McKinney there's a new addition that

2686
02:05:24,920 --> 02:05:27,849
just came out a couple of weeks ago

2687
02:05:27,849 --> 02:05:30,619
obviously being by the pandas author its

2688
02:05:30,619 --> 02:05:32,810
coverage of pandas is excellent but it

2689
02:05:32,810 --> 02:05:37,340
also covers numpy scipy matplotlib

2690
02:05:37,340 --> 02:05:42,199
scikit-learn - and Jupiter really well

2691
02:05:42,199 --> 02:05:45,940
okay and so I'm kind of going to assume

2692
02:05:45,940 --> 02:05:48,949
that you know your way around these

2693
02:05:48,949 --> 02:05:52,820
libraries to some extent also there was

2694
02:05:52,820 --> 02:05:54,980
the workshop we did before they started

2695
02:05:54,980 --> 02:05:56,630
and there's a video of that online where

2696
02:05:56,630 --> 02:05:59,030
we kind of have a brief mention of all

2697
02:05:59,030 --> 02:06:03,980
of those tools structured data is

2698
02:06:03,980 --> 02:06:06,710
generally shared as CSV files it was no

2699
02:06:06,710 --> 02:06:08,900
different in this competition as you'll

2700
02:06:08,900 --> 02:06:10,610
see there's a hyperlink to the rustman

2701
02:06:10,610 --> 02:06:14,210
data set here right now if you look at

2702
02:06:14,210 --> 02:06:15,500
the bottom of my screen you'll see this

2703
02:06:15,500 --> 02:06:17,900
goes to file start faster day.i because

2704
02:06:17,900 --> 02:06:19,579
this doesn't require any login or

2705
02:06:19,579 --> 02:06:21,980
anything to grab this data set it's as

2706
02:06:21,980 --> 02:06:24,409
simple as right clicking copy link

2707
02:06:24,409 --> 02:06:26,960
address head over to wherever you want

2708
02:06:26,960 --> 02:06:36,159
it and just type W get and the URL okay

2709
02:06:36,159 --> 02:06:40,040
so that's because you know it's it's not

2710
02:06:40,040 --> 02:06:44,150
behind a login or anything so you can

2711
02:06:44,150 --> 02:06:47,780
grab the grab it from there and you can

2712
02:06:47,780 --> 02:06:49,820
always read a CSV file with just pandas

2713
02:06:49,820 --> 02:06:52,040
don't read CSV now in this particular

2714
02:06:52,040 --> 02:06:55,639
case there's a lot of pre-processing

2715
02:06:55,639 --> 02:06:57,920
that we do and what I've actually done

2716
02:06:57,920 --> 02:07:03,710
here is I've I've actually stolen the

2717
02:07:03,710 --> 02:07:06,409
entire pipeline from the third place

2718
02:07:06,409 --> 02:07:08,780
winner roster okay so they made all

2719
02:07:08,780 --> 02:07:11,119
their data they're really great you know

2720
02:07:11,119 --> 02:07:12,440
they better get hub available with

2721
02:07:12,440 --> 02:07:14,420
everything that we need and I've ported

2722
02:07:14,420 --> 02:07:16,880
it all across and simplified it and

2723
02:07:16,880 --> 02:07:18,829
tried to make it pretty easy to

2724
02:07:18,829 --> 02:07:23,030
understand this course is about deep

2725
02:07:23,030 --> 02:07:25,820
learning not about data processing so

2726
02:07:25,820 --> 02:07:28,460
I'm not going to go through it but we

2727
02:07:28,460 --> 02:07:29,929
will be going through it in the machine

2728
02:07:29,929 --> 02:07:30,840
learning course

2729
02:07:30,840 --> 02:07:32,580
in some detail because feature

2730
02:07:32,580 --> 02:07:34,469
engineering is really important so if

2731
02:07:34,469 --> 02:07:37,110
you're interested you know check out the

2732
02:07:37,110 --> 02:07:41,520
machine learning course for that I will

2733
02:07:41,520 --> 02:07:44,190
however show you kind of what it looks

2734
02:07:44,190 --> 02:07:48,780
like so once we read the CSV Xin you can

2735
02:07:48,780 --> 02:07:50,909
see basically what's there so the key

2736
02:07:50,909 --> 02:08:01,739
one is for a particular store we have

2737
02:08:01,739 --> 02:08:05,400
the

2738
02:08:05,400 --> 02:08:09,170
we have the date and we have the sales

2739
02:08:09,170 --> 02:08:12,270
for that particular store we know

2740
02:08:12,270 --> 02:08:18,239
whether that thing is on promo or not we

2741
02:08:18,239 --> 02:08:19,860
know the number of customers at that

2742
02:08:19,860 --> 02:08:23,429
particular store had we know whether

2743
02:08:23,429 --> 02:08:29,940
that date was a school holiday

2744
02:08:29,940 --> 02:08:36,520
we also know what kind of store it is so

2745
02:08:36,520 --> 02:08:38,320
this is pretty common right you'll often

2746
02:08:38,320 --> 02:08:41,530
get datasets where there's some column

2747
02:08:41,530 --> 02:08:43,360
with like just some kind of code we

2748
02:08:43,360 --> 02:08:44,170
don't really know what the code means

2749
02:08:44,170 --> 02:08:47,410
and most of the time I find it doesn't

2750
02:08:47,410 --> 02:08:49,450
matter what it means like normally you

2751
02:08:49,450 --> 02:08:51,760
get given a data dictionary when you

2752
02:08:51,760 --> 02:08:53,170
start on a project and obviously if

2753
02:08:53,170 --> 02:08:54,550
you're working on an internal project

2754
02:08:54,550 --> 02:08:56,380
you can ask the people at your company

2755
02:08:56,380 --> 02:08:59,740
what does this column mean I kind of

2756
02:08:59,740 --> 02:09:01,810
stay away from learning too much about

2757
02:09:01,810 --> 02:09:03,400
it I prefer to like to see what the data

2758
02:09:03,400 --> 02:09:08,080
says first there's something about what

2759
02:09:08,080 --> 02:09:10,210
kind of product are we selling in this

2760
02:09:10,210 --> 02:09:13,989
particular row

2761
02:09:13,989 --> 02:09:16,230
and then there's information about like

2762
02:09:16,230 --> 02:09:19,239
how far away is the nearest competitor

2763
02:09:19,239 --> 02:09:24,820
how long have they been open for how

2764
02:09:24,820 --> 02:09:31,120
long is the promo being on for for each

2765
02:09:31,120 --> 02:09:32,920
store we can find out what state it's in

2766
02:09:32,920 --> 02:09:35,380
for each state we can find at the name

2767
02:09:35,380 --> 02:09:38,670
of the state this is in Germany and

2768
02:09:38,670 --> 02:09:40,540
interestingly they were allowed to

2769
02:09:40,540 --> 02:09:42,640
download any data external data they

2770
02:09:42,640 --> 02:09:44,380
wanted in this competition just very

2771
02:09:44,380 --> 02:09:46,150
common as long as you share it with

2772
02:09:46,150 --> 02:09:48,520
everybody else and so some folks tried

2773
02:09:48,520 --> 02:09:53,830
downloading data from Google Trends I'm

2774
02:09:53,830 --> 02:09:55,180
not sure exactly what it was that they

2775
02:09:55,180 --> 02:09:56,950
would check in the trend off but we have

2776
02:09:56,950 --> 02:09:59,430
this information from Google Trends

2777
02:09:59,430 --> 02:10:01,810
somebody downloaded the weather for

2778
02:10:01,810 --> 02:10:09,250
every day in Germany every state

2779
02:10:09,250 --> 02:10:17,320
and yeah that's about it right so you

2780
02:10:17,320 --> 02:10:20,710
can get a data frame summary with pandas

2781
02:10:20,710 --> 02:10:22,690
which kind of lets you see how many

2782
02:10:22,690 --> 02:10:24,430
observations and means and standard

2783
02:10:24,430 --> 02:10:27,190
deviations again I don't do a hell of a

2784
02:10:27,190 --> 02:10:30,130
lot with that early on but it's nice to

2785
02:10:30,130 --> 02:10:34,000
note there so what we do you know this

2786
02:10:34,000 --> 02:10:35,530
is called a relational data set a

2787
02:10:35,530 --> 02:10:37,420
relational data set is one where there's

2788
02:10:37,420 --> 02:10:38,920
quite a few tables we have to join

2789
02:10:38,920 --> 02:10:41,410
together it's very easy to do that in

2790
02:10:41,410 --> 02:10:43,780
pandas there's a thing called merge so

2791
02:10:43,780 --> 02:10:45,280
great little function to do that and so

2792
02:10:45,280 --> 02:10:46,360
I just started joining everything

2793
02:10:46,360 --> 02:10:46,750
together

2794
02:10:46,750 --> 02:10:49,380
joining the weather or the Google Trends

2795
02:10:49,380 --> 02:10:57,280
stores yeah that's about everything I

2796
02:10:57,280 --> 02:10:59,530
guess you'll see there's one thing that

2797
02:10:59,530 --> 02:11:02,650
I'm using from the FASTA a library which

2798
02:11:02,650 --> 02:11:04,300
is called add date part we talked about

2799
02:11:04,300 --> 02:11:06,100
this a lot in the machine learning

2800
02:11:06,100 --> 02:11:07,630
course but basically this is going to

2801
02:11:07,630 --> 02:11:10,570
take a date and pull out of a bunch of

2802
02:11:10,570 --> 02:11:12,970
columns day of week is at the start of a

2803
02:11:12,970 --> 02:11:16,000
quarter month of year so on and so forth

2804
02:11:16,000 --> 02:11:18,460
and add them all in to the dataset okay

2805
02:11:18,460 --> 02:11:22,740
so this is all standard pre-processing

2806
02:11:22,740 --> 02:11:24,640
all right so we join everything together

2807
02:11:24,640 --> 02:11:26,800
we fiddle around with some of the dates

2808
02:11:26,800 --> 02:11:28,330
a little bit some of them are in month

2809
02:11:28,330 --> 02:11:29,950
in year format we turn it into date

2810
02:11:29,950 --> 02:11:35,400
format we spend a lot of time trying to

2811
02:11:35,400 --> 02:11:37,930
take information about for example

2812
02:11:37,930 --> 02:11:40,690
holidays and add a column for like how

2813
02:11:40,690 --> 02:11:42,940
long until the next holiday how long has

2814
02:11:42,940 --> 02:11:45,130
it been since the last holiday ditto for

2815
02:11:45,130 --> 02:11:49,870
promos so on and so forth okay so we do

2816
02:11:49,870 --> 02:11:52,180
all that and at the very end we

2817
02:11:52,180 --> 02:11:54,610
basically save a big structured data

2818
02:11:54,610 --> 02:11:57,630
file that contains all that stuff

2819
02:11:57,630 --> 02:11:59,590
something that those of you that use

2820
02:11:59,590 --> 02:12:01,300
pandas may not be aware of is that

2821
02:12:01,300 --> 02:12:03,100
there's a very cool new format called

2822
02:12:03,100 --> 02:12:05,800
feather which you can save a panda's

2823
02:12:05,800 --> 02:12:08,890
data frame into this feather format it's

2824
02:12:08,890 --> 02:12:11,170
kind of pretty much takes it as it sits

2825
02:12:11,170 --> 02:12:13,960
in RAM and dumps it to the disk and so

2826
02:12:13,960 --> 02:12:16,420
it's like really really really fast the

2827
02:12:16,420 --> 02:12:18,130
reason that you need to know this is

2828
02:12:18,130 --> 02:12:20,770
because the ecuadorian grocery

2829
02:12:20,770 --> 02:12:22,720
competition is on now has

2830
02:12:22,720 --> 02:12:26,410
350 million records so you will care

2831
02:12:26,410 --> 02:12:28,420
about how long things take a talk I

2832
02:12:28,420 --> 02:12:30,850
believe about six seconds for me to save

2833
02:12:30,850 --> 02:12:32,410
three hundred and fifty million records

2834
02:12:32,410 --> 02:12:35,010
to feather format so that's pretty cool

2835
02:12:35,010 --> 02:12:37,540
so at the end of all that I'd save it as

2836
02:12:37,540 --> 02:12:39,070
a feather format and for the rest of

2837
02:12:39,070 --> 02:12:40,540
this discussion I'm just going to take

2838
02:12:40,540 --> 02:12:43,170
it as given that we've got this nicely

2839
02:12:43,170 --> 02:12:45,400
pre-processed feature engineered file

2840
02:12:45,400 --> 02:12:48,790
and I can just go read better okay but

2841
02:12:48,790 --> 02:12:50,650
for you to play along at home you will

2842
02:12:50,650 --> 02:12:53,220
have to run those previous cells oh

2843
02:12:53,220 --> 02:12:57,580
except the see these ones have commented

2844
02:12:57,580 --> 02:12:59,950
out you don't have to run those because

2845
02:12:59,950 --> 02:13:01,690
the file that you download from files

2846
02:13:01,690 --> 02:13:03,730
doc bastard AI has already done that for

2847
02:13:03,730 --> 02:13:09,400
you okay all right so we basically have

2848
02:13:09,400 --> 02:13:14,740
all these columns so it basically is

2849
02:13:14,740 --> 02:13:18,510
going to tell us you know how many of

2850
02:13:18,510 --> 02:13:23,830
this thing was sold on this date at this

2851
02:13:23,830 --> 02:13:25,780
store and so the goal of this

2852
02:13:25,780 --> 02:13:28,990
competition is to find out how many

2853
02:13:28,990 --> 02:13:32,590
things will be sold for each store for

2854
02:13:32,590 --> 02:13:36,160
each type of thing in the future okay

2855
02:13:36,160 --> 02:13:38,200
and so that's basically what we're going

2856
02:13:38,200 --> 02:13:40,900
to be trying to do and so here's an

2857
02:13:40,900 --> 02:13:42,340
example of what some of the data looks

2858
02:13:42,340 --> 02:13:48,640
like and so next week we're going to see

2859
02:13:48,640 --> 02:13:50,620
how to go through these steps but

2860
02:13:50,620 --> 02:13:52,300
basically what we're going to learn is

2861
02:13:52,300 --> 02:13:55,150
we're going to learn to split the

2862
02:13:55,150 --> 02:13:58,480
columns into two types some columns were

2863
02:13:58,480 --> 02:14:01,390
going to treat as categorical which is

2864
02:14:01,390 --> 02:14:06,310
to say store ID one and store ID - I'm

2865
02:14:06,310 --> 02:14:08,050
not numerically related to each other

2866
02:14:08,050 --> 02:14:11,050
they're categories right we're going to

2867
02:14:11,050 --> 02:14:13,150
treat day of week like that - Monday and

2868
02:14:13,150 --> 02:14:15,490
Tuesday day zero and day one not

2869
02:14:15,490 --> 02:14:17,800
numerically related to each other where

2870
02:14:17,800 --> 02:14:20,620
else distance in kilometers to the

2871
02:14:20,620 --> 02:14:24,100
nearest competitor that's a number that

2872
02:14:24,100 --> 02:14:26,170
we're going to treat numerically right

2873
02:14:26,170 --> 02:14:28,090
so in other words the categorical

2874
02:14:28,090 --> 02:14:29,770
variables we basically are going to one

2875
02:14:29,770 --> 02:14:32,410
how to encode them you can think of it

2876
02:14:32,410 --> 02:14:34,039
as one hot encoding on where

2877
02:14:34,039 --> 02:14:35,839
the continuous variables we're going to

2878
02:14:35,839 --> 02:14:38,199
be feeding into fully connected layers

2879
02:14:38,199 --> 02:14:43,609
just as is okay so what we'll be doing

2880
02:14:43,609 --> 02:14:47,199
is we'll be basically creating a

2881
02:14:47,199 --> 02:14:49,849
validation set and you'll see like a lot

2882
02:14:49,849 --> 02:14:51,109
of these are start to look familiar this

2883
02:14:51,109 --> 02:14:53,419
is the same function we used on planet

2884
02:14:53,419 --> 02:14:54,739
and dog breeds to create a validation

2885
02:14:54,739 --> 02:14:58,669
set there's some stuff that you haven't

2886
02:14:58,669 --> 02:14:59,659
seen before

2887
02:14:59,659 --> 02:15:02,899
where we're going to basically rather

2888
02:15:02,899 --> 02:15:06,019
than saying image data dot from CSV

2889
02:15:06,019 --> 02:15:09,109
we're going to say columnar data from

2890
02:15:09,109 --> 02:15:11,030
data frame right so you can see like the

2891
02:15:11,030 --> 02:15:13,780
basic API concepts will be the same but

2892
02:15:13,780 --> 02:15:17,689
they're a little different right but

2893
02:15:17,689 --> 02:15:18,919
just like before we're going to get a

2894
02:15:18,919 --> 02:15:23,659
learner and we're going to go lr find to

2895
02:15:23,659 --> 02:15:26,359
find our best learning rate and then

2896
02:15:26,359 --> 02:15:28,749
we're going to go dot fit with a metric

2897
02:15:28,749 --> 02:15:33,019
with a cycle length okay so the basic

2898
02:15:33,019 --> 02:15:35,469
sequence who's going to end up looking

2899
02:15:35,469 --> 02:15:39,079
hopefully very familiar okay so we're

2900
02:15:39,079 --> 02:15:40,339
out of time

2901
02:15:40,339 --> 02:15:42,409
so what I suggest you do this week is

2902
02:15:42,409 --> 02:15:47,479
like try to enter as many capital image

2903
02:15:47,479 --> 02:15:49,969
competitions as possible like like try

2904
02:15:49,969 --> 02:15:53,030
to really get this feel for like cycling

2905
02:15:53,030 --> 02:15:59,030
learning rates plotting things you know

2906
02:15:59,030 --> 02:16:03,949
that that post I showed you at the start

2907
02:16:03,949 --> 02:16:05,869
of class today that kind of took you

2908
02:16:05,869 --> 02:16:08,929
through lesson one like really go

2909
02:16:08,929 --> 02:16:11,030
through that on as many image datasets

2910
02:16:11,030 --> 02:16:13,789
as you can you just feel really

2911
02:16:13,789 --> 02:16:17,059
comfortable with it right because you

2912
02:16:17,059 --> 02:16:18,409
want to get to the point where next week

2913
02:16:18,409 --> 02:16:19,879
when we start talking about structured

2914
02:16:19,879 --> 02:16:22,309
data that this idea of like how learners

2915
02:16:22,309 --> 02:16:24,619
kind of work and data works and data

2916
02:16:24,619 --> 02:16:26,449
loaders and data sets and looking at

2917
02:16:26,449 --> 02:16:28,869
pictures should be really you know

2918
02:16:28,869 --> 02:16:31,819
intuitive all right good luck see you

2919
02:16:31,819 --> 02:16:35,510
next week

2920
02:16:35,510 --> 02:16:39,040
[Applause]

