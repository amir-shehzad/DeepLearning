1
00:00:00,030 --> 00:00:03,659
okay so welcome back to deep learning

2
00:00:03,659 --> 00:00:10,380
lesson 2 last week we got to the point

3
00:00:10,380 --> 00:00:12,150
where we had successfully trained a

4
00:00:12,150 --> 00:00:16,500
pretty accurate image classifier and so

5
00:00:16,500 --> 00:00:21,080
just to remind you about how we did that

6
00:00:21,080 --> 00:00:24,119
can you guys see okay I think the

7
00:00:24,119 --> 00:00:25,769
actually we can turn the phone once all

8
00:00:25,769 --> 00:00:26,880
right

9
00:00:26,880 --> 00:00:30,449
can you guys all see the screen okay we

10
00:00:30,449 --> 00:00:35,340
can to adjust these ones can we some

11
00:00:35,340 --> 00:00:37,469
pictures all into darkness but if that

12
00:00:37,469 --> 00:00:41,250
works then is that okay that's better

13
00:00:41,250 --> 00:00:46,160
isn't it yeah can I dream the other two

14
00:00:46,160 --> 00:00:47,969
and maybe that one as well

15
00:00:47,969 --> 00:00:53,820
oh but that one oh that's great sorry I

16
00:00:53,820 --> 00:00:55,699
don't know your renders oh okay great

17
00:00:55,699 --> 00:01:03,420
that's better isn't it me so just to

18
00:01:03,420 --> 00:01:05,430
remind you the way that we built this

19
00:01:05,430 --> 00:01:08,820
image classifier was we used a small

20
00:01:08,820 --> 00:01:11,850
amount of code basically three lines of

21
00:01:11,850 --> 00:01:14,250
code and these three lines of code

22
00:01:14,250 --> 00:01:17,490
pointed at a particular path which

23
00:01:17,490 --> 00:01:19,590
already had some data in it and so the

24
00:01:19,590 --> 00:01:21,600
key thing for this to know how to train

25
00:01:21,600 --> 00:01:25,020
this model was that this path which was

26
00:01:25,020 --> 00:01:27,990
data dogs cats and had to have a

27
00:01:27,990 --> 00:01:30,270
particular structure which is that it

28
00:01:30,270 --> 00:01:33,360
had a train folder and a valid folder

29
00:01:33,360 --> 00:01:35,670
and in each of those trained and valid

30
00:01:35,670 --> 00:01:37,259
folders there was a cats folder in the

31
00:01:37,259 --> 00:01:39,659
dogs folder and if the cats on the docs

32
00:01:39,659 --> 00:01:42,270
folders was a bunch of images of cats

33
00:01:42,270 --> 00:01:44,820
and votes but this is like a pretty

34
00:01:44,820 --> 00:01:48,360
standard it's one of two main structures

35
00:01:48,360 --> 00:01:51,240
that are used to say here is the data

36
00:01:51,240 --> 00:01:52,799
that I want you to train an image model

37
00:01:52,799 --> 00:01:55,290
from so I know some of you during the

38
00:01:55,290 --> 00:01:58,500
week went away and tried different data

39
00:01:58,500 --> 00:02:01,079
sets where you had folders with

40
00:02:01,079 --> 00:02:02,579
different sets of images and in credit

41
00:02:02,579 --> 00:02:05,369
your own image classifiers and generally

42
00:02:05,369 --> 00:02:06,750
that seems to be working pretty well

43
00:02:06,750 --> 00:02:08,670
from what I can see on the forums so to

44
00:02:08,670 --> 00:02:11,989
make it clear at this point this is

45
00:02:11,989 --> 00:02:13,890
everything you need

46
00:02:13,890 --> 00:02:16,920
to get started so if you create your own

47
00:02:16,920 --> 00:02:18,600
folders with different sets of images

48
00:02:18,600 --> 00:02:23,760
you know a few hundred or a few thousand

49
00:02:23,760 --> 00:02:28,170
at each folder and run the same three

50
00:02:28,170 --> 00:02:30,930
lines of code that will give you an

51
00:02:30,930 --> 00:02:32,490
image classifier and you'll be able to

52
00:02:32,490 --> 00:02:35,160
see this third column tells you how

53
00:02:35,160 --> 00:02:42,750
accurate is so we looked at some kind of

54
00:02:42,750 --> 00:02:46,140
simple visualizations to see like what

55
00:02:46,140 --> 00:02:48,630
was it uncertain about what was it wrong

56
00:02:48,630 --> 00:02:50,910
about and so forth and that's always a

57
00:02:50,910 --> 00:02:55,110
really good idea and then we learned

58
00:02:55,110 --> 00:02:57,330
about the one key number you have to

59
00:02:57,330 --> 00:02:59,100
pick so this is this number here is the

60
00:02:59,100 --> 00:03:02,340
one key number is 0.01 and this is

61
00:03:02,340 --> 00:03:05,010
called the learning rate and so I wanted

62
00:03:05,010 --> 00:03:07,980
to go over this again and we'll learn

63
00:03:07,980 --> 00:03:10,100
about the theory behind what this is

64
00:03:10,100 --> 00:03:12,630
during the rest of the course in quite a

65
00:03:12,630 --> 00:03:14,459
lot of detail and for now I just wanted

66
00:03:14,459 --> 00:03:23,180
to talk about the practice yes you know

67
00:03:23,180 --> 00:03:25,730
they cannot see you in the medium

68
00:03:25,730 --> 00:03:31,650
turnout I just turned it around you tell

69
00:03:31,650 --> 00:03:34,380
us about the other three numbers being

70
00:03:34,380 --> 00:03:38,850
bad we did these three here we're going

71
00:03:38,850 --> 00:03:40,079
to talk about the other other ones

72
00:03:40,079 --> 00:03:41,519
shortly so the main one we're going to

73
00:03:41,519 --> 00:03:43,650
look at for now is is the last column

74
00:03:43,650 --> 00:03:48,030
which is the accuracy the first column

75
00:03:48,030 --> 00:03:50,549
as you can see is the epoch number so

76
00:03:50,549 --> 00:03:52,200
this tells us how many times has it been

77
00:03:52,200 --> 00:03:55,260
through the entire dataset trying to

78
00:03:55,260 --> 00:03:57,450
learn a better classifier and in the

79
00:03:57,450 --> 00:03:58,890
next two columns is what's called the

80
00:03:58,890 --> 00:04:00,709
loss which we'll be learning about

81
00:04:00,709 --> 00:04:03,540
either later today or next week the

82
00:04:03,540 --> 00:04:05,070
first point is the loss on the training

83
00:04:05,070 --> 00:04:07,019
set these are the images that we're

84
00:04:07,019 --> 00:04:08,459
looking at in order to try to make a

85
00:04:08,459 --> 00:04:10,470
better classifier and the second is the

86
00:04:10,470 --> 00:04:12,480
loss of the validation set these are the

87
00:04:12,480 --> 00:04:14,100
images that we're not looking at and

88
00:04:14,100 --> 00:04:15,420
we're training but we're just sitting on

89
00:04:15,420 --> 00:04:17,970
the side to see how accurate we are so

90
00:04:17,970 --> 00:04:19,350
we'll learn about littering loss in

91
00:04:19,350 --> 00:04:24,900
accuracy later

92
00:04:24,900 --> 00:04:30,970
okay so so we've got the epoch number

93
00:04:30,970 --> 00:04:33,100
the training loss is the second column

94
00:04:33,100 --> 00:04:36,970
the validation loss is the third column

95
00:04:36,970 --> 00:04:40,880
and the accuracy is the fourth column

96
00:04:40,880 --> 00:04:44,899
you

97
00:04:44,899 --> 00:04:48,449
okay so the basic idea of the loading

98
00:04:48,449 --> 00:04:56,520
rate so the basic idea of the learning

99
00:04:56,520 --> 00:04:58,259
rate is it's the thing that's going to

100
00:04:58,259 --> 00:05:01,379
decide how quickly do we zoom do we kind

101
00:05:01,379 --> 00:05:05,159
of hone in on the solution and so I find

102
00:05:05,159 --> 00:05:06,839
that a good way to think about this is

103
00:05:06,839 --> 00:05:08,759
to think about like well what if we were

104
00:05:08,759 --> 00:05:13,740
trying to fit to a function that looks

105
00:05:13,740 --> 00:05:15,899
something like this right we're trying

106
00:05:15,899 --> 00:05:17,759
to say okay where's where abouts is the

107
00:05:17,759 --> 00:05:20,459
minimum point this is basically what we

108
00:05:20,459 --> 00:05:23,159
do when we do deep learning is we try to

109
00:05:23,159 --> 00:05:26,849
find the minimum point of a function now

110
00:05:26,849 --> 00:05:29,580
our function happens to have millions or

111
00:05:29,580 --> 00:05:30,809
hundreds of millions of parameters but

112
00:05:30,809 --> 00:05:32,819
it works the same basic way and so when

113
00:05:32,819 --> 00:05:33,839
we look at it you know we can

114
00:05:33,839 --> 00:05:36,419
immediately see that the lowest point is

115
00:05:36,419 --> 00:05:40,439
here but how would you do that if you

116
00:05:40,439 --> 00:05:42,449
are a computer algorithm and what we do

117
00:05:42,449 --> 00:05:45,119
is we we start out at some point at

118
00:05:45,119 --> 00:05:47,729
random so you pick say here and we have

119
00:05:47,729 --> 00:05:50,099
a look and we say okay what's the what's

120
00:05:50,099 --> 00:05:52,499
the loss or the error at this point and

121
00:05:52,499 --> 00:05:53,819
we say what's the gradient in other

122
00:05:53,819 --> 00:05:55,409
words which way is up and which way is

123
00:05:55,409 --> 00:05:58,680
down and it tells us that down is going

124
00:05:58,680 --> 00:06:00,749
to be in that direction and it also

125
00:06:00,749 --> 00:06:03,839
tells us how fast is it going down which

126
00:06:03,839 --> 00:06:05,430
is at this point is going down pretty

127
00:06:05,430 --> 00:06:09,029
quickly and so then we take a step in

128
00:06:09,029 --> 00:06:11,699
the direction that's down and the

129
00:06:11,699 --> 00:06:13,830
distance we travel is going to be

130
00:06:13,830 --> 00:06:15,539
proportional to the gradient sort of

131
00:06:15,539 --> 00:06:17,219
unfortunately how steep it is the idea

132
00:06:17,219 --> 00:06:20,039
is if it's deeper then we're probably

133
00:06:20,039 --> 00:06:22,409
further away that's the general idea

134
00:06:22,409 --> 00:06:25,110
right and so specifically what we do is

135
00:06:25,110 --> 00:06:26,879
we take the gradient which is how steep

136
00:06:26,879 --> 00:06:28,740
is it at this point and we multiply it

137
00:06:28,740 --> 00:06:30,360
by some number and that number is called

138
00:06:30,360 --> 00:06:31,889
the learning rate okay

139
00:06:31,889 --> 00:06:35,789
so if we pick a number that is very

140
00:06:35,789 --> 00:06:38,249
small then we're guaranteed that we're

141
00:06:38,249 --> 00:06:39,599
going to go a little bit closer and a

142
00:06:39,599 --> 00:06:40,740
little bit closer and a little bit

143
00:06:40,740 --> 00:06:43,019
closer each time right but it's going to

144
00:06:43,019 --> 00:06:45,659
take us a very long time to eventually

145
00:06:45,659 --> 00:06:48,419
get to the bottom if we dig a number

146
00:06:48,419 --> 00:06:51,479
that's very big we could actually step

147
00:06:51,479 --> 00:06:53,129
too far could go in the right direction

148
00:06:53,129 --> 00:06:54,930
but we could step all the way over to

149
00:06:54,930 --> 00:06:57,270
here right as

150
00:06:57,270 --> 00:06:59,220
result of which we end up further away

151
00:06:59,220 --> 00:07:01,800
than we started and we could oscillate

152
00:07:01,800 --> 00:07:04,590
and get worse and worse so if you start

153
00:07:04,590 --> 00:07:06,720
training a neural net and you find that

154
00:07:06,720 --> 00:07:08,700
your accuracy or your loss is like

155
00:07:08,700 --> 00:07:10,740
spitting off into infinity

156
00:07:10,740 --> 00:07:12,780
almost certainly your learning rates too

157
00:07:12,780 --> 00:07:17,520
high so in a sense learning rate too low

158
00:07:17,520 --> 00:07:20,580
is is a better problem to have because

159
00:07:20,580 --> 00:07:21,570
you're going to have to wait a long time

160
00:07:21,570 --> 00:07:23,100
but wouldn't it be nice if there was a

161
00:07:23,100 --> 00:07:25,560
way to figure out like what's the best

162
00:07:25,560 --> 00:07:27,450
learning rate something where you could

163
00:07:27,450 --> 00:07:30,690
kind of go quickly go like Bom Bom Bom

164
00:07:30,690 --> 00:07:34,470
right and so that's why we use this

165
00:07:34,470 --> 00:07:36,270
thing called a learning rate finder and

166
00:07:36,270 --> 00:07:39,300
what the learning rate finder does is it

167
00:07:39,300 --> 00:07:41,820
tries each each time it looks at another

168
00:07:41,820 --> 00:07:44,160
remember the mini-batch how many batches

169
00:07:44,160 --> 00:07:46,860
a few images that we look at each time

170
00:07:46,860 --> 00:07:48,900
so that we're using the parallel

171
00:07:48,900 --> 00:07:50,850
processing power of the GPU effectively

172
00:07:50,850 --> 00:07:53,760
we look generally at around 64 128

173
00:07:53,760 --> 00:07:56,370
images at a time for each mini batch

174
00:07:56,370 --> 00:07:59,340
which is labeled here as an iteration we

175
00:07:59,340 --> 00:08:01,040
gradually increase the learning rate

176
00:08:01,040 --> 00:08:02,640
multiplicatively increase the learning

177
00:08:02,640 --> 00:08:04,230
rate we started really really tiny

178
00:08:04,230 --> 00:08:06,450
learning rates to make sure that we

179
00:08:06,450 --> 00:08:09,090
don't start at something too high and we

180
00:08:09,090 --> 00:08:11,790
gradually increase it and so the idea is

181
00:08:11,790 --> 00:08:14,490
that eventually the learning rate will

182
00:08:14,490 --> 00:08:16,710
be so big that the loss will start

183
00:08:16,710 --> 00:08:17,730
getting worse

184
00:08:17,730 --> 00:08:19,260
and so what we're going to do then is

185
00:08:19,260 --> 00:08:22,470
we're a look at the plot of learning

186
00:08:22,470 --> 00:08:25,320
rate against loss right so when the

187
00:08:25,320 --> 00:08:28,620
learning rates tiny it increases slowly

188
00:08:28,620 --> 00:08:29,910
then it's that's where increase a bit

189
00:08:29,910 --> 00:08:33,030
faster and then eventually it starts not

190
00:08:33,030 --> 00:08:34,500
increasing as quickly and in fact it

191
00:08:34,500 --> 00:08:36,840
starts getting worse right so clearly

192
00:08:36,840 --> 00:08:38,850
here and make sure you're you want to be

193
00:08:38,850 --> 00:08:42,180
familiar with this scientific notation

194
00:08:42,180 --> 00:08:45,870
okay so ten to the negative one is 0.1

195
00:08:45,870 --> 00:08:48,510
10 to 50 or is 1 10 to the negative 2 is

196
00:08:48,510 --> 00:08:52,620
0.001 and when we write this in Python

197
00:08:52,620 --> 00:08:53,820
we'll generally write it like this

198
00:08:53,820 --> 00:08:56,430
rather than writing 10 to the negative 1

199
00:08:56,430 --> 00:08:59,220
or 10 to the negative 2 we'll just write

200
00:08:59,220 --> 00:09:05,430
1 a neg 1 or 1 e neg - okay I mean the

201
00:09:05,430 --> 00:09:06,600
same thing you're going to see that all

202
00:09:06,600 --> 00:09:10,830
the time and remember that equals 0.1

203
00:09:10,830 --> 00:09:19,620
Oh point O one okay so don't be confused

204
00:09:19,620 --> 00:09:21,990
by this text that it prints out here

205
00:09:21,990 --> 00:09:25,140
this this loss here is the the final

206
00:09:25,140 --> 00:09:27,600
loss at the very at the end of it's not

207
00:09:27,600 --> 00:09:29,190
of any interest right so ignore this

208
00:09:29,190 --> 00:09:31,020
this is only interesting when we're

209
00:09:31,020 --> 00:09:32,880
doing regular trading that's not

210
00:09:32,880 --> 00:09:34,380
interesting for the learning rate finder

211
00:09:34,380 --> 00:09:35,730
the thing that's interesting for the

212
00:09:35,730 --> 00:09:38,220
learning rate finder is this loan shed

213
00:09:38,220 --> 00:09:41,730
plot and specifically we're not looking

214
00:09:41,730 --> 00:09:43,350
for the point where it's the lowest back

215
00:09:43,350 --> 00:09:44,520
to the point where it's the lowest it's

216
00:09:44,520 --> 00:09:46,140
actually not getting better anymore so

217
00:09:46,140 --> 00:09:47,910
that's to higher learning rate so I

218
00:09:47,910 --> 00:09:49,320
generally look to see like where is it

219
00:09:49,320 --> 00:09:51,690
the lowest and then I go back like one

220
00:09:51,690 --> 00:09:56,760
for magnitude so one enoch two would be

221
00:09:56,760 --> 00:09:59,670
a pretty good choice yeah okay so that's

222
00:09:59,670 --> 00:10:05,460
why you saw when we ran our fit here we

223
00:10:05,460 --> 00:10:09,680
picked 0.01 right which is one a neg two

224
00:10:09,680 --> 00:10:12,870
so important point to make here is like

225
00:10:12,870 --> 00:10:15,060
this this is the one key number that

226
00:10:15,060 --> 00:10:21,000
we've learnt to adjust and if you just

227
00:10:21,000 --> 00:10:22,860
adjust this number at nothing else most

228
00:10:22,860 --> 00:10:24,120
of the time you're going to be able to

229
00:10:24,120 --> 00:10:26,340
get pretty good results and this is like

230
00:10:26,340 --> 00:10:28,140
a very different message to what you

231
00:10:28,140 --> 00:10:31,350
would hear or see in any textbook or any

232
00:10:31,350 --> 00:10:35,880
video or any course because up until now

233
00:10:35,880 --> 00:10:38,310
there's been like dozens and dozens of

234
00:10:38,310 --> 00:10:39,570
these they're called hyper parameters

235
00:10:39,570 --> 00:10:41,310
dozens and dozens of hyper parameters to

236
00:10:41,310 --> 00:10:43,110
set and they've been thought of as

237
00:10:43,110 --> 00:10:45,360
highly sensitive and difficult to set so

238
00:10:45,360 --> 00:10:48,210
inside the first AI library we kind of

239
00:10:48,210 --> 00:10:51,450
do all that stuff for you as much as we

240
00:10:51,450 --> 00:10:53,490
can and during the course we're going to

241
00:10:53,490 --> 00:10:55,110
learn that there are some more we can

242
00:10:55,110 --> 00:10:58,260
quake to get slightly better results but

243
00:10:58,260 --> 00:11:01,350
it's kind of like it's kind of in a

244
00:11:01,350 --> 00:11:03,210
funny situation here because for those

245
00:11:03,210 --> 00:11:04,290
of you that haven't done anything

246
00:11:04,290 --> 00:11:06,540
learning before is kind of like oh this

247
00:11:06,540 --> 00:11:09,180
is that's all there is to it this is

248
00:11:09,180 --> 00:11:10,830
very easy and then when you talk to

249
00:11:10,830 --> 00:11:12,450
people outside this class they'll be

250
00:11:12,450 --> 00:11:14,190
like deep learning so difficult as

251
00:11:14,190 --> 00:11:16,140
someone to say it's a real art form and

252
00:11:16,140 --> 00:11:18,000
so that's why there's this as is

253
00:11:18,000 --> 00:11:19,770
difference right and so that the truth

254
00:11:19,770 --> 00:11:21,360
is that the learning rate really is the

255
00:11:21,360 --> 00:11:24,000
key thing to set and this ability to use

256
00:11:24,000 --> 00:11:24,649
this

257
00:11:24,649 --> 00:11:26,509
to figure out how to set it well though

258
00:11:26,509 --> 00:11:29,949
the paper is now probably 18 months old

259
00:11:29,949 --> 00:11:33,439
almost nobody knows about this paper it

260
00:11:33,439 --> 00:11:35,119
was from a guy who's not from a famous

261
00:11:35,119 --> 00:11:36,860
research labs so most people kind of

262
00:11:36,860 --> 00:11:38,509
ignored it and in fact even this

263
00:11:38,509 --> 00:11:40,639
particular technique was one subpart of

264
00:11:40,639 --> 00:11:42,829
a paper that was about something else

265
00:11:42,829 --> 00:11:45,110
so again this idea of like this is how

266
00:11:45,110 --> 00:11:47,689
you can set the learning rate really

267
00:11:47,689 --> 00:11:49,610
nobody outside this classroom just about

268
00:11:49,610 --> 00:11:51,980
knows about it obviously the guy who

269
00:11:51,980 --> 00:11:53,480
wrote it Leslie Smith knows about it

270
00:11:53,480 --> 00:11:56,869
yeah so it's a good thing to tell your

271
00:11:56,869 --> 00:11:58,999
colleagues about is like here is

272
00:11:58,999 --> 00:12:00,290
actually a great way to set the learning

273
00:12:00,290 --> 00:12:03,230
rate and there's even been papers caught

274
00:12:03,230 --> 00:12:04,819
like one of the famous papers is called

275
00:12:04,819 --> 00:12:07,399
no more pesky learning rates which

276
00:12:07,399 --> 00:12:09,529
actually is a less effective technique

277
00:12:09,529 --> 00:12:11,029
than this one but this idea that like

278
00:12:11,029 --> 00:12:13,220
setting learning rates is is very

279
00:12:13,220 --> 00:12:15,679
difficult and thirdly is has been true

280
00:12:15,679 --> 00:12:17,600
for most of the kind of deep learning

281
00:12:17,600 --> 00:12:20,569
history so here's the trick right go

282
00:12:20,569 --> 00:12:23,029
look at this plot find kind of the

283
00:12:23,029 --> 00:12:24,949
lowest to go back about a multiple of

284
00:12:24,949 --> 00:12:28,069
ten and try that all right and if that

285
00:12:28,069 --> 00:12:29,509
doesn't quite work you can always try

286
00:12:29,509 --> 00:12:31,220
you know going back another multiple ten

287
00:12:31,220 --> 00:12:40,160
but this is always worked for me so far

288
00:12:40,160 --> 00:12:43,490
once why does this learning rate this

289
00:12:43,490 --> 00:12:45,860
method work versus something else like

290
00:12:45,860 --> 00:12:47,749
momentum base or what's like the

291
00:12:47,749 --> 00:12:50,120
advantages a disadvantage with just

292
00:12:50,120 --> 00:12:52,009
learning rate rate like technique we're

293
00:12:52,009 --> 00:13:00,189
just feels that's a great question so

294
00:13:00,189 --> 00:13:02,209
we're going to learn during this course

295
00:13:02,209 --> 00:13:03,499
about a number of ways of improving

296
00:13:03,499 --> 00:13:05,749
gradient percent like you mentioned

297
00:13:05,749 --> 00:13:08,779
momentum and atom and so forth this is

298
00:13:08,779 --> 00:13:10,970
orthogonal in fact so one of the things

299
00:13:10,970 --> 00:13:13,040
the faster a library tries to do is

300
00:13:13,040 --> 00:13:15,439
figure out the right gradient descent

301
00:13:15,439 --> 00:13:17,149
version and in fact behind the scenes

302
00:13:17,149 --> 00:13:18,379
this is actually using something called

303
00:13:18,379 --> 00:13:21,170
atom and so this technique is telling us

304
00:13:21,170 --> 00:13:23,139
this is the best learning rate to use

305
00:13:23,139 --> 00:13:26,389
given what I thought other tweaks you're

306
00:13:26,389 --> 00:13:29,029
using in this case the atom optimizer so

307
00:13:29,029 --> 00:13:31,339
it's not that there's some compromise

308
00:13:31,339 --> 00:13:32,899
between this and some other approaches

309
00:13:32,899 --> 00:13:34,759
who sits on top of those approaches and

310
00:13:34,759 --> 00:13:36,019
you still have to set the learning rate

311
00:13:36,019 --> 00:13:38,179
when you use with other approaches so

312
00:13:38,179 --> 00:13:38,660
we're trying to

313
00:13:38,660 --> 00:13:41,029
find the best kind of optimizer to use

314
00:13:41,029 --> 00:13:42,649
for a problem that you still have to set

315
00:13:42,649 --> 00:13:44,269
the learning rate and this is how we can

316
00:13:44,269 --> 00:13:46,459
do it and in fact this idea of using

317
00:13:46,459 --> 00:13:49,100
this technique on top of more advanced

318
00:13:49,100 --> 00:13:50,569
optimizers like Adam might haven't even

319
00:13:50,569 --> 00:13:52,579
seen mentioned in a paper before so I

320
00:13:52,579 --> 00:13:54,350
think this is like a I mean it's not a

321
00:13:54,350 --> 00:13:56,269
huge breakthrough it seems obvious but

322
00:13:56,269 --> 00:13:59,629
nobody else seems to tried it so as you

323
00:13:59,629 --> 00:14:05,800
can see it was well

324
00:14:05,800 --> 00:14:08,230
when we use optimizers like Adam ditched

325
00:14:08,230 --> 00:14:10,690
Harvick adaptive learning rate so and he

326
00:14:10,690 --> 00:14:12,339
said this learning rate is Italy initial

327
00:14:12,339 --> 00:14:14,050
learning rate because it changes during

328
00:14:14,050 --> 00:14:21,880
the people so we're going to be learning

329
00:14:21,880 --> 00:14:24,250
about things like Adam the details about

330
00:14:24,250 --> 00:14:26,350
it later in the class but the basic

331
00:14:26,350 --> 00:14:28,270
answer is no even with even the Adam

332
00:14:28,270 --> 00:14:29,910
that there actually is a learning rate

333
00:14:29,910 --> 00:14:35,860
it's just being it's being basically

334
00:14:35,860 --> 00:14:39,520
divided by the the gradient the average

335
00:14:39,520 --> 00:14:41,950
previous gradient and also the recent

336
00:14:41,950 --> 00:14:43,750
summer Squared's of gradients so there's

337
00:14:43,750 --> 00:14:45,220
still like a number called the learning

338
00:14:45,220 --> 00:14:47,589
rate there there isn't a even these so

339
00:14:47,589 --> 00:14:49,750
called dynamic learning rate methods

340
00:14:49,750 --> 00:15:00,339
still have unlearning rate okay so the

341
00:15:00,339 --> 00:15:04,510
most important thing that you can do to

342
00:15:04,510 --> 00:15:07,660
make your model better and is to give it

343
00:15:07,660 --> 00:15:10,990
more data so the challenge that happens

344
00:15:10,990 --> 00:15:13,720
is that these models have hundreds of

345
00:15:13,720 --> 00:15:16,810
millions of parameters and if you train

346
00:15:16,810 --> 00:15:19,870
them for a while they start to do what's

347
00:15:19,870 --> 00:15:21,970
called overfitting and so overfitting

348
00:15:21,970 --> 00:15:23,980
means that they're going to start to see

349
00:15:23,980 --> 00:15:26,079
like the specific details of the images

350
00:15:26,079 --> 00:15:27,790
you're giving them rather than the more

351
00:15:27,790 --> 00:15:31,570
general learning that can transfer

352
00:15:31,570 --> 00:15:34,300
across to the validation set so the best

353
00:15:34,300 --> 00:15:36,040
thing we can do to avoid overfitting is

354
00:15:36,040 --> 00:15:39,399
to find more data now obviously one way

355
00:15:39,399 --> 00:15:41,079
to do that would just be to collect more

356
00:15:41,079 --> 00:15:42,399
data from where you're getting it from

357
00:15:42,399 --> 00:15:45,070
or label more data but a really easy way

358
00:15:45,070 --> 00:15:47,200
that we should always do is to use

359
00:15:47,200 --> 00:15:51,040
something called data augmentation so

360
00:15:51,040 --> 00:15:53,200
they don't open tuition is one of these

361
00:15:53,200 --> 00:15:55,959
things that's key in many courses it's

362
00:15:55,959 --> 00:15:57,520
not even mentioned at all or if it is

363
00:15:57,520 --> 00:15:58,839
it's kind of like an advanced topic

364
00:15:58,839 --> 00:16:00,730
right at the end but actually it's like

365
00:16:00,730 --> 00:16:03,250
the most important thing that you can do

366
00:16:03,250 --> 00:16:05,320
to make a better model okay and so it's

367
00:16:05,320 --> 00:16:07,000
built into the faster you library to

368
00:16:07,000 --> 00:16:08,890
make it very easy to do and so we're

369
00:16:08,890 --> 00:16:10,420
going to look at the details of the code

370
00:16:10,420 --> 00:16:14,740
shortly but the basic idea is that

371
00:16:14,740 --> 00:16:19,930
as in our initial code we had a line

372
00:16:19,930 --> 00:16:22,240
that said image classified data from

373
00:16:22,240 --> 00:16:23,950
parts and we passed in the path to our

374
00:16:23,950 --> 00:16:26,770
data and for transforms we passed in

375
00:16:26,770 --> 00:16:30,010
basically the sizing the architecture

376
00:16:30,010 --> 00:16:31,360
we'll look at this in more detail

377
00:16:31,360 --> 00:16:34,149
shortly we just add one more parameter

378
00:16:34,149 --> 00:16:36,760
which is what kind of data augmentation

379
00:16:36,760 --> 00:16:41,020
do you want to do and so to understand

380
00:16:41,020 --> 00:16:43,240
data augmentation it's may be easiest to

381
00:16:43,240 --> 00:16:44,620
look at some pictures of data

382
00:16:44,620 --> 00:16:47,140
augmentation so what I've done here

383
00:16:47,140 --> 00:16:48,820
again we'll look at the code in more

384
00:16:48,820 --> 00:16:50,800
detail later but the basic idea is oh

385
00:16:50,800 --> 00:16:56,560
I've run I've built a data class like

386
00:16:56,560 --> 00:16:58,270
multiple times I'm going to do it six

387
00:16:58,270 --> 00:17:00,550
times and each time I'm going to plot

388
00:17:00,550 --> 00:17:03,910
the same catch and you can see that what

389
00:17:03,910 --> 00:17:07,329
happens is that this cap here is further

390
00:17:07,329 --> 00:17:09,100
over to the left this one here is

391
00:17:09,100 --> 00:17:10,510
further over to the right and this one

392
00:17:10,510 --> 00:17:13,839
here is fit horizontally and so forth so

393
00:17:13,839 --> 00:17:17,730
data augmentation different types of

394
00:17:17,730 --> 00:17:19,569
image you're going to want different

395
00:17:19,569 --> 00:17:22,660
types of data augmentation right so for

396
00:17:22,660 --> 00:17:25,020
example if you were trying to recognize

397
00:17:25,020 --> 00:17:27,730
letters and digits you wouldn't want to

398
00:17:27,730 --> 00:17:29,470
flip horizontally because like it's

399
00:17:29,470 --> 00:17:31,540
actually has a different meaning whereas

400
00:17:31,540 --> 00:17:33,929
on the other hand if you're looking at

401
00:17:33,929 --> 00:17:36,309
photos of cats and dogs you probably

402
00:17:36,309 --> 00:17:38,140
don't want to fit vertically because

403
00:17:38,140 --> 00:17:40,240
cats aren't generally upside down all

404
00:17:40,240 --> 00:17:42,220
right where else if you're looking at

405
00:17:42,220 --> 00:17:44,050
there's a current Cargill competition

406
00:17:44,050 --> 00:17:47,530
which is recognizing icebergs in

407
00:17:47,530 --> 00:17:50,080
satellite images you probably do want to

408
00:17:50,080 --> 00:17:52,360
fit them upside down because it's really

409
00:17:52,360 --> 00:17:54,400
matter which area around the iceberg or

410
00:17:54,400 --> 00:17:58,960
the satellite was right so one of the

411
00:17:58,960 --> 00:18:00,940
examples of the transform sets we have

412
00:18:00,940 --> 00:18:03,700
is transforms sidon so in other words if

413
00:18:03,700 --> 00:18:05,200
you have photos that are like generally

414
00:18:05,200 --> 00:18:07,420
taken from the side which generally

415
00:18:07,420 --> 00:18:08,679
means you want to be able to flip them

416
00:18:08,679 --> 00:18:10,690
horizontally but not vertically this is

417
00:18:10,690 --> 00:18:12,130
going to give you all the transforms you

418
00:18:12,130 --> 00:18:13,510
need for that so it'll flip them

419
00:18:13,510 --> 00:18:16,179
sideways rotate them by small amounts

420
00:18:16,179 --> 00:18:19,000
but not too much and slightly bury their

421
00:18:19,000 --> 00:18:22,630
contrast and brightness and slightly

422
00:18:22,630 --> 00:18:24,190
zoom in and out a little bit and move

423
00:18:24,190 --> 00:18:25,160
them around a little

424
00:18:25,160 --> 00:18:26,650
so each time it's a slightly different

425
00:18:26,650 --> 00:18:30,980
fight billionaires we're getting a

426
00:18:30,980 --> 00:18:33,610
couple of questions from people about

427
00:18:33,610 --> 00:18:36,620
you explaining in the reason why you

428
00:18:36,620 --> 00:18:38,930
don't think the minimum of the loss

429
00:18:38,930 --> 00:18:41,750
curve yeah but it's like the higher rate

430
00:18:41,750 --> 00:18:44,840
so yeah and also could you people

431
00:18:44,840 --> 00:18:48,650
understand if this works for every CNN

432
00:18:48,650 --> 00:18:52,910
for CNN every minute there's a running

433
00:18:52,910 --> 00:18:58,550
right fine done yeah exactly yeah okay

434
00:18:58,550 --> 00:18:58,850
great

435
00:18:58,850 --> 00:19:02,810
um could you put your hand up if there's

436
00:19:02,810 --> 00:19:11,480
a spare seat next to you so there was a

437
00:19:11,480 --> 00:19:13,160
question about the learning rate finder

438
00:19:13,160 --> 00:19:15,470
about why do we use the learning rate

439
00:19:15,470 --> 00:19:18,350
that's less than the lowest point and so

440
00:19:18,350 --> 00:19:20,150
the reason why is to understand what's

441
00:19:20,150 --> 00:19:24,160
going on with this learning rate finder

442
00:19:24,160 --> 00:19:28,030
so let's go back to our picture here

443
00:19:28,030 --> 00:19:31,190
like how do we figure out what learning

444
00:19:31,190 --> 00:19:33,470
rate to use right and so what we're

445
00:19:33,470 --> 00:19:35,750
going to do is we're going to take steps

446
00:19:35,750 --> 00:19:39,830
and each time we're going to double the

447
00:19:39,830 --> 00:19:41,300
learning rate so kind of double the

448
00:19:41,300 --> 00:19:43,100
amount by which we multiply the grander

449
00:19:43,100 --> 00:19:45,770
gradient so in other words would go tiny

450
00:19:45,770 --> 00:19:47,780
step slightly bigger slightly bigger

451
00:19:47,780 --> 00:19:51,980
slightly bigger slightly bigger slightly

452
00:19:51,980 --> 00:19:57,350
bigger slightly bigger okay and so the

453
00:19:57,350 --> 00:19:59,360
question is of the purpose of this is

454
00:19:59,360 --> 00:20:01,490
not to find the minimum the purpose of

455
00:20:01,490 --> 00:20:03,740
this is to figure out what learning rate

456
00:20:03,740 --> 00:20:07,030
is allowing us to decrease quickly right

457
00:20:07,030 --> 00:20:09,830
so the point at which the loss was

458
00:20:09,830 --> 00:20:13,310
lowest here is actually there right but

459
00:20:13,310 --> 00:20:15,170
that learning rate actually looks like

460
00:20:15,170 --> 00:20:16,670
it's probably too high it's going to

461
00:20:16,670 --> 00:20:19,070
just jump like probably backwards and

462
00:20:19,070 --> 00:20:22,250
forwards okay so instead what we do is

463
00:20:22,250 --> 00:20:24,290
we go back to the point where the

464
00:20:24,290 --> 00:20:26,450
learning rates quickly are giving us a

465
00:20:26,450 --> 00:20:33,020
quick increase in the loss so here is so

466
00:20:33,020 --> 00:20:34,610
here is the actual learning rate

467
00:20:34,610 --> 00:20:36,770
increasing every single time we look at

468
00:20:36,770 --> 00:20:38,090
a new mini batch

469
00:20:38,090 --> 00:20:40,009
so mini-batch reiteration versus

470
00:20:40,009 --> 00:20:42,469
learning right and then here is learning

471
00:20:42,469 --> 00:20:44,450
rate versus loss so here's that point at

472
00:20:44,450 --> 00:20:47,289
the bottom where is now already too high

473
00:20:47,289 --> 00:20:49,519
okay and so here's the point where we go

474
00:20:49,519 --> 00:20:51,320
back a little bit and it's increasing

475
00:20:51,320 --> 00:20:55,340
nice and quickly we're going to learn

476
00:20:55,340 --> 00:20:57,259
about something called stochastic

477
00:20:57,259 --> 00:20:59,659
gradient descent with restarts shortly

478
00:20:59,659 --> 00:21:01,249
where we're going to see like in a sense

479
00:21:01,249 --> 00:21:03,379
you might want to go back to 1 enoch 3

480
00:21:03,379 --> 00:21:05,330
where it's actually even steeper still

481
00:21:05,330 --> 00:21:07,190
and maybe we would actually find this

482
00:21:07,190 --> 00:21:11,330
book actually learn even quicker you

483
00:21:11,330 --> 00:21:13,009
could try it but we're going to see

484
00:21:13,009 --> 00:21:14,869
later why actually using a higher number

485
00:21:14,869 --> 00:21:16,729
is going to give us better

486
00:21:16,729 --> 00:21:19,580
generalization so for now let's put that

487
00:21:19,580 --> 00:21:22,129
aside do you mean higher learning rate

488
00:21:22,129 --> 00:21:24,259
when you say I know I mean higher

489
00:21:24,259 --> 00:21:25,580
letting retina say higher

490
00:21:25,580 --> 00:21:31,429
yeah yeah I mean I am learning rate so

491
00:21:31,429 --> 00:21:33,259
as we increase the iterations from the

492
00:21:33,259 --> 00:21:35,179
learning rate finder the learning rate

493
00:21:35,179 --> 00:21:37,909
is going up this is iterations versus

494
00:21:37,909 --> 00:21:41,299
learning ready okay so as we do that as

495
00:21:41,299 --> 00:21:43,249
the learning rate increases and we plot

496
00:21:43,249 --> 00:21:46,580
it here the loss Goes Down and here we

497
00:21:46,580 --> 00:21:47,779
get to the point where the learning rate

498
00:21:47,779 --> 00:21:50,330
is too high and at that point the most

499
00:21:50,330 --> 00:21:52,759
is now getting worse because I asked the

500
00:21:52,759 --> 00:21:54,019
question because you were just

501
00:21:54,019 --> 00:21:55,639
indicating that you know even though the

502
00:21:55,639 --> 00:21:58,519
minimum was at 10 to the minus 1 you

503
00:21:58,519 --> 00:22:00,559
were gonna you suggest that we should

504
00:22:00,559 --> 00:22:03,289
choose 10 to the minus 2 but now you're

505
00:22:03,289 --> 00:22:04,669
saying I mean we should go back the

506
00:22:04,669 --> 00:22:07,039
other way higher so I didn't mean to say

507
00:22:07,039 --> 00:22:08,749
that I'm sorry if I said something

508
00:22:08,749 --> 00:22:11,499
backwards I want to go back down to the

509
00:22:11,499 --> 00:22:14,899
lower learning rate so possibly I said a

510
00:22:14,899 --> 00:22:17,779
higher when I meant higher into this

511
00:22:17,779 --> 00:22:19,669
lower OS do you know I'm learning right

512
00:22:19,669 --> 00:22:22,909
okay thanks yep

513
00:22:22,909 --> 00:22:26,390
last class is said that the local all

514
00:22:26,390 --> 00:22:29,690
the local minima are the same and this

515
00:22:29,690 --> 00:22:32,270
graph also shows the same is that is

516
00:22:32,270 --> 00:22:33,770
this something that was observed or is

517
00:22:33,770 --> 00:22:37,789
the logic theory behind it that's not

518
00:22:37,789 --> 00:22:40,220
what this graph is showing this graph is

519
00:22:40,220 --> 00:22:41,750
simply showing that there's a point

520
00:22:41,750 --> 00:22:43,460
where if we increase the learning rate

521
00:22:43,460 --> 00:22:46,100
more then it stops getting better than

522
00:22:46,100 --> 00:22:48,230
actually starts getting worse the idea

523
00:22:48,230 --> 00:22:53,020
that all local minima are the same is a

524
00:22:53,020 --> 00:22:57,470
totally separate issue and it's actually

525
00:22:57,470 --> 00:22:58,789
something else we'll see a picture of

526
00:22:58,789 --> 00:23:02,409
shortly so let's come back to that

527
00:23:02,409 --> 00:23:05,630
Jeremy do we have to find the base

528
00:23:05,630 --> 00:23:08,710
learning rate every time we are going to

529
00:23:08,710 --> 00:23:13,400
run a poke third time we're running on a

530
00:23:13,400 --> 00:23:15,919
poke and a pop so how many times should

531
00:23:15,919 --> 00:23:19,220
I run this like let me write find my

532
00:23:19,220 --> 00:23:27,410
training that's a great question unit um

533
00:23:27,410 --> 00:23:31,930
I certainly run it once when I start

534
00:23:31,930 --> 00:23:34,070
later on in this class we're going to

535
00:23:34,070 --> 00:23:37,970
learn about unfreezing layers and after

536
00:23:37,970 --> 00:23:40,010
I unfreeze layers I sometimes run it

537
00:23:40,010 --> 00:23:42,800
again if I do something to like change

538
00:23:42,800 --> 00:23:44,750
the thing I'm training or change the way

539
00:23:44,750 --> 00:23:46,520
I'm training it you may want to run it

540
00:23:46,520 --> 00:23:50,540
again basically or you know if you

541
00:23:50,540 --> 00:23:51,770
particularly if you've changed something

542
00:23:51,770 --> 00:23:54,140
about how you train like unfreezing

543
00:23:54,140 --> 00:23:55,490
layers which we're gonna soon learn

544
00:23:55,490 --> 00:23:57,500
about and you're finding the other

545
00:23:57,500 --> 00:24:01,070
training is unstable or too slow

546
00:24:01,070 --> 00:24:03,260
well again you can run it again there's

547
00:24:03,260 --> 00:24:04,270
never any harm

548
00:24:04,270 --> 00:24:07,660
in running it it doesn't take very long

549
00:24:07,660 --> 00:24:11,540
that's great question okay so back to

550
00:24:11,540 --> 00:24:14,660
data augmentation so if we add to a when

551
00:24:14,660 --> 00:24:17,710
we run this little transforms from model

552
00:24:17,710 --> 00:24:20,480
function we pass in orientation

553
00:24:20,480 --> 00:24:23,320
transforms we can pass in the main to a

554
00:24:23,320 --> 00:24:26,330
transform side on or transforms top down

555
00:24:26,330 --> 00:24:28,340
later on we'll learn about creating your

556
00:24:28,340 --> 00:24:31,670
own custom transform lists as well but

557
00:24:31,670 --> 00:24:32,960
for now because we're taking pictures

558
00:24:32,960 --> 00:24:35,450
from the side cats and dogs will say

559
00:24:35,450 --> 00:24:38,720
transform side on and now each time we

560
00:24:38,720 --> 00:24:41,030
look at an image it's going to be zoomed

561
00:24:41,030 --> 00:24:42,830
in or out a little bit moved around a

562
00:24:42,830 --> 00:24:43,400
little bit

563
00:24:43,400 --> 00:24:47,740
rotated a little bit possibly flipped

564
00:24:47,740 --> 00:24:50,720
okay and so what this does is it's not

565
00:24:50,720 --> 00:24:53,210
exactly creating new data but as far as

566
00:24:53,210 --> 00:24:55,640
the convolutional neural net is

567
00:24:55,640 --> 00:24:57,110
concerned it's a different way of

568
00:24:57,110 --> 00:24:59,330
looking at this thing and it actually

569
00:24:59,330 --> 00:25:02,840
therefore allows it to learn how to

570
00:25:02,840 --> 00:25:06,050
recognize cats or dogs from somewhat

571
00:25:06,050 --> 00:25:07,850
different angles right so when we do

572
00:25:07,850 --> 00:25:09,410
data orientation we're basically trying

573
00:25:09,410 --> 00:25:12,850
to say based on our domain knowledge

574
00:25:12,850 --> 00:25:15,590
here here are different ways that we can

575
00:25:15,590 --> 00:25:18,950
mess with this image that we know still

576
00:25:18,950 --> 00:25:20,900
make it the same image you know and that

577
00:25:20,900 --> 00:25:22,490
we could expect that you might actually

578
00:25:22,490 --> 00:25:25,330
see that kind of image in the real world

579
00:25:25,330 --> 00:25:28,910
so what we can do now is when we call

580
00:25:28,910 --> 00:25:31,310
this from parts function which we'll

581
00:25:31,310 --> 00:25:33,410
learn more about shortly we can now pass

582
00:25:33,410 --> 00:25:35,960
in this set of transforms which actually

583
00:25:35,960 --> 00:25:41,410
have these augmentations in now

584
00:25:41,410 --> 00:25:42,820
so that's going to we're going to start

585
00:25:42,820 --> 00:25:47,250
from scratch here we do a fit and

586
00:25:47,250 --> 00:25:51,040
initially the augmentations actually

587
00:25:51,040 --> 00:25:53,590
don't do anything and the reason

588
00:25:53,590 --> 00:25:55,210
initially they don't do anything is

589
00:25:55,210 --> 00:25:56,860
because we've got here something that

590
00:25:56,860 --> 00:25:59,350
says precompute equals true we're going

591
00:25:59,350 --> 00:26:03,670
to come back to these lots of times but

592
00:26:03,670 --> 00:26:06,130
basically what this is doing is do you

593
00:26:06,130 --> 00:26:08,410
remember this picture we saw where we

594
00:26:08,410 --> 00:26:11,320
learn each different layer has these

595
00:26:11,320 --> 00:26:13,750
activations that basically look for it

596
00:26:13,750 --> 00:26:16,030
or anything from the middle of flowers

597
00:26:16,030 --> 00:26:21,280
to eyeballs of birds or whatever right

598
00:26:21,280 --> 00:26:25,050
and so literally what happens is that

599
00:26:25,050 --> 00:26:28,000
the the later layers of this

600
00:26:28,000 --> 00:26:30,040
convolutional neural network have these

601
00:26:30,040 --> 00:26:32,290
things called activations and activation

602
00:26:32,290 --> 00:26:34,900
literally it's a number an activation is

603
00:26:34,900 --> 00:26:38,340
a number that says this feature like

604
00:26:38,340 --> 00:26:42,640
eyeball of bird is in this location with

605
00:26:42,640 --> 00:26:44,260
this level of confidence with its

606
00:26:44,260 --> 00:26:46,780
probability right and so we're going to

607
00:26:46,780 --> 00:26:50,320
see a lot of this later but what we can

608
00:26:50,320 --> 00:26:52,870
do is we can say all right well in this

609
00:26:52,870 --> 00:26:55,690
we've got a pre trained network remember

610
00:26:55,690 --> 00:26:57,550
and a pre trained network is one where

611
00:26:57,550 --> 00:26:58,690
it's already learned to recognize

612
00:26:58,690 --> 00:27:00,430
certain things in this case it's learnt

613
00:27:00,430 --> 00:27:02,380
to recognize the one and a half million

614
00:27:02,380 --> 00:27:05,560
images in the imagenet dataset and so

615
00:27:05,560 --> 00:27:07,990
what we could do is we could take the

616
00:27:07,990 --> 00:27:10,240
the second last layer so the one which

617
00:27:10,240 --> 00:27:12,610
is like got all of the information

618
00:27:12,610 --> 00:27:14,740
necessary to figure out what kind of

619
00:27:14,740 --> 00:27:17,440
thing a thing is and we can save those

620
00:27:17,440 --> 00:27:19,510
activations so basically saving things

621
00:27:19,510 --> 00:27:21,970
saying you know there's this level of

622
00:27:21,970 --> 00:27:24,010
eyeball nurse here in this level of dogs

623
00:27:24,010 --> 00:27:25,810
facing us here or in this level of

624
00:27:25,810 --> 00:27:29,410
fluffy ear there and so forth and so we

625
00:27:29,410 --> 00:27:32,770
save for every image these activations

626
00:27:32,770 --> 00:27:35,140
and that we call them the pre computed

627
00:27:35,140 --> 00:27:38,110
activations and so the idea is now that

628
00:27:38,110 --> 00:27:41,020
when we want to create a new classifier

629
00:27:41,020 --> 00:27:43,990
which can basically take advantage of

630
00:27:43,990 --> 00:27:46,570
these pre computed applications we can

631
00:27:46,570 --> 00:27:48,910
just very quickly train when all the

632
00:27:48,910 --> 00:27:50,560
details there shortly we can very

633
00:27:50,560 --> 00:27:52,740
quickly train a simple linear model

634
00:27:52,740 --> 00:27:54,559
based on those

635
00:27:54,559 --> 00:27:55,879
and so that's what happens when we say

636
00:27:55,879 --> 00:27:58,759
pre-compute equals true and that's why

637
00:27:58,759 --> 00:28:01,279
you may have noticed this week the first

638
00:28:01,279 --> 00:28:05,389
time that you run a model a new model it

639
00:28:05,389 --> 00:28:08,509
takes a minute or two where else you saw

640
00:28:08,509 --> 00:28:10,129
when I ran it it took like five or ten

641
00:28:10,129 --> 00:28:12,139
seconds took you a minute or two and

642
00:28:12,139 --> 00:28:14,289
that's because it had to pre-compute

643
00:28:14,289 --> 00:28:16,669
these activations and just has to do

644
00:28:16,669 --> 00:28:19,340
that once if you're using like your own

645
00:28:19,340 --> 00:28:21,350
computer or AWS it just has to do it

646
00:28:21,350 --> 00:28:24,980
once ever if you're using Kressel it

647
00:28:24,980 --> 00:28:28,369
actually has to do it once every single

648
00:28:28,369 --> 00:28:30,919
time you rerun press all because press

649
00:28:30,919 --> 00:28:33,740
or uses are just for these pre computed

650
00:28:33,740 --> 00:28:35,509
activations it uses a special that all

651
00:28:35,509 --> 00:28:37,730
had a scratch space that disappears each

652
00:28:37,730 --> 00:28:39,740
time you restart your press or instance

653
00:28:39,740 --> 00:28:42,440
so other than the special case of cresol

654
00:28:42,440 --> 00:28:44,029
generally speak he does have to run at

655
00:28:44,029 --> 00:28:44,570
once

656
00:28:44,570 --> 00:28:50,629
ever for a data set okay so the issue

657
00:28:50,629 --> 00:28:53,059
with that is that since we pre computed

658
00:28:53,059 --> 00:28:56,149
for each image you know how much does it

659
00:28:56,149 --> 00:28:57,889
have an EI here and how much does it

660
00:28:57,889 --> 00:29:00,860
have a lizard's eyeball there and so

661
00:29:00,860 --> 00:29:02,840
forth that means that data augmentations

662
00:29:02,840 --> 00:29:04,940
don't work right in other words even

663
00:29:04,940 --> 00:29:05,840
though we're trying to show at a

664
00:29:05,840 --> 00:29:07,549
different version of the cat each time

665
00:29:07,549 --> 00:29:09,950
we've pre computed the activations for a

666
00:29:09,950 --> 00:29:13,369
particular version of that cat so in

667
00:29:13,369 --> 00:29:15,860
order to use data augmentation we just

668
00:29:15,860 --> 00:29:17,629
have to go and learn pre compute equals

669
00:29:17,629 --> 00:29:20,749
false okay and then we can run a few

670
00:29:20,749 --> 00:29:25,429
more APIs right and so you can see here

671
00:29:25,429 --> 00:29:29,149
that as we run more a Potts the accuracy

672
00:29:29,149 --> 00:29:31,580
isn't particularly getting better that's

673
00:29:31,580 --> 00:29:32,149
the bad news

674
00:29:32,149 --> 00:29:35,779
the good news is that you can see that

675
00:29:35,779 --> 00:29:39,379
the train loss practices like the way of

676
00:29:39,379 --> 00:29:40,850
measuring the error of this model

677
00:29:40,850 --> 00:29:42,889
although that's getting better the

678
00:29:42,889 --> 00:29:45,379
errors going down the validation error

679
00:29:45,379 --> 00:29:48,799
isn't going down and but we're not

680
00:29:48,799 --> 00:29:50,929
overfitting and overfitting would mean

681
00:29:50,929 --> 00:29:53,899
that the training loss is much lower

682
00:29:53,899 --> 00:29:56,360
than the validation loss and we're going

683
00:29:56,360 --> 00:29:57,740
to talk about that a lot during this

684
00:29:57,740 --> 00:30:00,080
course but the general idea here is if

685
00:30:00,080 --> 00:30:02,720
you're doing much better job on the

686
00:30:02,720 --> 00:30:04,399
training set then you are on the

687
00:30:04,399 --> 00:30:06,440
validation set that means your models

688
00:30:06,440 --> 00:30:07,560
not generalize

689
00:30:07,560 --> 00:30:10,250
so we're not at that point which is good

690
00:30:10,250 --> 00:30:14,760
but we're not really improving so we're

691
00:30:14,760 --> 00:30:15,930
going to have to figure out how to deal

692
00:30:15,930 --> 00:30:19,170
with that before we do I want to show

693
00:30:19,170 --> 00:30:21,740
you one other cool trick I've added here

694
00:30:21,740 --> 00:30:25,290
cycle length equals one and this is

695
00:30:25,290 --> 00:30:29,010
another really interesting idea here's

696
00:30:29,010 --> 00:30:30,090
the basic idea

697
00:30:30,090 --> 00:30:33,750
cycle length equals one enables a recent

698
00:30:33,750 --> 00:30:35,850
fairly recent discovery and deep

699
00:30:35,850 --> 00:30:37,080
learning called stochastic gradient

700
00:30:37,080 --> 00:30:40,230
descent with restarts and the basic idea

701
00:30:40,230 --> 00:30:45,090
is this as you as you get closer and

702
00:30:45,090 --> 00:30:49,020
closer as you get closer and closer to

703
00:30:49,020 --> 00:30:52,830
the right spot right now getting closer

704
00:30:52,830 --> 00:30:55,110
and closer I may want to start to

705
00:30:55,110 --> 00:30:57,390
decrease my learning rate right because

706
00:30:57,390 --> 00:30:59,220
as I get closer I'm kind of like oh I'm

707
00:30:59,220 --> 00:31:01,500
pretty close down so let's let's slow

708
00:31:01,500 --> 00:31:04,620
down my steps to try to get executive

709
00:31:04,620 --> 00:31:08,250
the right spot right and so as we do

710
00:31:08,250 --> 00:31:12,430
more iterations

711
00:31:12,430 --> 00:31:15,400
our learning rate perhaps should

712
00:31:15,400 --> 00:31:19,360
actually go down right because as we go

713
00:31:19,360 --> 00:31:20,980
along we're getting closer and closer to

714
00:31:20,980 --> 00:31:22,120
where we want to be and we want to like

715
00:31:22,120 --> 00:31:25,600
get exactly to the right spot okay so

716
00:31:25,600 --> 00:31:27,970
the idea of decreasing the learning rate

717
00:31:27,970 --> 00:31:29,530
as you train is called

718
00:31:29,530 --> 00:31:33,430
learning rate annealing and it's it's

719
00:31:33,430 --> 00:31:36,090
very very common very very popular

720
00:31:36,090 --> 00:31:39,420
everybody uses it basically all the time

721
00:31:39,420 --> 00:31:41,830
the most common kind of learning rate

722
00:31:41,830 --> 00:31:45,100
annealing is really horrendously hacky

723
00:31:45,100 --> 00:31:47,290
it's basically that researchers like

724
00:31:47,290 --> 00:31:49,600
pick a learning rate that seems to work

725
00:31:49,600 --> 00:31:51,160
for a while and then when it stops

726
00:31:51,160 --> 00:31:53,350
learning well they drop it down by about

727
00:31:53,350 --> 00:31:55,360
10 times and then they keep learning a

728
00:31:55,360 --> 00:31:56,650
bit more until it doesn't seem to be

729
00:31:56,650 --> 00:31:58,090
improving and they drop it down by

730
00:31:58,090 --> 00:32:00,220
another ten times that's what most

731
00:32:00,220 --> 00:32:02,470
academic research papers and most people

732
00:32:02,470 --> 00:32:04,570
in industry do so this would be like

733
00:32:04,570 --> 00:32:07,510
stepwise annealing very manual very

734
00:32:07,510 --> 00:32:11,860
annoying a better approach is simply to

735
00:32:11,860 --> 00:32:14,890
pick some kind of functional form like a

736
00:32:14,890 --> 00:32:17,470
line it turns out that a really good

737
00:32:17,470 --> 00:32:19,809
functional form is one half of the

738
00:32:19,809 --> 00:32:22,280
cosine curve

739
00:32:22,280 --> 00:32:25,790
right and the reason why is that for a

740
00:32:25,790 --> 00:32:27,770
while when you're not very close you

741
00:32:27,770 --> 00:32:29,270
kind of have a really high learning rate

742
00:32:29,270 --> 00:32:31,190
and that is you do get close you kind of

743
00:32:31,190 --> 00:32:33,290
quickly drop down and do a few

744
00:32:33,290 --> 00:32:35,390
iterations with a really low learning

745
00:32:35,390 --> 00:32:37,430
rate and so this is called cosine

746
00:32:37,430 --> 00:32:40,160
annealing so to those of you who haven't

747
00:32:40,160 --> 00:32:41,480
done trigonometry for a while

748
00:32:41,480 --> 00:32:44,210
cosine basically looks something like

749
00:32:44,210 --> 00:32:47,480
this right so we've picked one little

750
00:32:47,480 --> 00:32:52,280
half piece okay so we're going to use

751
00:32:52,280 --> 00:32:56,230
cosine annealing but here's the thing

752
00:32:56,230 --> 00:33:00,710
when you're in a very high dimensional

753
00:33:00,710 --> 00:33:02,660
space right near we're only able to show

754
00:33:02,660 --> 00:33:05,090
three dimensions right but in reality

755
00:33:05,090 --> 00:33:06,500
we've got hundreds of millions of

756
00:33:06,500 --> 00:33:10,930
dimensions we've got lots of different

757
00:33:10,930 --> 00:33:13,460
fairly flat points there no not the

758
00:33:13,460 --> 00:33:14,990
actual local minima but they're fairly

759
00:33:14,990 --> 00:33:17,480
flat points all of which are pretty good

760
00:33:17,480 --> 00:33:20,360
right but they might differ in a really

761
00:33:20,360 --> 00:33:22,520
interesting way which is that some of

762
00:33:22,520 --> 00:33:32,370
those flat points let me show you

763
00:33:32,370 --> 00:33:35,170
let's imagine we've got a surface that

764
00:33:35,170 --> 00:33:40,360
looks something like this

765
00:33:40,360 --> 00:33:43,890
right now imagine that where you kind of

766
00:33:43,890 --> 00:33:47,440
random guest started here and our

767
00:33:47,440 --> 00:33:49,419
initial therefore kind of learning rate

768
00:33:49,419 --> 00:33:51,779
annealing schedule got us down to here

769
00:33:51,779 --> 00:33:55,720
now indeed that's a pretty nice low

770
00:33:55,720 --> 00:33:57,850
error right but it probably doesn't

771
00:33:57,850 --> 00:34:00,190
generalize very well which is to say if

772
00:34:00,190 --> 00:34:02,380
we use a different data set where things

773
00:34:02,380 --> 00:34:04,750
are just kind of slightly different in

774
00:34:04,750 --> 00:34:07,149
one of these directions suddenly is a

775
00:34:07,149 --> 00:34:10,359
terrible solution right where else over

776
00:34:10,359 --> 00:34:13,899
here is basically equally good in terms

777
00:34:13,899 --> 00:34:16,179
of loss right but it rather suggests

778
00:34:16,179 --> 00:34:18,040
that if you move if you have slightly

779
00:34:18,040 --> 00:34:19,750
different data sets that are slightly

780
00:34:19,750 --> 00:34:21,669
moved in different directions it's still

781
00:34:21,669 --> 00:34:23,409
going to be good right so in other words

782
00:34:23,409 --> 00:34:25,419
we would expect this solution here is

783
00:34:25,419 --> 00:34:29,619
probably going to generalize better than

784
00:34:29,619 --> 00:34:33,159
this by key one so here's what we do is

785
00:34:33,159 --> 00:34:35,379
we've got like a bunch of different low

786
00:34:35,379 --> 00:34:39,700
bits right then our standard loading

787
00:34:39,700 --> 00:34:41,649
rate annealing approach will start of go

788
00:34:41,649 --> 00:34:43,720
down here or downhill downhill downhill

789
00:34:43,720 --> 00:34:47,290
downhill to one spot right but what we

790
00:34:47,290 --> 00:34:49,780
could do instead is use a learning rate

791
00:34:49,780 --> 00:34:54,220
schedule that looks like this which is

792
00:34:54,220 --> 00:34:56,349
to say we do a cosign annealing and then

793
00:34:56,349 --> 00:34:58,030
suddenly jump up again into a cosign

794
00:34:58,030 --> 00:34:59,530
annealing and then jump up again

795
00:34:59,530 --> 00:35:02,440
and so each time we jump up it means

796
00:35:02,440 --> 00:35:04,210
that if they're going to spiky bit and

797
00:35:04,210 --> 00:35:06,040
then we subtly increase the learning

798
00:35:06,040 --> 00:35:08,320
rate and it jumps now all the way over

799
00:35:08,320 --> 00:35:10,690
to here and so then we kind of learning

800
00:35:10,690 --> 00:35:11,890
right in your learning right near death

801
00:35:11,890 --> 00:35:13,660
down to here and then we jump up again

802
00:35:13,660 --> 00:35:16,690
to a high learning rate oh and it stays

803
00:35:16,690 --> 00:35:19,300
here right so in other words each time

804
00:35:19,300 --> 00:35:21,640
we jump up the learning rate that means

805
00:35:21,640 --> 00:35:24,160
that if it's in a nasty spiky part of

806
00:35:24,160 --> 00:35:26,050
the surface it's going to hop out of the

807
00:35:26,050 --> 00:35:28,150
spiky part and hopefully if we do that

808
00:35:28,150 --> 00:35:30,580
enough times it'll eventually find a

809
00:35:30,580 --> 00:35:40,070
nice smooth Bowl

810
00:35:40,070 --> 00:35:42,990
could you get the same effect by running

811
00:35:42,990 --> 00:35:44,730
multiple iterations through the

812
00:35:44,730 --> 00:35:46,260
different ground of my starting point so

813
00:35:46,260 --> 00:35:48,990
that eventually you explore all possible

814
00:35:48,990 --> 00:35:52,560
minimize yeah so in fact that that's a

815
00:35:52,560 --> 00:35:59,630
great question and before this approach

816
00:35:59,630 --> 00:36:01,830
which is called stochastic gradient

817
00:36:01,830 --> 00:36:05,240
descent with restarts was was created

818
00:36:05,240 --> 00:36:07,470
that's exactly what people used to do

819
00:36:07,470 --> 00:36:08,670
they used to create these things called

820
00:36:08,670 --> 00:36:11,810
ensembles where they would basically

821
00:36:11,810 --> 00:36:15,480
relearn a whole new model ten times in

822
00:36:15,480 --> 00:36:17,910
the hope that one of them's like but it

823
00:36:17,910 --> 00:36:22,500
ended up being better and so the cool

824
00:36:22,500 --> 00:36:24,090
thing about this decosta gradient

825
00:36:24,090 --> 00:36:26,100
descent with restarts is that the model

826
00:36:26,100 --> 00:36:29,010
once we're in a reasonably good spot

827
00:36:29,010 --> 00:36:30,810
each time we jump up the learning rate

828
00:36:30,810 --> 00:36:34,110
it doesn't restart it actually hangs out

829
00:36:34,110 --> 00:36:36,300
in this nice part part of the space and

830
00:36:36,300 --> 00:36:37,920
then keeps getting better so

831
00:36:37,920 --> 00:36:39,420
interestingly it turns out that this

832
00:36:39,420 --> 00:36:42,690
approach where we do this a bunch of

833
00:36:42,690 --> 00:36:45,960
separate cosine annealing steps we end

834
00:36:45,960 --> 00:36:49,260
up with a better result as then if we

835
00:36:49,260 --> 00:36:50,940
just randomly try it a few different

836
00:36:50,940 --> 00:36:54,300
starting points so it's a super neat

837
00:36:54,300 --> 00:36:58,200
trick and it's a fairly recent

838
00:36:58,200 --> 00:37:00,930
development but and again almost

839
00:37:00,930 --> 00:37:04,610
nobody's heard of it but I found like

840
00:37:04,610 --> 00:37:07,740
it's now like my superpower like using

841
00:37:07,740 --> 00:37:10,110
this along with the learning rate finder

842
00:37:10,110 --> 00:37:14,490
like I can get better results than

843
00:37:14,490 --> 00:37:16,050
nearly anybody like in a casual

844
00:37:16,050 --> 00:37:18,180
competition you know in the first week

845
00:37:18,180 --> 00:37:20,790
or two I can like jump in it's been an

846
00:37:20,790 --> 00:37:23,130
arrow to and back I've got a

847
00:37:23,130 --> 00:37:26,070
fantastically good result and so this is

848
00:37:26,070 --> 00:37:29,550
why I didn't pick the point where it's

849
00:37:29,550 --> 00:37:31,650
got the steepest slope I actually trying

850
00:37:31,650 --> 00:37:33,480
to pick something kind of aggressively

851
00:37:33,480 --> 00:37:35,490
high it's still getting down but maybe

852
00:37:35,490 --> 00:37:36,990
like getting to the point where it's

853
00:37:36,990 --> 00:37:39,030
nearly too high not because I want to

854
00:37:39,030 --> 00:37:40,590
make sure because that's because when we

855
00:37:40,590 --> 00:37:43,650
do this stochastic gradient descent with

856
00:37:43,650 --> 00:37:46,340
restarts this ten to the negative two

857
00:37:46,340 --> 00:37:48,090
represents the

858
00:37:48,090 --> 00:37:51,780
a highest number that it uses so it goes

859
00:37:51,780 --> 00:37:54,120
up to ten to the negative two and then

860
00:37:54,120 --> 00:37:56,130
goes down and then up to ten negative

861
00:37:56,130 --> 00:37:58,830
two and down so if I use to lower

862
00:37:58,830 --> 00:38:01,350
learning rate it's not going to jump to

863
00:38:01,350 --> 00:38:06,510
a different part of the function so I

864
00:38:06,510 --> 00:38:08,370
have a few questions but the first one

865
00:38:08,370 --> 00:38:10,290
is how many times do you change your

866
00:38:10,290 --> 00:38:13,380
learning rate you want to work we don't

867
00:38:13,380 --> 00:38:15,360
change the learning rate all three how

868
00:38:15,360 --> 00:38:17,460
many times - okay so in terms of this

869
00:38:17,460 --> 00:38:19,020
part here where it's going down we

870
00:38:19,020 --> 00:38:20,790
change the learning rate every single

871
00:38:20,790 --> 00:38:23,730
mini - all right and then the number of

872
00:38:23,730 --> 00:38:27,690
times we reset it is set by the cycle

873
00:38:27,690 --> 00:38:30,690
length parameter and so 1 means reset it

874
00:38:30,690 --> 00:38:33,840
up to every epoch so if I had to there

875
00:38:33,840 --> 00:38:36,090
it would reset it up to every to epochs

876
00:38:36,090 --> 00:38:38,280
and interestingly this this point that

877
00:38:38,280 --> 00:38:39,780
when we do the learning rate and

878
00:38:39,780 --> 00:38:41,070
kneeling that we actually change it

879
00:38:41,070 --> 00:38:43,880
every single batch it turns out to be

880
00:38:43,880 --> 00:38:47,190
really critical to making this work and

881
00:38:47,190 --> 00:38:48,900
it again is very different to what

882
00:38:48,900 --> 00:38:50,640
nearly everybody in industry in academia

883
00:38:50,640 --> 00:38:52,970
has done before

884
00:38:52,970 --> 00:38:54,990
what do you get a chance could you

885
00:38:54,990 --> 00:38:58,500
explain recompute it was true because

886
00:38:58,500 --> 00:39:02,190
it's still yeah we're going to come back

887
00:39:02,190 --> 00:39:04,770
to that multiple times in this course so

888
00:39:04,770 --> 00:39:06,330
the way this course has been a work is

889
00:39:06,330 --> 00:39:07,380
we're going to like do a really

890
00:39:07,380 --> 00:39:09,900
high-level version of each thing and

891
00:39:09,900 --> 00:39:11,400
then we're going to like come back to it

892
00:39:11,400 --> 00:39:12,930
in two or three lessons and then come

893
00:39:12,930 --> 00:39:14,220
back to it at the end of the course and

894
00:39:14,220 --> 00:39:15,840
each time we're going to see like more

895
00:39:15,840 --> 00:39:18,240
of the math more of the code and get a

896
00:39:18,240 --> 00:39:20,520
deeper view okay and we can talk about

897
00:39:20,520 --> 00:39:26,540
it also in the forums during the week

898
00:39:26,540 --> 00:39:29,580
our main goal is to generalize and we

899
00:39:29,580 --> 00:39:31,970
don't want to get those like narrow

900
00:39:31,970 --> 00:39:34,590
demas yeah that's a it's a very short

901
00:39:34,590 --> 00:39:37,710
summary this method are we keeping track

902
00:39:37,710 --> 00:39:40,760
off to minimize and averaging them ah

903
00:39:40,760 --> 00:39:44,010
that's that's another level of

904
00:39:44,010 --> 00:39:45,750
sophistication and indeed you can see

905
00:39:45,750 --> 00:39:47,280
there's something here called snapshot

906
00:39:47,280 --> 00:39:50,040
ensemble so we're not doing it in the

907
00:39:50,040 --> 00:39:53,190
code right now but yes if you wanted to

908
00:39:53,190 --> 00:39:55,410
make us generalize even better you can

909
00:39:55,410 --> 00:39:58,590
save the weights here and here and here

910
00:39:58,590 --> 00:40:00,980
and then take the average

911
00:40:00,980 --> 00:40:03,650
ishes but for now we're just going to

912
00:40:03,650 --> 00:40:10,940
pick the last one if you want to skip

913
00:40:10,940 --> 00:40:16,310
ahead if you want to skip ahead there's

914
00:40:16,310 --> 00:40:19,970
a parameter called cycle safe name which

915
00:40:19,970 --> 00:40:21,530
you can add as well as cycle them and

916
00:40:21,530 --> 00:40:23,630
that will save a set of weights at the

917
00:40:23,630 --> 00:40:26,359
end of every learning rate cycle and

918
00:40:26,359 --> 00:40:35,150
then you can ensemble them ok so we've

919
00:40:35,150 --> 00:40:38,359
got a pretty decent model here ninety

920
00:40:38,359 --> 00:40:41,240
nine point three percent accuracy and

921
00:40:41,240 --> 00:40:42,859
we've gone through of you know a few

922
00:40:42,859 --> 00:40:44,720
steps that is taken you know a minute or

923
00:40:44,720 --> 00:40:47,330
two to run and so from time to time I

924
00:40:47,330 --> 00:40:49,010
tend to save my weight so if you go

925
00:40:49,010 --> 00:40:51,260
learn dot save and then pass in a file

926
00:40:51,260 --> 00:40:53,510
name it's going to go ahead and save

927
00:40:53,510 --> 00:40:55,670
that for you later on if you go learn

928
00:40:55,670 --> 00:40:57,890
load you'll be straight back to where

929
00:40:57,890 --> 00:41:00,530
you came from okay so it's a good idea

930
00:41:00,530 --> 00:41:03,350
to do that from time to time this is a

931
00:41:03,350 --> 00:41:06,410
good time to mention what happens when

932
00:41:06,410 --> 00:41:10,040
you do this when you go learn dot save

933
00:41:10,040 --> 00:41:12,380
when you create precomputed activations

934
00:41:12,380 --> 00:41:14,270
another thing we learn about soon when

935
00:41:14,270 --> 00:41:16,760
you create resized images these are all

936
00:41:16,760 --> 00:41:20,090
creating various temporary files okay

937
00:41:20,090 --> 00:41:28,070
and so what happens is if we go to data

938
00:41:28,070 --> 00:41:34,580
and we go to dogs cats this is my data

939
00:41:34,580 --> 00:41:36,710
folder and you'll see there's a folder

940
00:41:36,710 --> 00:41:41,030
here called TMP or - and so this is

941
00:41:41,030 --> 00:41:43,609
automatically created and all of my pre

942
00:41:43,609 --> 00:41:46,430
computed activations end up in here I

943
00:41:46,430 --> 00:41:50,420
mention this because if if things are if

944
00:41:50,420 --> 00:41:52,550
you're getting weird errors that might

945
00:41:52,550 --> 00:41:54,020
be because you've got some Oh pre

946
00:41:54,020 --> 00:41:56,090
computed activations like we're only

947
00:41:56,090 --> 00:41:59,090
half completed or are in some way

948
00:41:59,090 --> 00:42:01,130
incompatible with what you're doing so

949
00:42:01,130 --> 00:42:03,080
you can always go ahead and just delete

950
00:42:03,080 --> 00:42:05,990
this TMP this temporary directory and

951
00:42:05,990 --> 00:42:08,090
see if that causes your error to go away

952
00:42:08,090 --> 00:42:10,820
this is the faster I equivalent of

953
00:42:10,820 --> 00:42:12,920
turning it off and then on again

954
00:42:12,920 --> 00:42:14,990
you'll also see there's a directory

955
00:42:14,990 --> 00:42:16,730
called models and that's where all of

956
00:42:16,730 --> 00:42:19,160
these when you say dot save with a model

957
00:42:19,160 --> 00:42:22,520
that's where that's going to go actually

958
00:42:22,520 --> 00:42:23,900
it reminds me when the stochastic

959
00:42:23,900 --> 00:42:25,610
gradient descent with restarts paper

960
00:42:25,610 --> 00:42:27,800
came out I saw a tweet that was somebody

961
00:42:27,800 --> 00:42:29,150
was like Oh to make your deep learning

962
00:42:29,150 --> 00:42:30,620
work better turn it off and then on

963
00:42:30,620 --> 00:42:38,180
again question it so if I want to see I

964
00:42:38,180 --> 00:42:40,100
want to retrain my model fuselage again

965
00:42:40,100 --> 00:42:43,400
do I just do everything the 10 folder if

966
00:42:43,400 --> 00:42:51,230
you want if you want to train your model

967
00:42:51,230 --> 00:42:54,260
from scratch there's generally no reason

968
00:42:54,260 --> 00:42:56,570
to delete the pre computed activations

969
00:42:56,570 --> 00:42:59,260
because the pre computed activations are

970
00:42:59,260 --> 00:43:02,480
without any training that's what the pre

971
00:43:02,480 --> 00:43:06,410
trained model created with the with the

972
00:43:06,410 --> 00:43:07,580
weights that you downloaded off the

973
00:43:07,580 --> 00:43:12,440
internet the only yeah I mean the only

974
00:43:12,440 --> 00:43:14,120
reason you want to delete the pre

975
00:43:14,120 --> 00:43:15,770
computed activations is that there was

976
00:43:15,770 --> 00:43:18,410
some error caused by like half creating

977
00:43:18,410 --> 00:43:20,810
them and crashing or some something like

978
00:43:20,810 --> 00:43:23,570
that as you change the size of your

979
00:43:23,570 --> 00:43:25,400
import change different architectures

980
00:43:25,400 --> 00:43:26,870
and so forth they all create different

981
00:43:26,870 --> 00:43:28,490
sets of activations with different file

982
00:43:28,490 --> 00:43:30,320
names so you don't generally you

983
00:43:30,320 --> 00:43:32,540
shouldn't have to worry about it if you

984
00:43:32,540 --> 00:43:33,680
want to start training again from

985
00:43:33,680 --> 00:43:35,960
scratch all you have to do is create a

986
00:43:35,960 --> 00:43:40,250
new learn object so each time you go

987
00:43:40,250 --> 00:43:42,530
like conch learner dot pre-trained that

988
00:43:42,530 --> 00:43:45,620
creates a new object with with new sets

989
00:43:45,620 --> 00:43:50,960
of weights fever train from okay so

990
00:43:50,960 --> 00:43:52,850
before our break we'll finish off by

991
00:43:52,850 --> 00:43:56,770
talking about about fine-tuning and

992
00:43:56,770 --> 00:44:00,680
differential learning rates and so so

993
00:44:00,680 --> 00:44:05,780
far everything we've done has not

994
00:44:05,780 --> 00:44:08,180
changed any of these free trained

995
00:44:08,180 --> 00:44:09,950
filters right so we've used a pre

996
00:44:09,950 --> 00:44:12,110
trained model that already knows how to

997
00:44:12,110 --> 00:44:17,510
find at the early stages edges

998
00:44:17,510 --> 00:44:21,400
ingredients and then corners and curves

999
00:44:21,400 --> 00:44:26,090
and then repeating patterns and bits of

1000
00:44:26,090 --> 00:44:26,660
text

1001
00:44:26,660 --> 00:44:30,559
and eventually eyeballs right we have

1002
00:44:30,559 --> 00:44:35,869
not retrained any of those activations

1003
00:44:35,869 --> 00:44:38,569
any of those features well specifically

1004
00:44:38,569 --> 00:44:40,640
any of those weights in the

1005
00:44:40,640 --> 00:44:43,190
convolutional kernels all we've done is

1006
00:44:43,190 --> 00:44:46,549
we've learnt some new layers that we've

1007
00:44:46,549 --> 00:44:48,200
added on top of these things we've

1008
00:44:48,200 --> 00:44:50,119
learned how to mix and match these

1009
00:44:50,119 --> 00:44:54,529
pre-trained features now obviously it

1010
00:44:54,529 --> 00:44:58,460
may turn out that your pictures have you

1011
00:44:58,460 --> 00:45:00,680
know different kinds of eyeballs or

1012
00:45:00,680 --> 00:45:03,380
faces or if you're using different kinds

1013
00:45:03,380 --> 00:45:05,720
of images like satellite images totally

1014
00:45:05,720 --> 00:45:07,069
different kinds of features altogether

1015
00:45:07,069 --> 00:45:10,069
right so if you're like training to

1016
00:45:10,069 --> 00:45:12,650
recognize icebergs you'll probably want

1017
00:45:12,650 --> 00:45:15,470
to go all the way back and learn you

1018
00:45:15,470 --> 00:45:16,670
know all the way back to kind of

1019
00:45:16,670 --> 00:45:18,650
different combinations of these simple

1020
00:45:18,650 --> 00:45:22,430
gradients and edges in our cases dogs

1021
00:45:22,430 --> 00:45:24,829
vs. cats we're going to have some minor

1022
00:45:24,829 --> 00:45:26,720
differences but we still may find it's

1023
00:45:26,720 --> 00:45:29,740
helpful to slightly tune some of these

1024
00:45:29,740 --> 00:45:34,069
later layers as well so to tell the

1025
00:45:34,069 --> 00:45:36,200
learner that we now want to start

1026
00:45:36,200 --> 00:45:38,750
actually changing the convolutional

1027
00:45:38,750 --> 00:45:41,390
filters themselves we simply say

1028
00:45:41,390 --> 00:45:44,539
unfreeze okay so a frozen layer is a

1029
00:45:44,539 --> 00:45:46,430
layer which is not trained is not

1030
00:45:46,430 --> 00:45:49,640
updated okay so unfreeze unfreezes all

1031
00:45:49,640 --> 00:45:51,950
of the layers now when you think about

1032
00:45:51,950 --> 00:45:56,920
it it's pretty obvious that layer one

1033
00:45:56,920 --> 00:45:59,809
right which is like a diagonal edge or a

1034
00:45:59,809 --> 00:46:02,509
gradient probably doesn't need to change

1035
00:46:02,509 --> 00:46:05,089
by much if at all right from the 1 and a

1036
00:46:05,089 --> 00:46:06,920
half million images on image net it

1037
00:46:06,920 --> 00:46:08,299
probably already is figured out pretty

1038
00:46:08,299 --> 00:46:10,849
well how to find like edges of gradients

1039
00:46:10,849 --> 00:46:13,099
it probably already knows also like

1040
00:46:13,099 --> 00:46:15,319
which kind of corners to look for and

1041
00:46:15,319 --> 00:46:17,390
how to find which kinds of curves and so

1042
00:46:17,390 --> 00:46:19,759
forth so in other words these early

1043
00:46:19,759 --> 00:46:22,450
layers probably need little if any

1044
00:46:22,450 --> 00:46:26,269
learning where else these later ones are

1045
00:46:26,269 --> 00:46:28,549
much more likely to need more learning

1046
00:46:28,549 --> 00:46:30,650
and this is universally true regardless

1047
00:46:30,650 --> 00:46:32,869
of whether you're looking for satellite

1048
00:46:32,869 --> 00:46:34,940
images of rainforests or icebergs or

1049
00:46:34,940 --> 00:46:36,200
whether you're looking for cats versus

1050
00:46:36,200 --> 00:46:38,809
dogs right

1051
00:46:38,809 --> 00:46:41,809
so what we do is we create an array of

1052
00:46:41,809 --> 00:46:45,109
learning rates where we say okay these

1053
00:46:45,109 --> 00:46:47,869
are the learning rates to use for our

1054
00:46:47,869 --> 00:46:50,419
additional layers that we've added on

1055
00:46:50,419 --> 00:46:53,239
top these are the learning rates to use

1056
00:46:53,239 --> 00:46:56,659
in the middle few layers and these are

1057
00:46:56,659 --> 00:46:58,759
the learning rates to use for the first

1058
00:46:58,759 --> 00:47:00,799
few layers so these are the ones for the

1059
00:47:00,799 --> 00:47:02,569
layers that represent like very basic

1060
00:47:02,569 --> 00:47:05,569
geometric features these are the ones

1061
00:47:05,569 --> 00:47:08,719
that are used to for the more complex

1062
00:47:08,719 --> 00:47:11,509
kind of sophisticated convolutional

1063
00:47:11,509 --> 00:47:13,369
features and these are the ones that are

1064
00:47:13,369 --> 00:47:14,689
used for the features that we've added

1065
00:47:14,689 --> 00:47:17,539
and went from stretch right so you can

1066
00:47:17,539 --> 00:47:19,900
create a array of learning rates and

1067
00:47:19,900 --> 00:47:22,729
then when we called up fit and pass an

1068
00:47:22,729 --> 00:47:24,409
array of learning rates it's now going

1069
00:47:24,409 --> 00:47:25,729
to use those different learning rates

1070
00:47:25,729 --> 00:47:30,489
for different parts of the model this is

1071
00:47:30,489 --> 00:47:34,759
not something that we've like invented

1072
00:47:34,759 --> 00:47:37,429
but I'd also say it's like it's so not

1073
00:47:37,429 --> 00:47:39,650
that common that it doesn't even have a

1074
00:47:39,650 --> 00:47:42,499
name as far as I know so we're going to

1075
00:47:42,499 --> 00:47:46,519
call it differential learning rates if

1076
00:47:46,519 --> 00:47:48,380
it actually has a name or indeed if

1077
00:47:48,380 --> 00:47:49,519
somebody's actually written a paper

1078
00:47:49,519 --> 00:47:51,769
specifically talking about it I don't

1079
00:47:51,769 --> 00:47:54,829
know there's a great researcher called

1080
00:47:54,829 --> 00:47:56,809
Jason your Sinskey who who did write a

1081
00:47:56,809 --> 00:47:58,429
paper about the kind of the idea that

1082
00:47:58,429 --> 00:47:59,719
you might want different learning rates

1083
00:47:59,719 --> 00:48:02,479
and showing why but I don't think any

1084
00:48:02,479 --> 00:48:05,209
other libraries support it and yeah I

1085
00:48:05,209 --> 00:48:07,579
don't know of a name for it having said

1086
00:48:07,579 --> 00:48:10,609
that though this ability to like

1087
00:48:10,609 --> 00:48:12,919
unfreeze and then use these differential

1088
00:48:12,919 --> 00:48:14,959
learning rates I found it's like the

1089
00:48:14,959 --> 00:48:17,929
secret to taking a pretty good model and

1090
00:48:17,929 --> 00:48:25,909
putting it into an awesome model so just

1091
00:48:25,909 --> 00:48:29,449
to clarify so you have three numbers

1092
00:48:29,449 --> 00:48:32,359
there okay three hyper parameters the

1093
00:48:32,359 --> 00:48:36,140
first one is the photo late model so the

1094
00:48:36,140 --> 00:48:41,199
mall that are late layers the so with

1095
00:48:41,199 --> 00:48:44,209
the it's your answer is many many right

1096
00:48:44,209 --> 00:48:46,309
and they're kind of in groups and we're

1097
00:48:46,309 --> 00:48:47,449
going to learn about the architecture

1098
00:48:47,449 --> 00:48:49,159
this is called a ResNet for residual

1099
00:48:49,159 --> 00:48:50,130
network

1100
00:48:50,130 --> 00:48:53,190
it kind of has ResNet blocks and so what

1101
00:48:53,190 --> 00:48:54,779
we're doing is we're grouping the blocks

1102
00:48:54,779 --> 00:48:58,259
into three groups and so this one is

1103
00:48:58,259 --> 00:49:00,019
actually this first number is for the

1104
00:49:00,019 --> 00:49:04,589
earliest layers yeah they're ones

1105
00:49:04,589 --> 00:49:06,119
closest to the pixels that represent

1106
00:49:06,119 --> 00:49:09,059
like corners and edges and gradients but

1107
00:49:09,059 --> 00:49:12,930
why why do you well I thought those

1108
00:49:12,930 --> 00:49:15,569
layers are frozen at first so yeah right

1109
00:49:15,569 --> 00:49:17,339
so we just said unfreeze the streets

1110
00:49:17,339 --> 00:49:19,019
also we so yeah I'm freezing them

1111
00:49:19,019 --> 00:49:20,460
because you have kind of partially

1112
00:49:20,460 --> 00:49:24,059
trained although lately we've trained

1113
00:49:24,059 --> 00:49:26,579
we've trained our added layers yes now

1114
00:49:26,579 --> 00:49:28,549
you are we training the Oh step exactly

1115
00:49:28,549 --> 00:49:30,869
obviously so it waits and the learning

1116
00:49:30,869 --> 00:49:33,329
rate is particularly small for the early

1117
00:49:33,329 --> 00:49:34,829
layers that's right because you just

1118
00:49:34,829 --> 00:49:37,890
find a want to find food yeah yeah we

1119
00:49:37,890 --> 00:49:39,210
probably don't want to change them at

1120
00:49:39,210 --> 00:49:42,569
all but you know if it does need to then

1121
00:49:42,569 --> 00:49:50,160
it can thanks no problem so using the

1122
00:49:50,160 --> 00:49:51,930
differential in rates a little different

1123
00:49:51,930 --> 00:49:56,069
from like grid search there's no

1124
00:49:56,069 --> 00:49:58,109
similarity to grid search so grid search

1125
00:49:58,109 --> 00:50:00,180
is where we're trying to find the best

1126
00:50:00,180 --> 00:50:03,719
hyper parameter for something so for

1127
00:50:03,719 --> 00:50:07,219
example you could kind of think of the

1128
00:50:07,219 --> 00:50:09,539
learning rate finder as a really

1129
00:50:09,539 --> 00:50:11,460
sophisticated grid search which is like

1130
00:50:11,460 --> 00:50:12,869
trying lots and lots of learning rates

1131
00:50:12,869 --> 00:50:15,350
to find which one is best

1132
00:50:15,350 --> 00:50:16,490
but this has nothing to do with that

1133
00:50:16,490 --> 00:50:18,650
this is actually for the entire training

1134
00:50:18,650 --> 00:50:20,990
from now on it's actually going to use a

1135
00:50:20,990 --> 00:50:28,480
different learning rate for each layer

1136
00:50:28,480 --> 00:50:33,349
and so I was wondering so you give a pre

1137
00:50:33,349 --> 00:50:35,630
train model then you have to use the

1138
00:50:35,630 --> 00:50:39,230
same input dimensions right because I

1139
00:50:39,230 --> 00:50:40,640
was thinking okay let's say you have

1140
00:50:40,640 --> 00:50:43,819
this big they use like big machines to

1141
00:50:43,819 --> 00:50:45,079
train these things and you want to take

1142
00:50:45,079 --> 00:50:46,880
advantage of it how would you go about

1143
00:50:46,880 --> 00:50:48,500
you know you have like images that are

1144
00:50:48,500 --> 00:50:50,930
like bigger than the ones that they used

1145
00:50:50,930 --> 00:50:52,789
or we're going to be talking about sizes

1146
00:50:52,789 --> 00:50:54,950
later but the short answer is that with

1147
00:50:54,950 --> 00:50:56,769
this library and the modern

1148
00:50:56,769 --> 00:50:58,849
architectures were using we can use any

1149
00:50:58,849 --> 00:51:04,819
size we like so did I mean do we need

1150
00:51:04,819 --> 00:51:08,119
can we at least just a specific layer we

1151
00:51:08,119 --> 00:51:09,650
can we're not doing it yet but if you

1152
00:51:09,650 --> 00:51:12,109
wanted to you can learn dot freeze

1153
00:51:12,109 --> 00:51:14,420
underscore two and pass into layer

1154
00:51:14,420 --> 00:51:21,380
number much to my surprise or at least

1155
00:51:21,380 --> 00:51:23,839
initial my surprise it turns out I

1156
00:51:23,839 --> 00:51:26,569
almost never need to do that I almost

1157
00:51:26,569 --> 00:51:28,490
never find it helpful and I think it's

1158
00:51:28,490 --> 00:51:29,569
because we're using differential

1159
00:51:29,569 --> 00:51:33,440
learning rates the the optimizer can

1160
00:51:33,440 --> 00:51:35,720
kind of learn just as much as it needs

1161
00:51:35,720 --> 00:51:45,650
to so yeah it's a little data like very

1162
00:51:45,650 --> 00:51:49,279
little data yeah it still doesn't seem

1163
00:51:49,279 --> 00:51:51,019
to help the one place I have found it

1164
00:51:51,019 --> 00:51:54,380
helpful is if I'm using like a really

1165
00:51:54,380 --> 00:51:56,450
big memory intensive model and I'm like

1166
00:51:56,450 --> 00:52:01,430
running out of GPU crazy having the the

1167
00:52:01,430 --> 00:52:03,440
less layers you unfreeze the less memory

1168
00:52:03,440 --> 00:52:05,210
it takes and the less time it takes so

1169
00:52:05,210 --> 00:52:07,279
there's that kind of practical aspect so

1170
00:52:07,279 --> 00:52:09,619
to me she'll say I asked the question

1171
00:52:09,619 --> 00:52:13,519
right can I just like unfreezes specific

1172
00:52:13,519 --> 00:52:16,400
layer no you you can only unfreeze

1173
00:52:16,400 --> 00:52:21,769
layers from layer n onwards you could

1174
00:52:21,769 --> 00:52:23,180
probably delve inside the library in

1175
00:52:23,180 --> 00:52:24,740
phase one phase one layer but I don't

1176
00:52:24,740 --> 00:52:28,089
know why you would

1177
00:52:28,089 --> 00:52:30,200
okay so I'm really excited to be showing

1178
00:52:30,200 --> 00:52:31,490
you guys this stuff because it's like

1179
00:52:31,490 --> 00:52:32,630
it's something we've been kind of

1180
00:52:32,630 --> 00:52:34,430
researching all year it's figuring out

1181
00:52:34,430 --> 00:52:37,369
how to train state of the art models and

1182
00:52:37,369 --> 00:52:39,619
we've kind of found these like tiny

1183
00:52:39,619 --> 00:52:41,930
number of tricks and so once we do that

1184
00:52:41,930 --> 00:52:45,410
we now go learn about fit right and you

1185
00:52:45,410 --> 00:52:46,880
can see look at this we get right up to

1186
00:52:46,880 --> 00:52:51,280
that 99.5% accuracy which is crazy

1187
00:52:51,280 --> 00:52:54,170
there's one other trick you might see

1188
00:52:54,170 --> 00:52:55,849
here that as well as using stochastic

1189
00:52:55,849 --> 00:52:58,490
gradient descent with restarts a cycle

1190
00:52:58,490 --> 00:53:00,800
length equals one we've done three

1191
00:53:00,800 --> 00:53:04,070
cycles so earlier on I lied to you I

1192
00:53:04,070 --> 00:53:05,480
said this is this is the number of

1193
00:53:05,480 --> 00:53:07,130
epochs it's actually the number of

1194
00:53:07,130 --> 00:53:09,200
cyclists right so if you said cycle

1195
00:53:09,200 --> 00:53:11,420
length equals two it would do three

1196
00:53:11,420 --> 00:53:14,900
cycles of each of two epochs or do six

1197
00:53:14,900 --> 00:53:17,240
because so here I've said two three

1198
00:53:17,240 --> 00:53:19,250
cycles yet somehow it's done seven

1199
00:53:19,250 --> 00:53:22,040
epochs and the reason why is I've got

1200
00:53:22,040 --> 00:53:23,300
one last trick to show you which is

1201
00:53:23,300 --> 00:53:26,180
cycle mult equals two and to tell you

1202
00:53:26,180 --> 00:53:27,859
what that does I'm simply going to draw

1203
00:53:27,859 --> 00:53:30,740
you a picture you the picture if I go

1204
00:53:30,740 --> 00:53:32,480
learn Dutch share top plot learning rate

1205
00:53:32,480 --> 00:53:34,000
there it is

1206
00:53:34,000 --> 00:53:36,440
now you can see what cycle mode equals

1207
00:53:36,440 --> 00:53:40,130
to is doing okay it's it's doubling the

1208
00:53:40,130 --> 00:53:42,619
length of the cycle after each cycle and

1209
00:53:42,619 --> 00:53:44,630
so in the paper that introduced this

1210
00:53:44,630 --> 00:53:46,010
stochastic gradient descent with

1211
00:53:46,010 --> 00:53:46,760
restarts

1212
00:53:46,760 --> 00:53:49,520
the researcher kind of said hey this is

1213
00:53:49,520 --> 00:53:51,650
something that seems to sometimes work

1214
00:53:51,650 --> 00:53:53,630
pretty well and I've certainly found

1215
00:53:53,630 --> 00:53:57,310
that often to be the case so basically

1216
00:53:57,310 --> 00:54:00,170
intuitively speaking if your cycle

1217
00:54:00,170 --> 00:54:04,069
length is too short right then it's kind

1218
00:54:04,069 --> 00:54:06,109
of starts going down to find a good spot

1219
00:54:06,109 --> 00:54:08,300
and then it pops out and it goes to a

1220
00:54:08,300 --> 00:54:09,950
try and photographs button pops out it

1221
00:54:09,950 --> 00:54:11,300
never actually gets to find a good spot

1222
00:54:11,300 --> 00:54:14,480
right so earlier on you want it to do

1223
00:54:14,480 --> 00:54:16,040
that because it's trying to find the bit

1224
00:54:16,040 --> 00:54:18,470
that's like smoother but then later on

1225
00:54:18,470 --> 00:54:20,300
you want it to fight do more exploring

1226
00:54:20,300 --> 00:54:22,910
and then more exploring right so that's

1227
00:54:22,910 --> 00:54:26,300
why this cycle mole equals two thing

1228
00:54:26,300 --> 00:54:28,970
often seems to be a pretty good approach

1229
00:54:28,970 --> 00:54:32,660
right so suddenly we're introducing more

1230
00:54:32,660 --> 00:54:34,160
and more hyper parameters having told

1231
00:54:34,160 --> 00:54:36,349
you that there aren't that many but

1232
00:54:36,349 --> 00:54:38,420
the reason is that like you can really

1233
00:54:38,420 --> 00:54:40,579
get away with just taking a good

1234
00:54:40,579 --> 00:54:42,950
learning rate but then adding these

1235
00:54:42,950 --> 00:54:47,869
extra tweaks really helps get that extra

1236
00:54:47,869 --> 00:54:50,690
level up without any effort right and so

1237
00:54:50,690 --> 00:54:55,579
in practice I find this kind of three

1238
00:54:55,579 --> 00:54:59,690
cycles starting at 1 mode equals 2 works

1239
00:54:59,690 --> 00:55:02,180
very very often to get a pretty decent

1240
00:55:02,180 --> 00:55:07,130
model if it does doesn't then often I'll

1241
00:55:07,130 --> 00:55:10,670
just do 3 cycles of length 2 with no

1242
00:55:10,670 --> 00:55:11,660
molt

1243
00:55:11,660 --> 00:55:12,979
okay there's kind of like two things

1244
00:55:12,979 --> 00:55:14,989
that seem to work a lot and there's not

1245
00:55:14,989 --> 00:55:17,690
too much fiddling I find necessary and

1246
00:55:17,690 --> 00:55:19,579
as I say even even if you just if you

1247
00:55:19,579 --> 00:55:21,979
use this line every time I'd be

1248
00:55:21,979 --> 00:55:23,660
surprised if you didn't get a reasonable

1249
00:55:23,660 --> 00:55:29,539
result so a question here why does a

1250
00:55:29,539 --> 00:55:32,479
smoother services correlate to more

1251
00:55:32,479 --> 00:55:40,849
generalize networks so it's kind of this

1252
00:55:40,849 --> 00:55:44,239
some this intuitive explanation I try to

1253
00:55:44,239 --> 00:55:46,339
just kill the whole thing I try to give

1254
00:55:46,339 --> 00:55:51,369
back here which is that if you've got

1255
00:55:51,369 --> 00:55:58,420
something spiky right and so what this

1256
00:55:58,420 --> 00:56:02,440
what this x-axis is showing is like how

1257
00:56:02,440 --> 00:56:05,089
how good is this at recognizing dogs

1258
00:56:05,089 --> 00:56:06,739
versus cats as you change this

1259
00:56:06,739 --> 00:56:10,069
particular parameter right and so so

1260
00:56:10,069 --> 00:56:12,589
something to be generalizable that means

1261
00:56:12,589 --> 00:56:14,150
that we wanted to work when we give it

1262
00:56:14,150 --> 00:56:15,259
when we give it a slightly different

1263
00:56:15,259 --> 00:56:17,329
data set and so a slightly different

1264
00:56:17,329 --> 00:56:20,509
data set may have a slightly different

1265
00:56:20,509 --> 00:56:22,880
relationship between this parameter and

1266
00:56:22,880 --> 00:56:25,400
how caddy versus dog it is it may

1267
00:56:25,400 --> 00:56:30,060
instead look a little bit like this

1268
00:56:30,060 --> 00:56:33,820
right so in other words if we end up at

1269
00:56:33,820 --> 00:56:37,240
this point right then it's not going to

1270
00:56:37,240 --> 00:56:38,800
do a good job on this slightly different

1271
00:56:38,800 --> 00:56:40,510
data set for else if we end up on this

1272
00:56:40,510 --> 00:56:42,160
point it's still going to do a good job

1273
00:56:42,160 --> 00:56:49,230
on this data set

1274
00:56:49,230 --> 00:56:52,480
okay so that's what psychomotor equals

1275
00:56:52,480 --> 00:56:54,310
doing okay so we've got one last thing

1276
00:56:54,310 --> 00:56:55,900
before we going to take a break which is

1277
00:56:55,900 --> 00:56:58,930
we're now going to take this model which

1278
00:56:58,930 --> 00:57:01,540
has 99.5 percent accuracy and we're

1279
00:57:01,540 --> 00:57:02,590
going to try and make it better still

1280
00:57:02,590 --> 00:57:05,020
and what we're going to do is we're not

1281
00:57:05,020 --> 00:57:06,550
actually going to change the model at

1282
00:57:06,550 --> 00:57:08,859
all right but instead we're going to

1283
00:57:08,859 --> 00:57:13,330
look back at the original virtual

1284
00:57:13,330 --> 00:57:15,369
visualization we did where we looked at

1285
00:57:15,369 --> 00:57:22,480
some of our incorrect pictures now what

1286
00:57:22,480 --> 00:57:24,040
I've done is I've printed out the whole

1287
00:57:24,040 --> 00:57:26,590
of these incorrect pictures but the key

1288
00:57:26,590 --> 00:57:30,760
thing to realize is that particularly in

1289
00:57:30,760 --> 00:57:34,050
fact when we do the the validation set

1290
00:57:34,050 --> 00:57:37,960
all of our inputs to our model all the

1291
00:57:37,960 --> 00:57:40,990
time have to be square right and the

1292
00:57:40,990 --> 00:57:43,750
reason for that is it's kind of a minor

1293
00:57:43,750 --> 00:57:45,820
technical detail but basically the GPU

1294
00:57:45,820 --> 00:57:48,490
doesn't go very quickly if you have like

1295
00:57:48,490 --> 00:57:50,200
different dimensions for different

1296
00:57:50,200 --> 00:57:51,640
images because it needs seems to be

1297
00:57:51,640 --> 00:57:53,710
consistent so that every part of the GPU

1298
00:57:53,710 --> 00:57:55,839
can do the same thing and I think this

1299
00:57:55,839 --> 00:57:57,430
is probably fixable but it now that's

1300
00:57:57,430 --> 00:57:59,050
the state of the technology we have so

1301
00:57:59,050 --> 00:58:01,660
our validation set when we actually say

1302
00:58:01,660 --> 00:58:03,310
for this particular thing is it's a dog

1303
00:58:03,310 --> 00:58:05,380
what we actually do to make it square as

1304
00:58:05,380 --> 00:58:08,410
we just pick out the square in the

1305
00:58:08,410 --> 00:58:10,510
middle right so we would take off its

1306
00:58:10,510 --> 00:58:12,820
two edges and so we take the whole

1307
00:58:12,820 --> 00:58:15,010
height and then as much of the middle as

1308
00:58:15,010 --> 00:58:16,930
we can and so you can see in this case

1309
00:58:16,930 --> 00:58:19,380
we wouldn't actually see this dog's head

1310
00:58:19,380 --> 00:58:21,730
right so I think the reason this was

1311
00:58:21,730 --> 00:58:24,130
actually not correctly classified was

1312
00:58:24,130 --> 00:58:26,230
because the validation set only got to

1313
00:58:26,230 --> 00:58:29,470
see the body and the body doesn't look

1314
00:58:29,470 --> 00:58:31,720
particularly doglike or cat-like it's

1315
00:58:31,720 --> 00:58:35,140
not at all punctual what it is so what

1316
00:58:35,140 --> 00:58:38,140
we're going to do when we calculate the

1317
00:58:38,140 --> 00:58:39,790
predictions for our validation set is

1318
00:58:39,790 --> 00:58:40,960
we're going to use something called test

1319
00:58:40,960 --> 00:58:43,780
time augmentation and what this means is

1320
00:58:43,780 --> 00:58:46,119
that every time we decide is this cat or

1321
00:58:46,119 --> 00:58:48,070
a dog not in the training but after

1322
00:58:48,070 --> 00:58:50,140
we've trained the model is we're going

1323
00:58:50,140 --> 00:58:56,349
to actually take four random data

1324
00:58:56,349 --> 00:58:58,330
augmentations and remember the data

1325
00:58:58,330 --> 00:59:00,310
augmentations move around

1326
00:59:00,310 --> 00:59:03,400
and zoom in and out and flip okay

1327
00:59:03,400 --> 00:59:04,870
so we're going to take four of them at

1328
00:59:04,870 --> 00:59:06,850
random and we're going to take the

1329
00:59:06,850 --> 00:59:09,790
original and augmented sent a cropped

1330
00:59:09,790 --> 00:59:11,440
image and we're going to do a prediction

1331
00:59:11,440 --> 00:59:13,570
for all of those and then we're going to

1332
00:59:13,570 --> 00:59:16,630
take the average of those predictions so

1333
00:59:16,630 --> 00:59:18,880
I'm going to say is this a cat is this a

1334
00:59:18,880 --> 00:59:21,460
cat is this a cat is this a cat but and

1335
00:59:21,460 --> 00:59:24,370
so hopefully in one of those random ones

1336
00:59:24,370 --> 00:59:26,020
we actually make sure that the face is

1337
00:59:26,020 --> 00:59:28,480
there zoomed in by a similar amount to

1338
00:59:28,480 --> 00:59:30,340
other dogs faces at sea and it's rotated

1339
00:59:30,340 --> 00:59:31,840
by the amount that it expects to see it

1340
00:59:31,840 --> 00:59:36,040
and so forth and so do that all we have

1341
00:59:36,040 --> 00:59:38,890
to do is just call tt8

1342
00:59:38,890 --> 00:59:41,200
TTA stands for Test time augmentation

1343
00:59:41,200 --> 00:59:43,960
this term of like what a what do we call

1344
00:59:43,960 --> 00:59:45,610
up when we're making predictions from up

1345
00:59:45,610 --> 00:59:47,110
from a model we've trained sometimes

1346
00:59:47,110 --> 00:59:48,730
it's called inference time sometimes

1347
00:59:48,730 --> 00:59:50,590
it's called test time everybody since

1348
00:59:50,590 --> 00:59:53,200
have a different name so TTA and so when

1349
00:59:53,200 --> 00:59:55,600
we do that we go learn TTA check the

1350
00:59:55,600 --> 00:59:57,250
accuracy and lo and behold

1351
00:59:57,250 --> 00:59:59,290
we're now at ninety nine point six five

1352
00:59:59,290 --> 01:00:01,960
percent which is kind of crazy where's

1353
01:00:01,960 --> 01:00:07,420
our green box but for every park we are

1354
01:00:07,420 --> 01:00:11,530
only showing one type of augmentation or

1355
01:00:11,530 --> 01:00:13,840
for particular image right so when we

1356
01:00:13,840 --> 01:00:16,540
are training back here we're not doing

1357
01:00:16,540 --> 01:00:20,170
any TTA right so TTA is not like you

1358
01:00:20,170 --> 01:00:22,660
could and sometimes like I've written

1359
01:00:22,660 --> 01:00:24,730
libraries where after a cheap up I run

1360
01:00:24,730 --> 01:00:26,680
TTA to see how well it's going but

1361
01:00:26,680 --> 01:00:27,760
that's not what's happening here I

1362
01:00:27,760 --> 01:00:31,030
trained the whole thing with training

1363
01:00:31,030 --> 01:00:33,190
time organization which doesn't have a

1364
01:00:33,190 --> 01:00:34,510
special name because that's what we mean

1365
01:00:34,510 --> 01:00:36,070
when we say data augmentation we need

1366
01:00:36,070 --> 01:00:38,230
training time augmentation so here every

1367
01:00:38,230 --> 01:00:39,850
time we showed a picture

1368
01:00:39,850 --> 01:00:41,800
we were randomly changing it a little

1369
01:00:41,800 --> 01:00:43,900
bit so each epoch each of these seven

1370
01:00:43,900 --> 01:00:45,520
epochs it was seen slightly different

1371
01:00:45,520 --> 01:00:47,830
versions of the picture having done that

1372
01:00:47,830 --> 01:00:50,470
we now have a fully trained model we

1373
01:00:50,470 --> 01:00:52,180
then said okay let's look at the

1374
01:00:52,180 --> 01:00:54,400
validation set so TTA by default uses

1375
01:00:54,400 --> 01:00:56,350
the validation set and said okay what

1376
01:00:56,350 --> 01:00:57,670
are your predictions of which ones are

1377
01:00:57,670 --> 01:00:59,590
cats and which ones are dogs and it did

1378
01:00:59,590 --> 01:01:02,410
4 predictions with different random

1379
01:01:02,410 --> 01:01:04,540
orientations plus one on the organ under

1380
01:01:04,540 --> 01:01:06,100
Augmented version average them all

1381
01:01:06,100 --> 01:01:07,870
together and that's what we got and

1382
01:01:07,870 --> 01:01:10,200
that's what we can't clear the accurate

1383
01:01:10,200 --> 01:01:12,500
so is there a high probability of having

1384
01:01:12,500 --> 01:01:15,359
sample in TTA that was not shown in

1385
01:01:15,359 --> 01:01:19,619
doing trained yeah actually every data

1386
01:01:19,619 --> 01:01:22,800
augmented for image is is unique because

1387
01:01:22,800 --> 01:01:24,869
the rotation could be like point zero

1388
01:01:24,869 --> 01:01:28,380
three four degrees and zoom could be 1.0

1389
01:01:28,380 --> 01:01:30,780
one sixty five so every time it's

1390
01:01:30,780 --> 01:01:32,970
slightly different no problem

1391
01:01:32,970 --> 01:01:39,480
was behind you what's your might not use

1392
01:01:39,480 --> 01:01:40,980
white padding or something like that

1393
01:01:40,980 --> 01:01:44,369
just one of your white padding like just

1394
01:01:44,369 --> 01:01:46,170
you know put like a white water around

1395
01:01:46,170 --> 01:01:48,990
oh padding's not yes so like there's

1396
01:01:48,990 --> 01:01:50,220
lots of different types of a better

1397
01:01:50,220 --> 01:01:52,260
orientation you can do and so one of the

1398
01:01:52,260 --> 01:01:54,300
things you can do is to add a border

1399
01:01:54,300 --> 01:01:57,150
around it basically adding a border

1400
01:01:57,150 --> 01:01:58,859
around it in my experiments doesn't

1401
01:01:58,859 --> 01:02:00,980
doesn't help it doesn't make it any less

1402
01:02:00,980 --> 01:02:03,660
cat-like it's not the convolutional

1403
01:02:03,660 --> 01:02:04,830
neural network doesn't seem to find it

1404
01:02:04,830 --> 01:02:07,290
very interesting basically something

1405
01:02:07,290 --> 01:02:08,910
that I do do we'll see later is I do

1406
01:02:08,910 --> 01:02:10,380
something called reflection padding

1407
01:02:10,380 --> 01:02:12,420
which is where I add some borders that

1408
01:02:12,420 --> 01:02:15,060
are the outside just reflected it's a

1409
01:02:15,060 --> 01:02:16,830
way to kind of make some bigger images

1410
01:02:16,830 --> 01:02:18,810
works well with satellite imagery in

1411
01:02:18,810 --> 01:02:21,599
particular but yeah in general I don't

1412
01:02:21,599 --> 01:02:23,220
do I have a lot of padding instead I do

1413
01:02:23,220 --> 01:02:28,380
a bit of zooming

1414
01:02:28,380 --> 01:02:30,120
it's kind of follow-up to that last one

1415
01:02:30,120 --> 01:02:33,600
but rather than cropping just at white

1416
01:02:33,600 --> 01:02:35,550
space because when you crop you lose the

1417
01:02:35,550 --> 01:02:37,800
dog's face but if you added white space

1418
01:02:37,800 --> 01:02:40,710
you wouldn't yeah so that's that's where

1419
01:02:40,710 --> 01:02:43,260
the kind of the reflection padding or

1420
01:02:43,260 --> 01:02:44,910
the zooming or whatever can help so

1421
01:02:44,910 --> 01:02:46,200
there are ways in the faster you know

1422
01:02:46,200 --> 01:02:48,090
library when you do custom transforms of

1423
01:02:48,090 --> 01:02:58,620
of making that happen I find that it

1424
01:02:58,620 --> 01:03:00,720
kind of depends on the image size you

1425
01:03:00,720 --> 01:03:03,330
know but generally speaking it seems

1426
01:03:03,330 --> 01:03:06,420
that using TTA plus data augmentation

1427
01:03:06,420 --> 01:03:08,430
the best thing to do is to try to use

1428
01:03:08,430 --> 01:03:10,440
this larger image as possible and so if

1429
01:03:10,440 --> 01:03:11,880
you kind of crop the thing down and put

1430
01:03:11,880 --> 01:03:13,860
white borders on top and bottom it's now

1431
01:03:13,860 --> 01:03:16,710
quite a lot smaller and so to make it as

1432
01:03:16,710 --> 01:03:18,480
big as it was before you now have to use

1433
01:03:18,480 --> 01:03:20,130
more GPU and if you're going to use more

1434
01:03:20,130 --> 01:03:21,480
that multi figure you could have zoomed

1435
01:03:21,480 --> 01:03:24,030
in and used a bigger image so in my

1436
01:03:24,030 --> 01:03:25,740
playing around that doesn't seem to be

1437
01:03:25,740 --> 01:03:36,060
generally as successful there is a

1438
01:03:36,060 --> 01:03:38,460
little interest on the topic of how do

1439
01:03:38,460 --> 01:03:41,690
the domain tation in older than images

1440
01:03:41,690 --> 01:03:47,940
indeed at least not images um yeah um no

1441
01:03:47,940 --> 01:03:53,130
one seems to know I actually um I asked

1442
01:03:53,130 --> 01:03:54,210
some of my friends in the natural

1443
01:03:54,210 --> 01:03:55,710
language processing community about this

1444
01:03:55,710 --> 01:03:57,150
we'll get to natural language processing

1445
01:03:57,150 --> 01:03:59,910
in a couple of lessons you know it seems

1446
01:03:59,910 --> 01:04:01,200
like it'd be really helpful there's been

1447
01:04:01,200 --> 01:04:04,260
a few example I carry very few number

1448
01:04:04,260 --> 01:04:06,030
examples of people where papers would

1449
01:04:06,030 --> 01:04:08,490
like try replacing synonyms for instance

1450
01:04:08,490 --> 01:04:10,800
but on the whole and understanding of

1451
01:04:10,800 --> 01:04:12,870
like appropriate data augmentation for

1452
01:04:12,870 --> 01:04:16,830
non image domains is under-researched in

1453
01:04:16,830 --> 01:04:24,450
under under developed the question was

1454
01:04:24,450 --> 01:04:26,220
could couldn't we just use a sliding

1455
01:04:26,220 --> 01:04:28,800
window to generate on the images so in

1456
01:04:28,800 --> 01:04:30,960
that dog thank you couldn't we generate

1457
01:04:30,960 --> 01:04:33,840
three parts of it wouldn't that be

1458
01:04:33,840 --> 01:04:34,080
better

1459
01:04:34,080 --> 01:04:37,560
yeah PTI you mean just just in general

1460
01:04:37,560 --> 01:04:40,349
when you're creating your so

1461
01:04:40,349 --> 01:04:42,180
training time I would say no that

1462
01:04:42,180 --> 01:04:43,680
wouldn't be better because we're not

1463
01:04:43,680 --> 01:04:45,690
gonna get as much variation you know we

1464
01:04:45,690 --> 01:04:48,210
want to have it like like one degree off

1465
01:04:48,210 --> 01:04:49,950
five you know five degrees off ten

1466
01:04:49,950 --> 01:04:51,420
pixels up like lots of slightly

1467
01:04:51,420 --> 01:04:52,979
different versions and so if you just

1468
01:04:52,979 --> 01:04:56,549
have three standard ways then you're not

1469
01:04:56,549 --> 01:04:58,380
giving it as many different ways of

1470
01:04:58,380 --> 01:05:00,569
looking at the data for testing

1471
01:05:00,569 --> 01:05:03,900
augmentation having fixed cropped

1472
01:05:03,900 --> 01:05:07,079
locations I think probably would be

1473
01:05:07,079 --> 01:05:09,809
better and I just haven't gotten around

1474
01:05:09,809 --> 01:05:12,210
to writing that yet I have a version in

1475
01:05:12,210 --> 01:05:13,979
an old library I think having fixed

1476
01:05:13,979 --> 01:05:19,200
cropped locations plus random contrast

1477
01:05:19,200 --> 01:05:21,239
brightness rotation changes might be

1478
01:05:21,239 --> 01:05:23,119
better

1479
01:05:23,119 --> 01:05:25,319
the reason I've got around to it yet is

1480
01:05:25,319 --> 01:05:27,630
because in my testing it didn't seem to

1481
01:05:27,630 --> 01:05:29,400
help him practice very much and it made

1482
01:05:29,400 --> 01:05:31,289
the code a lot more complicated so you

1483
01:05:31,289 --> 01:05:32,460
know it's kind of it's an interesting

1484
01:05:32,460 --> 01:05:34,490
question

1485
01:05:34,490 --> 01:05:39,170
I just wanted all of this last AI api's

1486
01:05:39,170 --> 01:05:44,240
that you are using is it yeah that's a

1487
01:05:44,240 --> 01:05:45,290
great question

1488
01:05:45,290 --> 01:05:47,300
so the faster you go libraries open

1489
01:05:47,300 --> 01:05:48,829
source and let's talk about it a bit

1490
01:05:48,829 --> 01:05:52,430
more generally because you know it's

1491
01:05:52,430 --> 01:05:55,069
like the fact that the fact that we're

1492
01:05:55,069 --> 01:05:56,390
using this library is kind of

1493
01:05:56,390 --> 01:05:59,329
interesting and unusual and it sits on

1494
01:05:59,329 --> 01:06:02,440
top of something called a torch right so

1495
01:06:02,440 --> 01:06:08,380
pi torch is a fairly recent development

1496
01:06:08,380 --> 01:06:11,050
and it's kind of I've noticed all the

1497
01:06:11,050 --> 01:06:13,430
researchers that I respect pretty much

1498
01:06:13,430 --> 01:06:16,940
are now using high torch I found in part

1499
01:06:16,940 --> 01:06:18,950
two of last year's course that a lot of

1500
01:06:18,950 --> 01:06:20,510
the cutting-edge stuff I wanted to teach

1501
01:06:20,510 --> 01:06:23,809
I couldn't do it in chaos and tensorflow

1502
01:06:23,809 --> 01:06:26,540
which is what we used to teach with and

1503
01:06:26,540 --> 01:06:29,030
so I had to switch the course to pay

1504
01:06:29,030 --> 01:06:31,700
torch halfway through part two the

1505
01:06:31,700 --> 01:06:35,240
problem was that PI torch isn't very

1506
01:06:35,240 --> 01:06:37,220
easy to use you have to write your own

1507
01:06:37,220 --> 01:06:38,809
training loop from scratch I basically

1508
01:06:38,809 --> 01:06:40,250
write everything from scratch or the

1509
01:06:40,250 --> 01:06:41,839
stuff you see inside the class they are

1510
01:06:41,839 --> 01:06:43,520
library we would have had to written it

1511
01:06:43,520 --> 01:06:47,780
you know to learn and so it really makes

1512
01:06:47,780 --> 01:06:49,309
it very hard to learn deep learning when

1513
01:06:49,309 --> 01:06:50,660
you have to write hundreds of lines of

1514
01:06:50,660 --> 01:06:55,069
code to do anything so so we decided to

1515
01:06:55,069 --> 01:06:57,109
create a library on top of Pi torch

1516
01:06:57,109 --> 01:07:00,950
because we you know this our mission is

1517
01:07:00,950 --> 01:07:03,170
to teach world class big morning so we

1518
01:07:03,170 --> 01:07:04,309
wanted to show you like here's how you

1519
01:07:04,309 --> 01:07:06,369
can be the best in the world at doing it

1520
01:07:06,369 --> 01:07:09,589
and we found that a lot of the world

1521
01:07:09,589 --> 01:07:12,020
class stuff we needed to show really

1522
01:07:12,020 --> 01:07:13,790
needed PI torch or at least with PI

1523
01:07:13,790 --> 01:07:16,730
torch it was far easier and but then PI

1524
01:07:16,730 --> 01:07:19,119
thought itself just wasn't suitable as a

1525
01:07:19,119 --> 01:07:23,390
first thing to teach with for new for

1526
01:07:23,390 --> 01:07:25,970
new deep learning practitioners so we

1527
01:07:25,970 --> 01:07:28,839
built this library on up of PI torch

1528
01:07:28,839 --> 01:07:31,220
initially heavily influenced by chaos

1529
01:07:31,220 --> 01:07:32,990
which is what we taught last year and

1530
01:07:32,990 --> 01:07:34,670
but then we realized we could actually

1531
01:07:34,670 --> 01:07:36,530
make things much much much easier than

1532
01:07:36,530 --> 01:07:38,660
care us so in care us if you look back

1533
01:07:38,660 --> 01:07:41,059
at last year's course notes you'll find

1534
01:07:41,059 --> 01:07:43,490
that all of the code is two to three

1535
01:07:43,490 --> 01:07:45,920
times longer and there's lots more

1536
01:07:45,920 --> 01:07:47,599
opportunities for

1537
01:07:47,599 --> 01:07:48,890
stakes because there's just a lot of

1538
01:07:48,890 --> 01:07:52,339
things you have to get right so we ended

1539
01:07:52,339 --> 01:07:54,440
up kind of building this this this

1540
01:07:54,440 --> 01:07:57,680
library in order to make it easier to

1541
01:07:57,680 --> 01:08:00,289
get into deep learning but also easier

1542
01:08:00,289 --> 01:08:03,229
to get state-of-the-art results and then

1543
01:08:03,229 --> 01:08:04,729
over the last year as we started

1544
01:08:04,729 --> 01:08:06,200
developing on top of that we started

1545
01:08:06,200 --> 01:08:09,849
discovering that by using this library

1546
01:08:09,849 --> 01:08:12,440
it made us so much more productive that

1547
01:08:12,440 --> 01:08:14,479
we actually started kind of developing

1548
01:08:14,479 --> 01:08:15,920
you state-of-the-art results and new

1549
01:08:15,920 --> 01:08:17,779
methods ourselves and we started

1550
01:08:17,779 --> 01:08:19,520
realizing that there's a whole bunch of

1551
01:08:19,520 --> 01:08:21,080
like papers that have kind of been

1552
01:08:21,080 --> 01:08:23,839
ignored or lost which when you use them

1553
01:08:23,839 --> 01:08:26,270
it could like automate or semi-automated

1554
01:08:26,270 --> 01:08:28,460
stuff like learning read finder that's

1555
01:08:28,460 --> 01:08:31,909
not in any other library so so it kind

1556
01:08:31,909 --> 01:08:33,440
of got to the point where now not only

1557
01:08:33,440 --> 01:08:36,469
is kind of fast AI lets us do things

1558
01:08:36,469 --> 01:08:38,839
easier much easier than any other

1559
01:08:38,839 --> 01:08:41,359
approach but at the same time it

1560
01:08:41,359 --> 01:08:44,989
actually has a lot more kind of

1561
01:08:44,989 --> 01:08:46,339
sophisticated stuff behind the scenes

1562
01:08:46,339 --> 01:08:48,859
than anything else so so it's kind of an

1563
01:08:48,859 --> 01:08:52,880
interesting mix so yeah so we've

1564
01:08:52,880 --> 01:08:54,980
released this library like at this stage

1565
01:08:54,980 --> 01:08:57,500
it's like very early version and so

1566
01:08:57,500 --> 01:08:58,790
through this course by the end of this

1567
01:08:58,790 --> 01:09:00,739
course I hope as a group you know we

1568
01:09:00,739 --> 01:09:02,929
will all a lot of people are already

1569
01:09:02,929 --> 01:09:04,460
helping have developed it into something

1570
01:09:04,460 --> 01:09:08,000
that's you know really pretty stable and

1571
01:09:08,000 --> 01:09:12,290
rock-solid and yeah anybody can then can

1572
01:09:12,290 --> 01:09:16,489
use it to build your own models under an

1573
01:09:16,489 --> 01:09:18,409
open-source license as you can see it's

1574
01:09:18,409 --> 01:09:23,989
available on github behind the scenes

1575
01:09:23,989 --> 01:09:26,900
it's it's creating play torch models and

1576
01:09:26,900 --> 01:09:29,690
so apply torch models can then be

1577
01:09:29,690 --> 01:09:33,650
exported into various different formats

1578
01:09:33,650 --> 01:09:35,449
having said that like a lot of folks

1579
01:09:35,449 --> 01:09:36,710
like issue if you want to do something

1580
01:09:36,710 --> 01:09:38,839
on a mobile phone for example you're

1581
01:09:38,839 --> 01:09:40,540
probably going to need to use tensorflow

1582
01:09:40,540 --> 01:09:44,839
and so later on in this course we're

1583
01:09:44,839 --> 01:09:46,159
going to show like how some of the

1584
01:09:46,159 --> 01:09:47,449
things that we're doing in the past AI

1585
01:09:47,449 --> 01:09:50,210
library you can do in chaos and cancel

1586
01:09:50,210 --> 01:09:51,799
flow so you can going to get a sense of

1587
01:09:51,799 --> 01:09:53,089
what the different libraries look like

1588
01:09:53,089 --> 01:09:56,500
and generally speaking the simple stuff

1589
01:09:56,500 --> 01:09:59,960
is like it'll take you a small number of

1590
01:09:59,960 --> 01:10:00,860
days

1591
01:10:00,860 --> 01:10:02,690
to learn to do it and care us in

1592
01:10:02,690 --> 01:10:04,790
tensorflow versus fast AI and high torch

1593
01:10:04,790 --> 01:10:08,090
and the more complex stuff often this

1594
01:10:08,090 --> 01:10:09,739
won't be possible so that like if you

1595
01:10:09,739 --> 01:10:12,170
needed to be intensive flow you're just

1596
01:10:12,170 --> 01:10:17,560
kind of simplify it off in a little bit

1597
01:10:17,560 --> 01:10:19,850
but you know I think the more important

1598
01:10:19,850 --> 01:10:24,830
thing to realize is every year the kind

1599
01:10:24,830 --> 01:10:26,810
of the libraries that are available and

1600
01:10:26,810 --> 01:10:28,460
which ones are the best totally changes

1601
01:10:28,460 --> 01:10:30,139
so like the main thing I hope that you

1602
01:10:30,139 --> 01:10:31,190
get out of this course is an

1603
01:10:31,190 --> 01:10:33,110
understanding of the concepts like

1604
01:10:33,110 --> 01:10:34,880
here's how you find a learning rate

1605
01:10:34,880 --> 01:10:36,350
here's why differential learning rates

1606
01:10:36,350 --> 01:10:38,030
are important is they do learn where the

1607
01:10:38,030 --> 01:10:40,850
kneeling you know here's what stochastic

1608
01:10:40,850 --> 01:10:42,409
gradient a second's restarts does so on

1609
01:10:42,409 --> 01:10:45,619
and so forth because you know by the

1610
01:10:45,619 --> 01:10:47,949
time we do this course again next year

1611
01:10:47,949 --> 01:10:52,400
you know the library situations and the

1612
01:10:52,400 --> 01:10:55,580
difference the king that's a question of

1613
01:10:55,580 --> 01:11:05,960
that I was wondering if you've had an

1614
01:11:05,960 --> 01:11:08,659
opinion on pyro which is ubers new

1615
01:11:08,659 --> 01:11:10,909
release I haven't looked at it no I'm

1616
01:11:10,909 --> 01:11:11,960
very interested in probabilistic

1617
01:11:11,960 --> 01:11:14,210
programming and it's really cool that's

1618
01:11:14,210 --> 01:11:15,860
built on top of paper so one of the

1619
01:11:15,860 --> 01:11:17,090
things we'll learn about in this course

1620
01:11:17,090 --> 01:11:19,070
is we'll see that PI torch is much more

1621
01:11:19,070 --> 01:11:20,810
than just a deep learning library it

1622
01:11:20,810 --> 01:11:24,190
actually lets us write arbitrary

1623
01:11:24,190 --> 01:11:28,369
gpu-accelerated algorithms from scratch

1624
01:11:28,369 --> 01:11:29,719
which we're actually going to do and

1625
01:11:29,719 --> 01:11:31,369
pyro is a great example of what people

1626
01:11:31,369 --> 01:11:33,260
are now doing with might watch outside

1627
01:11:33,260 --> 01:11:38,199
of the deep level great ok let's take a

1628
01:11:38,199 --> 01:11:40,190
eight-minute break and we'll come back

1629
01:11:40,190 --> 01:11:52,520
at 7:55 so ninety nine point six five

1630
01:11:52,520 --> 01:11:56,119
percent accuracy what does that mean so

1631
01:11:56,119 --> 01:11:59,060
in classification when we do

1632
01:11:59,060 --> 01:12:01,130
classification and machine learning the

1633
01:12:01,130 --> 01:12:04,130
really simple way to look at the result

1634
01:12:04,130 --> 01:12:05,750
of a classification is what's called the

1635
01:12:05,750 --> 01:12:07,850
confusion matrix this is not just deep

1636
01:12:07,850 --> 01:12:09,530
learning but in any kind of classifier

1637
01:12:09,530 --> 01:12:12,110
machine learning where we say okay what

1638
01:12:12,110 --> 01:12:14,690
was the actual truth there were

1639
01:12:14,690 --> 01:12:17,960
thousand cats and a thousand dogs out of

1640
01:12:17,960 --> 01:12:20,449
the thousand actual cats how many did we

1641
01:12:20,449 --> 01:12:22,969
predict were cats this is obviously in

1642
01:12:22,969 --> 01:12:25,219
the validation step this is the images

1643
01:12:25,219 --> 01:12:27,320
that we didn't use to train with it

1644
01:12:27,320 --> 01:12:28,070
turns out there were nine hundred

1645
01:12:28,070 --> 01:12:29,989
ninety-eight cats that we actually

1646
01:12:29,989 --> 01:12:32,210
predicted as cats and two that we got

1647
01:12:32,210 --> 01:12:32,660
wrong

1648
01:12:32,660 --> 01:12:35,180
okay and then for dogs there were nine

1649
01:12:35,180 --> 01:12:36,440
hundred ninety-five that we predicted

1650
01:12:36,440 --> 01:12:38,150
were dogs and then five that we got

1651
01:12:38,150 --> 01:12:41,210
wrong and so often these confusion

1652
01:12:41,210 --> 01:12:42,980
matrices can be helpful particularly if

1653
01:12:42,980 --> 01:12:44,690
you've got like four or five classes

1654
01:12:44,690 --> 01:12:46,219
you're trying to predict to see like

1655
01:12:46,219 --> 01:12:48,199
which group you having the most trouble

1656
01:12:48,199 --> 01:12:49,880
with and you can see it uses color

1657
01:12:49,880 --> 01:12:52,910
coding to tell you you know to highlight

1658
01:12:52,910 --> 01:12:55,070
the large the large bits you've got to

1659
01:12:55,070 --> 01:12:57,980
hope that the diagonal is the

1660
01:12:57,980 --> 01:13:01,070
highlighted section so now that we've

1661
01:13:01,070 --> 01:13:02,510
retrained the model it can be quite

1662
01:13:02,510 --> 01:13:04,699
helpful now that's better to actually

1663
01:13:04,699 --> 01:13:07,130
look back and see like okay which ones

1664
01:13:07,130 --> 01:13:09,469
in particular were incorrect and we can

1665
01:13:09,469 --> 01:13:12,380
see here there were actually only two

1666
01:13:12,380 --> 01:13:15,260
incorrect cats it prints out four by

1667
01:13:15,260 --> 01:13:16,580
default so you can actually see these

1668
01:13:16,580 --> 01:13:19,190
two actually less than 0.5 so they

1669
01:13:19,190 --> 01:13:21,320
weren't they weren't wrong okay so it's

1670
01:13:21,320 --> 01:13:23,900
actually these two were wrong cats and

1671
01:13:23,900 --> 01:13:26,080
this one isn't obviously a cat at all

1672
01:13:26,080 --> 01:13:28,580
this one is but it looks like it's got a

1673
01:13:28,580 --> 01:13:31,370
lot of weird artifacts and you can't see

1674
01:13:31,370 --> 01:13:34,190
its eyeballs at all so and then here are

1675
01:13:34,190 --> 01:13:36,469
the how many dogs where they're all

1676
01:13:36,469 --> 01:13:38,660
wrong there were five wrong dogs here

1677
01:13:38,660 --> 01:13:40,610
are four of them that's not obviously a

1678
01:13:40,610 --> 01:13:44,120
dog that looks like a mistake that looks

1679
01:13:44,120 --> 01:13:46,400
like a mistake that one I guess doesn't

1680
01:13:46,400 --> 01:13:47,870
have enough information that I guess

1681
01:13:47,870 --> 01:13:51,350
it's a mistake so so we've done a pretty

1682
01:13:51,350 --> 01:13:53,989
good job here of creating a good

1683
01:13:53,989 --> 01:13:57,890
classifier I would based on entering a

1684
01:13:57,890 --> 01:13:59,150
lot of capital competitions and

1685
01:13:59,150 --> 01:14:00,860
comparing results I've done two various

1686
01:14:00,860 --> 01:14:02,870
research papers I can tell you it's a

1687
01:14:02,870 --> 01:14:05,210
state of the art classifier it's it's

1688
01:14:05,210 --> 01:14:06,320
right up there with the best in the

1689
01:14:06,320 --> 01:14:08,090
world we're going to make it a little

1690
01:14:08,090 --> 01:14:09,890
bit better in a moment but here in the

1691
01:14:09,890 --> 01:14:11,989
basic steps right so if you want to

1692
01:14:11,989 --> 01:14:14,270
create a world-class image classifier

1693
01:14:14,270 --> 01:14:16,280
the steps that we just went through was

1694
01:14:16,280 --> 01:14:18,320
that we started our week's term data

1695
01:14:18,320 --> 01:14:21,050
augmentation on by saying oil transforms

1696
01:14:21,050 --> 01:14:22,790
equals and you either say sidon or

1697
01:14:22,790 --> 01:14:24,380
top-down depending on what you're doing

1698
01:14:24,380 --> 01:14:27,679
start with pre compute equals true find

1699
01:14:27,679 --> 01:14:28,550
a decent learning

1700
01:14:28,550 --> 01:14:31,010
eight we then train just like it one or

1701
01:14:31,010 --> 01:14:33,080
two epochs which that takes a few

1702
01:14:33,080 --> 01:14:34,820
seconds as we got through compute equals

1703
01:14:34,820 --> 01:14:37,670
true then we turn off pre compute which

1704
01:14:37,670 --> 01:14:40,430
allows us to use data augmentation to do

1705
01:14:40,430 --> 01:14:43,220
another two or three epochs generally

1706
01:14:43,220 --> 01:14:45,710
with cycle length equals one then I

1707
01:14:45,710 --> 01:14:48,260
unfreeze all them as I then set the

1708
01:14:48,260 --> 01:14:50,360
earlier layers to be like either

1709
01:14:50,360 --> 01:14:52,640
somewhere between a 3 times 2 10 times

1710
01:14:52,640 --> 01:14:54,830
mobile learning rate in the previous so

1711
01:14:54,830 --> 01:15:01,760
in this case I did 10 times right so

1712
01:15:01,760 --> 01:15:03,230
it's like this was my learning rate that

1713
01:15:03,230 --> 01:15:04,250
I found from the learning rate finer

1714
01:15:04,250 --> 01:15:06,140
than I went 10 times smaller and then 10

1715
01:15:06,140 --> 01:15:08,770
times smaller as a rule of thumb like

1716
01:15:08,770 --> 01:15:11,090
knowing that you're starting with a pre

1717
01:15:11,090 --> 01:15:13,640
trained imagenet model if you know if

1718
01:15:13,640 --> 01:15:15,050
you can see that the things that you're

1719
01:15:15,050 --> 01:15:17,030
now trying to classify a pretty similar

1720
01:15:17,030 --> 01:15:18,740
the kinds of things in imagenet ie

1721
01:15:18,740 --> 01:15:21,230
pictures of normal objects in normal

1722
01:15:21,230 --> 01:15:21,920
environments

1723
01:15:21,920 --> 01:15:24,350
you probably want about a 10x difference

1724
01:15:24,350 --> 01:15:26,150
because you want those earlier layers

1725
01:15:26,150 --> 01:15:27,920
like you think that the earlier layers

1726
01:15:27,920 --> 01:15:30,410
are probably very good already but also

1727
01:15:30,410 --> 01:15:31,700
if you're doing something like satellite

1728
01:15:31,700 --> 01:15:34,370
imagery or medical imaging which is not

1729
01:15:34,370 --> 01:15:36,080
at all like image net then you probably

1730
01:15:36,080 --> 01:15:37,940
want to be training those earlier layers

1731
01:15:37,940 --> 01:15:39,440
a lot more so you might have like oh

1732
01:15:39,440 --> 01:15:42,380
just a 3/8 difference all right so

1733
01:15:42,380 --> 01:15:45,770
that's like one change that I make is to

1734
01:15:45,770 --> 01:15:52,220
try to make it out of 10x or 3x yes so

1735
01:15:52,220 --> 01:15:56,150
then after unfreezing you can now call

1736
01:15:56,150 --> 01:15:59,480
LR find again but at Nike didn't in this

1737
01:15:59,480 --> 01:16:01,490
case but like once you've unfrozen all

1738
01:16:01,490 --> 01:16:03,230
the layers you've turned on differential

1739
01:16:03,230 --> 01:16:05,390
learning rates you can then call a lot

1740
01:16:05,390 --> 01:16:10,310
of fine again right and so you can then

1741
01:16:10,310 --> 01:16:12,050
check like oh does it still look like

1742
01:16:12,050 --> 01:16:13,970
the same point I had last time is about

1743
01:16:13,970 --> 01:16:16,460
right something to note is that if you

1744
01:16:16,460 --> 01:16:20,150
call LR find having set differential

1745
01:16:20,150 --> 01:16:21,950
learning rates the thing that's actually

1746
01:16:21,950 --> 01:16:23,780
going to print out is the learning rate

1747
01:16:23,780 --> 01:16:25,820
of the last layers right because you've

1748
01:16:25,820 --> 01:16:27,200
got three different learning rates so

1749
01:16:27,200 --> 01:16:28,690
it's actually showing you the last layer

1750
01:16:28,690 --> 01:16:31,130
so then yeah then I trained the full

1751
01:16:31,130 --> 01:16:33,290
network with cycle more equals two and

1752
01:16:33,290 --> 01:16:35,180
it'll either it starts with the fitting

1753
01:16:35,180 --> 01:16:38,480
or I run out of time right so like let

1754
01:16:38,480 --> 01:16:40,580
me show you all right so let's do this

1755
01:16:40,580 --> 01:16:41,640
again

1756
01:16:41,640 --> 01:16:43,020
a totally different data set so this

1757
01:16:43,020 --> 01:16:45,510
morning I noticed that some of you on

1758
01:16:45,510 --> 01:16:47,100
the forums were playing around with this

1759
01:16:47,100 --> 01:16:49,860
playground Kegel competition very

1760
01:16:49,860 --> 01:16:53,180
similar called dog breed identification

1761
01:16:53,180 --> 01:16:56,490
so the dog breed identification cat will

1762
01:16:56,490 --> 01:16:59,910
challenge is one where you don't

1763
01:16:59,910 --> 01:17:01,500
actually have to decide which ones are

1764
01:17:01,500 --> 01:17:02,760
cats and which ones the dogs they're all

1765
01:17:02,760 --> 01:17:05,190
dogs but you have to decide what kind of

1766
01:17:05,190 --> 01:17:07,380
dog it is but there are 120 different

1767
01:17:07,380 --> 01:17:10,020
breeds of dogs okay

1768
01:17:10,020 --> 01:17:14,030
so you know obviously this could be like

1769
01:17:14,030 --> 01:17:18,600
different types of cells and pathology

1770
01:17:18,600 --> 01:17:20,160
slides it could be different kinds of

1771
01:17:20,160 --> 01:17:23,600
cancers in CT scans it could be

1772
01:17:23,600 --> 01:17:25,830
different kinds of icebergs and

1773
01:17:25,830 --> 01:17:28,050
satellite images whatever right as long

1774
01:17:28,050 --> 01:17:29,460
as you've got some kind of labeled

1775
01:17:29,460 --> 01:17:33,390
images so I want to show you what I did

1776
01:17:33,390 --> 01:17:34,640
this morning so it took me about an hour

1777
01:17:34,640 --> 01:17:38,610
basically to go in to end from something

1778
01:17:38,610 --> 01:17:42,150
I'd never seen before so I downloaded

1779
01:17:42,150 --> 01:17:43,770
the data from kaggle and I'll show you

1780
01:17:43,770 --> 01:17:45,420
how to do that shortly but the short

1781
01:17:45,420 --> 01:17:47,190
answer is there's something called cable

1782
01:17:47,190 --> 01:17:49,680
CLI which is a github project you can

1783
01:17:49,680 --> 01:17:51,870
search for and if you read the docs to

1784
01:17:51,870 --> 01:17:54,150
basically run cagey download provide the

1785
01:17:54,150 --> 01:17:55,680
competition name and it will grab all

1786
01:17:55,680 --> 01:17:57,690
the data for you to your crystal or

1787
01:17:57,690 --> 01:18:01,380
Amazon or whatever instance I put in my

1788
01:18:01,380 --> 01:18:07,530
data folder and I then went LS and I saw

1789
01:18:07,530 --> 01:18:12,060
that it's a little bit different to our

1790
01:18:12,060 --> 01:18:14,880
previous data set it's not that there's

1791
01:18:14,880 --> 01:18:17,670
a train folder which has a separate

1792
01:18:17,670 --> 01:18:20,130
folder for each kind of dog but instead

1793
01:18:20,130 --> 01:18:22,680
of tonette there was a CSV file and the

1794
01:18:22,680 --> 01:18:25,590
CSV file I read it in with pandas so

1795
01:18:25,590 --> 01:18:27,990
pandas is the thing we use in python to

1796
01:18:27,990 --> 01:18:30,210
do structured data analysis like csv

1797
01:18:30,210 --> 01:18:33,090
files so he picked pandas we called pd

1798
01:18:33,090 --> 01:18:36,030
that's pretty much universal PDR htsb

1799
01:18:36,030 --> 01:18:38,700
reads in the csv file we can then take a

1800
01:18:38,700 --> 01:18:39,840
look at it and you can see that

1801
01:18:39,840 --> 01:18:41,220
basically it had like some kind of

1802
01:18:41,220 --> 01:18:45,000
identifier and then the debris right so

1803
01:18:45,000 --> 01:18:46,980
this is like a different way this is the

1804
01:18:46,980 --> 01:18:49,860
second main way that people kind of give

1805
01:18:49,860 --> 01:18:51,660
you image labels one is to put different

1806
01:18:51,660 --> 01:18:53,160
images into different folders

1807
01:18:53,160 --> 01:18:55,349
the second is generally to give you as

1808
01:18:55,349 --> 01:18:57,840
some kind of file like a CSV file to

1809
01:18:57,840 --> 01:18:59,820
tell you here's the image name and

1810
01:18:59,820 --> 01:19:05,369
here's the label okay so what I then did

1811
01:19:05,369 --> 01:19:08,159
was I used pandas again to create a

1812
01:19:08,159 --> 01:19:10,019
pivot table which basically groups it up

1813
01:19:10,019 --> 01:19:13,289
just to see how many of each breed there

1814
01:19:13,289 --> 01:19:15,719
were and I sorted them and so I saw okay

1815
01:19:15,719 --> 01:19:18,840
they've got like about a hundred some of

1816
01:19:18,840 --> 01:19:21,449
the more common breeds and some of the

1817
01:19:21,449 --> 01:19:23,489
less common breeds they've got like 60

1818
01:19:23,489 --> 01:19:27,030
or so okay altogether there are 120 rows

1819
01:19:27,030 --> 01:19:28,679
and I've been 120 different breeds

1820
01:19:28,679 --> 01:19:32,399
represented okay so I'm going to go

1821
01:19:32,399 --> 01:19:35,879
through the steps right so enable data

1822
01:19:35,879 --> 01:19:37,889
augmentation so to enable data

1823
01:19:37,889 --> 01:19:39,329
augmentation when we call this

1824
01:19:39,329 --> 01:19:42,030
transforms from model you just pass in

1825
01:19:42,030 --> 01:19:44,219
and all transformers in this case I

1826
01:19:44,219 --> 01:19:46,109
chose side on again these are pictures

1827
01:19:46,109 --> 01:19:48,359
of dots and stuff so this side on photos

1828
01:19:48,359 --> 01:19:53,039
I we're talking about maqsuum as more

1829
01:19:53,039 --> 01:19:55,769
detail later but maximum basically says

1830
01:19:55,769 --> 01:19:58,139
when you do the data augmentation we

1831
01:19:58,139 --> 01:20:01,769
like zoom into it by up to one point one

1832
01:20:01,769 --> 01:20:04,979
times okay so but randomly between one

1833
01:20:04,979 --> 01:20:06,749
the original image size and one point

1834
01:20:06,749 --> 01:20:08,999
one points so it's not always cropping

1835
01:20:08,999 --> 01:20:11,070
out in the middle or an edge but it

1836
01:20:11,070 --> 01:20:12,389
could be cropping out a smaller part

1837
01:20:12,389 --> 01:20:16,439
okay so having done that the key step

1838
01:20:16,439 --> 01:20:18,769
now is to graphically going from paths

1839
01:20:18,769 --> 01:20:21,539
so previously we went from paths and

1840
01:20:21,539 --> 01:20:23,820
that tells it that the the names of the

1841
01:20:23,820 --> 01:20:25,679
folders are the names of the labels we

1842
01:20:25,679 --> 01:20:29,609
go from CSV and we pass in the CSV file

1843
01:20:29,609 --> 01:20:32,129
that contains the letters so we're

1844
01:20:32,129 --> 01:20:34,800
passing in the path that contains all of

1845
01:20:34,800 --> 01:20:37,050
the data the name of the folder that

1846
01:20:37,050 --> 01:20:40,139
contains the training data the CSV that

1847
01:20:40,139 --> 01:20:43,859
contains the labels we need to also tell

1848
01:20:43,859 --> 01:20:45,300
it where the test set is if you want to

1849
01:20:45,300 --> 01:20:47,099
submit to cattle later talk more about

1850
01:20:47,099 --> 01:20:53,010
that next week now this time the

1851
01:20:53,010 --> 01:20:55,229
previous data set we had had actually

1852
01:20:55,229 --> 01:20:57,449
separated a validation set out into a

1853
01:20:57,449 --> 01:20:59,789
separate folder right but in this case

1854
01:20:59,789 --> 01:21:02,459
you'll see that there is not a separate

1855
01:21:02,459 --> 01:21:05,999
folder called validation right so we

1856
01:21:05,999 --> 01:21:07,739
want to be able to track how good our

1857
01:21:07,739 --> 01:21:09,130
performance is low

1858
01:21:09,130 --> 01:21:10,570
so we're going to have to separate some

1859
01:21:10,570 --> 01:21:12,490
of the images out to put it into a

1860
01:21:12,490 --> 01:21:15,310
validation set okay so I do that at

1861
01:21:15,310 --> 01:21:18,340
random and so up here you can see how it

1862
01:21:18,340 --> 01:21:22,960
basically opened up the CSV file turned

1863
01:21:22,960 --> 01:21:25,840
it into a list of rows and then taken

1864
01:21:25,840 --> 01:21:28,180
the length of that minus one because

1865
01:21:28,180 --> 01:21:30,790
there's a header at the top right and so

1866
01:21:30,790 --> 01:21:33,250
that's the number of rows in the CSV

1867
01:21:33,250 --> 01:21:35,320
file which must be the number of images

1868
01:21:35,320 --> 01:21:37,900
that we have and then this is a fast AI

1869
01:21:37,900 --> 01:21:40,840
thing get cross-validation indexes now

1870
01:21:40,840 --> 01:21:42,310
we'll talk about cross-validation later

1871
01:21:42,310 --> 01:21:44,590
but basically if you call this and pass

1872
01:21:44,590 --> 01:21:48,090
in a number it's going to return to you

1873
01:21:48,090 --> 01:21:52,480
by default a random twenty percent of

1874
01:21:52,480 --> 01:21:54,790
the rows who uses your validation set

1875
01:21:54,790 --> 01:21:57,010
and you can pass in parameters to get

1876
01:21:57,010 --> 01:21:58,930
different amounts right so this is now

1877
01:21:58,930 --> 01:22:00,430
going to grab twenty percent of the data

1878
01:22:00,430 --> 01:22:03,610
and say all right this is the this is

1879
01:22:03,610 --> 01:22:05,890
the indexes the numbers of the files

1880
01:22:05,890 --> 01:22:07,690
which we're going to use as a validation

1881
01:22:07,690 --> 01:22:08,080
set

1882
01:22:08,080 --> 01:22:12,790
okay so now that we've got that in fact

1883
01:22:12,790 --> 01:22:14,530
let's kind of run this so you can see

1884
01:22:14,530 --> 01:22:19,980
what that looks like so well indexes is

1885
01:22:19,980 --> 01:22:23,590
just a big bunch of numbers okay and so

1886
01:22:23,590 --> 01:22:26,179
an is

1887
01:22:26,179 --> 01:22:31,909
10,000 right and so we have about twenty

1888
01:22:31,909 --> 01:22:33,800
percent of those is going to be in a

1889
01:22:33,800 --> 01:22:34,810
validation set

1890
01:22:34,810 --> 01:22:44,929
so when we call from CSV we can pass in

1891
01:22:44,929 --> 01:22:46,820
a parameter which is talent which

1892
01:22:46,820 --> 01:22:48,890
indexes to treat us a validation set and

1893
01:22:48,890 --> 01:22:52,850
so that's passed in those indexes one

1894
01:22:52,850 --> 01:22:54,739
thing that's a little bit tricky here is

1895
01:22:54,739 --> 01:23:03,590
that the file names actually have I

1896
01:23:03,590 --> 01:23:05,960
checked they actually have a dot jpg on

1897
01:23:05,960 --> 01:23:07,909
the end and these obviously don't have a

1898
01:23:07,909 --> 01:23:11,480
dot jpg so you can pass in when you call

1899
01:23:11,480 --> 01:23:13,850
from CSV you can pass in a suffix it

1900
01:23:13,850 --> 01:23:15,770
says that the labels don't actually

1901
01:23:15,770 --> 01:23:17,390
contain the full file names you need to

1902
01:23:17,390 --> 01:23:22,820
add this to them okay so that's

1903
01:23:22,820 --> 01:23:24,710
basically all I need to do to set up my

1904
01:23:24,710 --> 01:23:28,429
data and as a lot of Europe noticed

1905
01:23:28,429 --> 01:23:31,820
during the week inside that data object

1906
01:23:31,820 --> 01:23:33,890
you can actually get access to the data

1907
01:23:33,890 --> 01:23:35,690
set like what the training data set by

1908
01:23:35,690 --> 01:23:38,929
same train yes and inside train des is a

1909
01:23:38,929 --> 01:23:41,060
whole bunch of things including the file

1910
01:23:41,060 --> 01:23:41,510
names

1911
01:23:41,510 --> 01:23:43,580
okay so train desktop file names

1912
01:23:43,580 --> 01:23:45,140
contains all of the file names of

1913
01:23:45,140 --> 01:23:47,000
everything in the training set and so

1914
01:23:47,000 --> 01:23:48,800
here's like one file name

1915
01:23:48,800 --> 01:23:50,630
okay so here's an example of one file

1916
01:23:50,630 --> 01:23:54,140
name so I can now go ahead and open that

1917
01:23:54,140 --> 01:23:56,690
file and take a look at it that's the

1918
01:23:56,690 --> 01:23:58,040
next thing I did was to try and

1919
01:23:58,040 --> 01:23:59,780
understand what my file my dataset looks

1920
01:23:59,780 --> 01:24:02,600
like and it found an adorable puppy so

1921
01:24:02,600 --> 01:24:03,440
that was very nice

1922
01:24:03,440 --> 01:24:06,380
so feeling good about this I also want

1923
01:24:06,380 --> 01:24:08,150
to know like how big of these files

1924
01:24:08,150 --> 01:24:09,890
right like how big are the images

1925
01:24:09,890 --> 01:24:12,290
because that's a key issue if they're

1926
01:24:12,290 --> 01:24:13,909
huge and then I have to think really

1927
01:24:13,909 --> 01:24:15,560
carefully about how to deal with huge

1928
01:24:15,560 --> 01:24:17,719
images that's really challenging if

1929
01:24:17,719 --> 01:24:19,489
they're tiny well that's also

1930
01:24:19,489 --> 01:24:22,580
challenging most of imagenet models are

1931
01:24:22,580 --> 01:24:25,969
trained on either 224 by 224 or 299 by

1932
01:24:25,969 --> 01:24:28,909
299 images so anytime you have images in

1933
01:24:28,909 --> 01:24:31,370
that kind of range that's that's really

1934
01:24:31,370 --> 01:24:32,540
hopeful you're probably not going to

1935
01:24:32,540 --> 01:24:34,489
have to do too much different in this

1936
01:24:34,489 --> 01:24:36,350
case the first image I looked at was

1937
01:24:36,350 --> 01:24:38,369
about the right size so I'm thinking of

1938
01:24:38,369 --> 01:24:41,280
pretty hopeful so what I did then is I

1939
01:24:41,280 --> 01:24:42,989
created a dictionary comprehension now

1940
01:24:42,989 --> 01:24:44,219
if you don't know about list

1941
01:24:44,219 --> 01:24:45,599
comprehensions and dictionary

1942
01:24:45,599 --> 01:24:48,840
comprehensions in Python go study them

1943
01:24:48,840 --> 01:24:51,179
they're the most useful thing super

1944
01:24:51,179 --> 01:24:54,000
handy you can see the basic idea here is

1945
01:24:54,000 --> 01:24:55,260
that are going through all of the files

1946
01:24:55,260 --> 01:24:57,510
and then putting a dictionary that map's

1947
01:24:57,510 --> 01:25:01,770
the name of the file to the size of that

1948
01:25:01,770 --> 01:25:05,880
file again this is a handy little Python

1949
01:25:05,880 --> 01:25:07,500
feature which I'll let you think learn

1950
01:25:07,500 --> 01:25:08,639
about during the week if you don't know

1951
01:25:08,639 --> 01:25:10,650
about it which is zip and using a

1952
01:25:10,650 --> 01:25:12,719
special star notation is never to take

1953
01:25:12,719 --> 01:25:15,260
this dictionary and turn it into the

1954
01:25:15,260 --> 01:25:19,920
rows and the columns and so I can now

1955
01:25:19,920 --> 01:25:22,080
turn those into num pay arrays and like

1956
01:25:22,080 --> 01:25:26,219
okay here are the first five rows sizes

1957
01:25:26,219 --> 01:25:28,469
for each of my images and then

1958
01:25:28,469 --> 01:25:30,330
matplotlib is something you want to be

1959
01:25:30,330 --> 01:25:32,130
very familiar with if you do any kind of

1960
01:25:32,130 --> 01:25:33,480
data science or machine learning in

1961
01:25:33,480 --> 01:25:36,239
python matplotlib we always refer to as

1962
01:25:36,239 --> 01:25:39,780
PLT as if this is a histogram and so I

1963
01:25:39,780 --> 01:25:43,440
got a histogram of the how high how many

1964
01:25:43,440 --> 01:25:45,420
rows there are in each image so you can

1965
01:25:45,420 --> 01:25:46,739
see here I'm kind of getting a sense

1966
01:25:46,739 --> 01:25:49,020
before I start doing any modeling I kind

1967
01:25:49,020 --> 01:25:50,190
of need to know what I'm modeling with

1968
01:25:50,190 --> 01:25:52,170
and I can see some of the images are

1969
01:25:52,170 --> 01:25:54,690
going to be like 2500 3000 pixels high

1970
01:25:54,690 --> 01:25:57,560
but most of them seem to be around 500

1971
01:25:57,560 --> 01:26:00,270
so given it so few of them were bigger

1972
01:26:00,270 --> 01:26:03,409
than a thousand I use standard numpy

1973
01:26:03,409 --> 01:26:05,940
slicing to just grab those at a smaller

1974
01:26:05,940 --> 01:26:08,460
than a thousand and histogram that just

1975
01:26:08,460 --> 01:26:10,409
to zoom in a little bit and I can see

1976
01:26:10,409 --> 01:26:12,119
here all right it looks like yet the

1977
01:26:12,119 --> 01:26:15,090
vast majority are around 500 and so this

1978
01:26:15,090 --> 01:26:17,969
actually also prints out the histogram

1979
01:26:17,969 --> 01:26:19,320
so I can actually go through and I can

1980
01:26:19,320 --> 01:26:21,090
see here for four thousand five hundred

1981
01:26:21,090 --> 01:26:23,849
of them are about 450 okay so I get

1982
01:26:23,849 --> 01:26:28,440
about that seems about anywhere so

1983
01:26:28,440 --> 01:26:32,219
generally how many images should we get

1984
01:26:32,219 --> 01:26:38,679
in the validation set is always a 20%

1985
01:26:38,679 --> 01:26:44,409
so the size of the validation set like

1986
01:26:44,409 --> 01:26:48,019
using 20% is fine unless you kind of

1987
01:26:48,019 --> 01:26:49,849
feeling like my data is my data sets

1988
01:26:49,849 --> 01:26:53,530
really small I'm not sure that's enough

1989
01:26:53,530 --> 01:26:57,949
you know like if you've got basically

1990
01:26:57,949 --> 01:26:59,539
think of it this way if you train like

1991
01:26:59,539 --> 01:27:01,489
the same model multiple times and you're

1992
01:27:01,489 --> 01:27:02,959
getting very different validation set

1993
01:27:02,959 --> 01:27:04,849
results and your validation sets kind of

1994
01:27:04,849 --> 01:27:08,409
small but smaller than a thousand or so

1995
01:27:08,409 --> 01:27:10,369
then it's going to be quite hard to

1996
01:27:10,369 --> 01:27:13,070
interpret how well you're doing now this

1997
01:27:13,070 --> 01:27:14,959
is particularly true like if you're like

1998
01:27:14,959 --> 01:27:16,610
if you care about the third decimal

1999
01:27:16,610 --> 01:27:19,670
place of accuracy and you've got like a

2000
01:27:19,670 --> 01:27:21,349
thousand things in your validation set

2001
01:27:21,349 --> 01:27:22,849
then you bring about like a single image

2002
01:27:22,849 --> 01:27:25,489
changing class is changing you know it's

2003
01:27:25,489 --> 01:27:28,519
what you're looking at so it's it really

2004
01:27:28,519 --> 01:27:31,400
depends on my cow accurate you have much

2005
01:27:31,400 --> 01:27:35,389
difference you care about I would say in

2006
01:27:35,389 --> 01:27:37,969
general like at the point where you care

2007
01:27:37,969 --> 01:27:39,019
about difference between like out of

2008
01:27:39,019 --> 01:27:41,090
0.01 and 0.02 like the second decimal

2009
01:27:41,090 --> 01:27:44,510
place you want that to represent like 10

2010
01:27:44,510 --> 01:27:48,229
or 20 roads you know like changing the

2011
01:27:48,229 --> 01:27:51,619
class of that 10 or 20 rows then that's

2012
01:27:51,619 --> 01:27:53,440
something you can be pretty confident of

2013
01:27:53,440 --> 01:27:57,739
so like most of the time you know give

2014
01:27:57,739 --> 01:28:00,229
them the data sizes we normally have 20

2015
01:28:00,229 --> 01:28:04,099
percent seems to work fine but yeah it's

2016
01:28:04,099 --> 01:28:07,209
it's it's kind of a it depends a lot on

2017
01:28:07,209 --> 01:28:09,260
specifically what you're doing and what

2018
01:28:09,260 --> 01:28:12,979
you care about and it's not it's not a

2019
01:28:12,979 --> 01:28:15,019
deep learning specific question either

2020
01:28:15,019 --> 01:28:17,059
you know so those who are interested in

2021
01:28:17,059 --> 01:28:17,989
this kind of thing we're going to look

2022
01:28:17,989 --> 01:28:19,639
into it a lot more detail in our machine

2023
01:28:19,639 --> 01:28:23,209
learning course which will also be

2024
01:28:23,209 --> 01:28:28,459
available online ok so I did the same

2025
01:28:28,459 --> 01:28:29,869
thing for the columns just to make sure

2026
01:28:29,869 --> 01:28:31,699
that these aren't like super wide and

2027
01:28:31,699 --> 01:28:33,409
I've got similar results and checked in

2028
01:28:33,409 --> 01:28:35,030
and again found they're kind of like 4

2029
01:28:35,030 --> 01:28:37,630
or 500 seem to be about the average size

2030
01:28:37,630 --> 01:28:39,829
so based on all of that I kind of

2031
01:28:39,829 --> 01:28:41,239
thought ok this looks like a pretty

2032
01:28:41,239 --> 01:28:43,280
normal kind of image data set that I can

2033
01:28:43,280 --> 01:28:44,929
probably use pretty normal kinds of

2034
01:28:44,929 --> 01:28:46,969
models on I was also particularly

2035
01:28:46,969 --> 01:28:48,320
encouraged to see that when I looked at

2036
01:28:48,320 --> 01:28:48,630
the

2037
01:28:48,630 --> 01:28:51,060
that the dog like takes up most of the

2038
01:28:51,060 --> 01:28:53,040
frame right so I'm not too worried about

2039
01:28:53,040 --> 01:28:56,220
like cropping problems you know if the

2040
01:28:56,220 --> 01:28:57,840
if the dog was just like a tiny little

2041
01:28:57,840 --> 01:29:00,480
piece of one little corner that I'd be

2042
01:29:00,480 --> 01:29:02,910
thinking about doing different you know

2043
01:29:02,910 --> 01:29:04,710
maybe zooming in a lot more or something

2044
01:29:04,710 --> 01:29:07,230
like a medical imaging that happens a

2045
01:29:07,230 --> 01:29:09,150
lot like often the tumor or the cell

2046
01:29:09,150 --> 01:29:11,130
whatever is like one tiny piece and

2047
01:29:11,130 --> 01:29:14,070
there's much more complex so yeah based

2048
01:29:14,070 --> 01:29:15,780
on all that and this morning I kind of

2049
01:29:15,780 --> 01:29:18,390
thought like okay this looks pretty

2050
01:29:18,390 --> 01:29:24,630
standard so I I went ahead and created a

2051
01:29:24,630 --> 01:29:26,190
little function called get data that

2052
01:29:26,190 --> 01:29:28,140
basically had my normal two lines of

2053
01:29:28,140 --> 01:29:31,890
code in it but I made it so I could

2054
01:29:31,890 --> 01:29:35,520
passed in a size and a batch size the

2055
01:29:35,520 --> 01:29:37,350
reason for this is that when I start

2056
01:29:37,350 --> 01:29:38,820
working with new data set I want

2057
01:29:38,820 --> 01:29:41,010
everything to go super fast and so if I

2058
01:29:41,010 --> 01:29:43,350
use small images it's going to go super

2059
01:29:43,350 --> 01:29:46,380
fast so I actually started out with size

2060
01:29:46,380 --> 01:29:49,350
equals 64 just to create some super

2061
01:29:49,350 --> 01:29:51,390
small images that just go like a second

2062
01:29:51,390 --> 01:29:54,840
to run through and see how it later on I

2063
01:29:54,840 --> 01:29:57,420
started using some big images and some

2064
01:29:57,420 --> 01:29:59,310
and some also some bigger architectures

2065
01:29:59,310 --> 01:30:01,140
at which point I started running out of

2066
01:30:01,140 --> 01:30:02,970
GPU memory so I started getting these

2067
01:30:02,970 --> 01:30:05,910
errors saying CUDA out of memory error

2068
01:30:05,910 --> 01:30:08,220
when you get a CUDA out of memory error

2069
01:30:08,220 --> 01:30:10,110
the first thing you need to do is to go

2070
01:30:10,110 --> 01:30:13,770
kernel restart once you get a code an

2071
01:30:13,770 --> 01:30:15,450
out of memory error on your GPU you

2072
01:30:15,450 --> 01:30:17,130
can't really recover from it right

2073
01:30:17,130 --> 01:30:18,780
doesn't matter what you do you know you

2074
01:30:18,780 --> 01:30:22,080
have to go restart and once I've

2075
01:30:22,080 --> 01:30:24,120
restarted I then just changed my batch

2076
01:30:24,120 --> 01:30:26,430
size to something smaller so when you

2077
01:30:26,430 --> 01:30:30,780
call create your data object you can

2078
01:30:30,780 --> 01:30:34,800
pass in a batch size parameter okay and

2079
01:30:34,800 --> 01:30:37,800
like i normally use 64 until i hit

2080
01:30:37,800 --> 01:30:39,240
something that says out of memory and

2081
01:30:39,240 --> 01:30:40,980
then i'll just have it and if i still

2082
01:30:40,980 --> 01:30:42,390
get out of memory I was hobbit again

2083
01:30:42,390 --> 01:30:45,930
okay so that's where I created this to

2084
01:30:45,930 --> 01:30:47,910
allow me to like start making my size as

2085
01:30:47,910 --> 01:30:50,310
bigger as I looked into it more and you

2086
01:30:50,310 --> 01:30:51,450
know as I started running out of memory

2087
01:30:51,450 --> 01:30:55,290
to decrease my batch size so at this

2088
01:30:55,290 --> 01:30:57,810
point you know I went through this a

2089
01:30:57,810 --> 01:30:59,400
couple of iterations but I basically

2090
01:30:59,400 --> 01:31:01,350
found everything was working fine so

2091
01:31:01,350 --> 01:31:02,320
once it's working fine

2092
01:31:02,320 --> 01:31:07,900
set size 2 to 24 and I created my you

2093
01:31:07,900 --> 01:31:10,000
know pre-compute equals true first time

2094
01:31:10,000 --> 01:31:11,680
I did that it took a minute to create

2095
01:31:11,680 --> 01:31:13,810
the precomputed activations and then it

2096
01:31:13,810 --> 01:31:15,700
ran through this in about 4 or 5 seconds

2097
01:31:15,700 --> 01:31:17,230
and you can see I was getting

2098
01:31:17,230 --> 01:31:18,460
eighty-three percent accuracy

2099
01:31:18,460 --> 01:31:21,670
now remember accuracy means it's it's

2100
01:31:21,670 --> 01:31:24,070
exactly right and so it's predicting out

2101
01:31:24,070 --> 01:31:26,170
of a hundred and twenty categories it's

2102
01:31:26,170 --> 01:31:27,820
predicting exactly right so when you see

2103
01:31:27,820 --> 01:31:30,850
something with two classes is you know

2104
01:31:30,850 --> 01:31:33,490
80% accurate versus something with 120

2105
01:31:33,490 --> 01:31:36,010
classes is 80% accurate they're very

2106
01:31:36,010 --> 01:31:39,130
different levels you know so when I saw

2107
01:31:39,130 --> 01:31:41,320
like eighty-three percent accuracy with

2108
01:31:41,320 --> 01:31:43,930
just a pre computed classify and OData

2109
01:31:43,930 --> 01:31:45,460
augmentation though I'm freezing

2110
01:31:45,460 --> 01:31:46,690
anything else

2111
01:31:46,690 --> 01:31:49,780
across 120 classes of the oh this looks

2112
01:31:49,780 --> 01:31:54,100
good right so um then I just kind of

2113
01:31:54,100 --> 01:31:56,380
kept going throughout at all standard

2114
01:31:56,380 --> 01:32:01,060
process right so then I turn precompute

2115
01:32:01,060 --> 01:32:06,670
off okay and cycle length equals one and

2116
01:32:06,670 --> 01:32:10,090
I started doing a few more cycles few

2117
01:32:10,090 --> 01:32:15,400
more epochs so remember an epoch is one

2118
01:32:15,400 --> 01:32:19,500
pass through the data and a cycle is

2119
01:32:19,500 --> 01:32:22,480
however many epochs you said is in a

2120
01:32:22,480 --> 01:32:24,910
cycle it's one it's the learning rate

2121
01:32:24,910 --> 01:32:26,530
going from the top that you asked for

2122
01:32:26,530 --> 01:32:29,290
all the way down so since here cycle

2123
01:32:29,290 --> 01:32:31,990
length equals one a cycle in an epoch at

2124
01:32:31,990 --> 01:32:35,670
the same okay so I did I tried a few

2125
01:32:35,670 --> 01:32:39,490
epochs I did actually do the learning

2126
01:32:39,490 --> 01:32:41,140
rate finder and I found one in a two

2127
01:32:41,140 --> 01:32:43,720
again looked fine it often looks fine

2128
01:32:43,720 --> 01:32:46,690
and I found it kind of kept improving so

2129
01:32:46,690 --> 01:32:48,460
I tried five epochs and I found my

2130
01:32:48,460 --> 01:32:54,220
accuracy getting better so then I saved

2131
01:32:54,220 --> 01:32:56,980
that and I tried something which we

2132
01:32:56,980 --> 01:32:58,480
haven't looked at before but it's kind

2133
01:32:58,480 --> 01:33:01,830
of cool if you train something on a

2134
01:33:01,830 --> 01:33:05,800
smaller size you can then actually call

2135
01:33:05,800 --> 01:33:10,060
learned set data and pass in a larger

2136
01:33:10,060 --> 01:33:12,370
size data set and that's gonna take your

2137
01:33:12,370 --> 01:33:14,710
model however it's trained so far and

2138
01:33:14,710 --> 01:33:16,389
it's going to let you can

2139
01:33:16,389 --> 01:33:20,439
in you to train on on larger images and

2140
01:33:20,439 --> 01:33:22,599
I tell you something amazing

2141
01:33:22,599 --> 01:33:25,659
this actually is another way you can get

2142
01:33:25,659 --> 01:33:26,919
state-of-the-art results and I've never

2143
01:33:26,919 --> 01:33:29,079
seen this written in any paper or

2144
01:33:29,079 --> 01:33:30,880
discussed anywhere as far as I know this

2145
01:33:30,880 --> 01:33:34,269
is a new insight basically I've got a

2146
01:33:34,269 --> 01:33:36,249
pre trained model which in this case

2147
01:33:36,249 --> 01:33:38,349
I've trained a few epochs with the size

2148
01:33:38,349 --> 01:33:41,619
of 224 by 224 and I'm now going to do a

2149
01:33:41,619 --> 01:33:43,749
few more air pops with the size of 299

2150
01:33:43,749 --> 01:33:47,409
by 299 now I've gotten very little data

2151
01:33:47,409 --> 01:33:49,090
cut out by deep learning standards only

2152
01:33:49,090 --> 01:33:52,209
about 10,000 images right so with a 224

2153
01:33:52,209 --> 01:33:55,179
by 224 I kind of built this these final

2154
01:33:55,179 --> 01:33:57,880
layers to try to find things that work

2155
01:33:57,880 --> 01:34:01,329
well to 24 but to 24 but I go to 299 by

2156
01:34:01,329 --> 01:34:05,919
299 I basically if I over fit before I'm

2157
01:34:05,919 --> 01:34:07,239
definitely not going to over fit now

2158
01:34:07,239 --> 01:34:08,949
might have changed the size of my images

2159
01:34:08,949 --> 01:34:10,289
they're kind of like totally different

2160
01:34:10,289 --> 01:34:13,269
but like conceptually they're still

2161
01:34:13,269 --> 01:34:14,860
picked the same kinds of pictures are

2162
01:34:14,860 --> 01:34:16,929
the same kinds of things so I found this

2163
01:34:16,929 --> 01:34:19,090
trick of like starting training on small

2164
01:34:19,090 --> 01:34:21,219
images for a few a box and then

2165
01:34:21,219 --> 01:34:22,809
switching to bigger images and

2166
01:34:22,809 --> 01:34:25,209
continuing training is an amazingly

2167
01:34:25,209 --> 01:34:28,709
effective way to avoid overfitting and

2168
01:34:28,709 --> 01:34:32,590
it's like it's so easy and so obvious I

2169
01:34:32,590 --> 01:34:34,570
don't understand why it's never been

2170
01:34:34,570 --> 01:34:36,070
written about before maybe it's in some

2171
01:34:36,070 --> 01:34:37,479
paper somewhere and I haven't found it

2172
01:34:37,479 --> 01:34:44,260
but it's I haven't seen it would it be

2173
01:34:44,260 --> 01:34:46,300
possible to do the same thing on using

2174
01:34:46,300 --> 01:34:49,719
let's take a resort our disposal to feed

2175
01:34:49,719 --> 01:34:53,709
a different size yeah I think so like as

2176
01:34:53,709 --> 01:34:56,010
long as you use one of these more modern

2177
01:34:56,010 --> 01:34:57,760
architectures what we call fully

2178
01:34:57,760 --> 01:35:00,340
convolutional architectures which means

2179
01:35:00,340 --> 01:35:02,979
not vgg and you'll see we don't use vgg

2180
01:35:02,979 --> 01:35:04,389
in this course because it doesn't have

2181
01:35:04,389 --> 01:35:06,209
this property but most of the

2182
01:35:06,209 --> 01:35:07,840
architectures developed in the last

2183
01:35:07,840 --> 01:35:09,550
couple of years can handle pretty much

2184
01:35:09,550 --> 01:35:13,320
arbitrary sizes yeah be worth trying

2185
01:35:13,320 --> 01:35:18,189
yeah I think it ought to work okay so I

2186
01:35:18,189 --> 01:35:20,110
call get data again remember get data is

2187
01:35:20,110 --> 01:35:21,280
the just a little function that I

2188
01:35:21,280 --> 01:35:22,929
created back up here right get data is

2189
01:35:22,929 --> 01:35:24,669
just this little function that's oh I

2190
01:35:24,669 --> 01:35:27,209
just passed a different size to it and

2191
01:35:27,209 --> 01:35:31,899
so I call freeze just to make sure that

2192
01:35:31,899 --> 01:35:33,669
but everything so the last layer is

2193
01:35:33,669 --> 01:35:35,709
frozen I mean it actually already was at

2194
01:35:35,709 --> 01:35:37,809
its point that really doing a thing

2195
01:35:37,809 --> 01:35:42,150
and

2196
01:35:42,150 --> 01:35:44,670
you can see now with free compute off

2197
01:35:44,670 --> 01:35:46,590
I've now got that data augmentation

2198
01:35:46,590 --> 01:35:48,510
working so I kind of run a few more a

2199
01:35:48,510 --> 01:35:51,750
pox and what I notice here is that the

2200
01:35:51,750 --> 01:35:54,420
loss to my training set and the loss of

2201
01:35:54,420 --> 01:35:56,340
my validation set my validation set loss

2202
01:35:56,340 --> 01:35:58,620
is a lot lower than my training set this

2203
01:35:58,620 --> 01:36:00,929
is still just training the last layer so

2204
01:36:00,929 --> 01:36:02,880
what this is telling me is I'm under

2205
01:36:02,880 --> 01:36:05,280
fitting right and so from under fitting

2206
01:36:05,280 --> 01:36:07,469
it means this cycle length equals one is

2207
01:36:07,469 --> 01:36:08,850
too short it means it's like finding

2208
01:36:08,850 --> 01:36:10,800
something better popped with popping out

2209
01:36:10,800 --> 01:36:12,330
and it's like never getting a chance to

2210
01:36:12,330 --> 01:36:15,750
zoom in properly so then I'd set cycle

2211
01:36:15,750 --> 01:36:18,540
mod equals two to give it more time so

2212
01:36:18,540 --> 01:36:21,270
like the first time is one epoch the

2213
01:36:21,270 --> 01:36:23,010
second one is two epochs

2214
01:36:23,010 --> 01:36:26,070
the third one is for epochs and you can

2215
01:36:26,070 --> 01:36:28,350
see now the validation train and

2216
01:36:28,350 --> 01:36:30,750
training are about the same okay so

2217
01:36:30,750 --> 01:36:32,130
that's kind of thinking yeah this is

2218
01:36:32,130 --> 01:36:34,800
this is about the right track and so

2219
01:36:34,800 --> 01:36:36,560
then I tried using test time

2220
01:36:36,560 --> 01:36:38,489
augmentation to see if that gets any

2221
01:36:38,489 --> 01:36:40,679
better still didn't actually help a hell

2222
01:36:40,679 --> 01:36:44,010
of a lot just a tiny bit and just kind

2223
01:36:44,010 --> 01:36:46,020
of at this point I think here this is

2224
01:36:46,020 --> 01:36:48,630
nearly done so I just did it like you

2225
01:36:48,630 --> 01:36:50,550
know one more cycle of two to see if it

2226
01:36:50,550 --> 01:36:53,190
got any better and it did get a little

2227
01:36:53,190 --> 01:36:55,679
bit better and then I'm like okay that

2228
01:36:55,679 --> 01:36:57,600
looks pretty good

2229
01:36:57,600 --> 01:37:02,540
I've got a validation set lost 0.199

2230
01:37:02,540 --> 01:37:05,520
and so your Lotus here actually you

2231
01:37:05,520 --> 01:37:08,010
haven't tried unfreezing the reason why

2232
01:37:08,010 --> 01:37:09,630
I was going to try to unfreezing and

2233
01:37:09,630 --> 01:37:11,370
training more it didn't get any better

2234
01:37:11,370 --> 01:37:13,890
and so the reason for this clearly is

2235
01:37:13,890 --> 01:37:16,469
that this data set is so similar the

2236
01:37:16,469 --> 01:37:19,230
image net that the training that

2237
01:37:19,230 --> 01:37:20,790
convolutional layers actually doesn't

2238
01:37:20,790 --> 01:37:23,580
help in the slightest and actually when

2239
01:37:23,580 --> 01:37:25,230
I loaded up into it it turns out that

2240
01:37:25,230 --> 01:37:27,980
this competition is actually using a

2241
01:37:27,980 --> 01:37:30,750
subset of improve image net so that's

2242
01:37:30,750 --> 01:37:33,360
okay so that if we check this out point

2243
01:37:33,360 --> 01:37:36,110
one nine nine against the leaderboard

2244
01:37:36,110 --> 01:37:38,550
this is only a playground competition so

2245
01:37:38,550 --> 01:37:40,620
it's not like the best of here but you

2246
01:37:40,620 --> 01:37:44,780
know it's still interesting it gets us

2247
01:37:44,780 --> 01:37:48,110
somewhere around ten thrillers okay and

2248
01:37:48,110 --> 01:37:52,050
in fact we're competing against I

2249
01:37:52,050 --> 01:37:53,420
noticed other

2250
01:37:53,420 --> 01:37:55,340
the first AI student this is a first AI

2251
01:37:55,340 --> 01:37:58,880
student these people up here I know they

2252
01:37:58,880 --> 01:38:00,890
actually posted that they cheated they

2253
01:38:00,890 --> 01:38:02,060
actually went you downloaded the

2254
01:38:02,060 --> 01:38:07,429
original images and train to that so and

2255
01:38:07,429 --> 01:38:08,630
this is why this is a playground

2256
01:38:08,630 --> 01:38:10,489
competition they call it it's not it's

2257
01:38:10,489 --> 01:38:12,290
not real right you know it's just to

2258
01:38:12,290 --> 01:38:14,270
allow us to try things out but you can

2259
01:38:14,270 --> 01:38:18,110
basically see out of two hundred and

2260
01:38:18,110 --> 01:38:20,270
something people where you know we're

2261
01:38:20,270 --> 01:38:23,300
getting some very good results without

2262
01:38:23,300 --> 01:38:25,429
doing anything remotely interesting or

2263
01:38:25,429 --> 01:38:26,660
clever and we haven't even used the

2264
01:38:26,660 --> 01:38:27,739
whole data set you're going to use to

2265
01:38:27,739 --> 01:38:29,390
eighty percent of it like to get a

2266
01:38:29,390 --> 01:38:31,699
better result I would go back and remove

2267
01:38:31,699 --> 01:38:33,679
that validation set and just rerun the

2268
01:38:33,679 --> 01:38:36,110
same steps and then submit that exact

2269
01:38:36,110 --> 01:38:37,100
let's just use it under percent of the

2270
01:38:37,100 --> 01:38:48,650
data I have three questions the first

2271
01:38:48,650 --> 01:38:50,480
one is like that class in this case is

2272
01:38:50,480 --> 01:38:53,780
very it's not balanced

2273
01:38:53,780 --> 01:38:56,929
instead unbalanced like it's not totally

2274
01:38:56,929 --> 01:38:58,520
balanced but it's not bad right it's

2275
01:38:58,520 --> 01:39:02,420
like between sixty and a hundred like

2276
01:39:02,420 --> 01:39:04,670
it's it's it's it's not unbalanced

2277
01:39:04,670 --> 01:39:05,870
enough that I would give it a second

2278
01:39:05,870 --> 01:39:06,080
thought

2279
01:39:06,080 --> 01:39:14,600
okay yeah let's get to that later in

2280
01:39:14,600 --> 01:39:16,070
this course and don't let me forget

2281
01:39:16,070 --> 01:39:18,710
right the short answer is that there was

2282
01:39:18,710 --> 01:39:20,090
a recent list the paper came out about

2283
01:39:20,090 --> 01:39:21,980
two or three weeks ago on this and it

2284
01:39:21,980 --> 01:39:23,390
said the best way to deal with very

2285
01:39:23,390 --> 01:39:25,640
unbalanced data sets is to basically

2286
01:39:25,640 --> 01:39:35,600
make copies of the rare cases yeah my

2287
01:39:35,600 --> 01:39:38,660
second question is I want to pin down a

2288
01:39:38,660 --> 01:39:40,310
difference between creation he read was

2289
01:39:40,310 --> 01:39:44,390
and so you have these two options right

2290
01:39:44,390 --> 01:39:47,179
so when you beginning I did an

2291
01:39:47,179 --> 01:39:49,130
optimization use that pre computed it

2292
01:39:49,130 --> 01:39:52,370
was true by not using layers right right

2293
01:39:52,370 --> 01:39:54,290
so it's not only they frozen their pre

2294
01:39:54,290 --> 01:39:55,670
computed so the data augmentation

2295
01:39:55,670 --> 01:39:59,410
doesn't do anything at that point

2296
01:39:59,410 --> 01:40:03,580
right before you outcries everything

2297
01:40:03,580 --> 01:40:06,050
what as examples you and IV only you

2298
01:40:06,050 --> 01:40:08,130
only on freeze

2299
01:40:08,130 --> 01:40:10,650
so we're going to learn more about the

2300
01:40:10,650 --> 01:40:12,630
details as we look into the the math and

2301
01:40:12,630 --> 01:40:14,459
stuff in coming lessons but basically

2302
01:40:14,459 --> 01:40:16,979
what happened was we started with a pre

2303
01:40:16,979 --> 01:40:20,550
trained network right which was kind of

2304
01:40:20,550 --> 01:40:23,400
finding activations that had these kind

2305
01:40:23,400 --> 01:40:27,539
of rich features and we were adding then

2306
01:40:27,539 --> 01:40:29,400
we add a couple of layers on the end of

2307
01:40:29,400 --> 01:40:33,659
it which which start out random and so

2308
01:40:33,659 --> 01:40:36,659
with fries equals with with everything

2309
01:40:36,659 --> 01:40:38,550
frozen and indeed with pre compute

2310
01:40:38,550 --> 01:40:40,829
equals true all we're learning is told

2311
01:40:40,829 --> 01:40:42,539
is those couple of layers that we've

2312
01:40:42,539 --> 01:40:45,179
added and so with pre compute equals

2313
01:40:45,179 --> 01:40:47,639
true we actually pretty calculate like

2314
01:40:47,639 --> 01:40:49,769
how much does this image have something

2315
01:40:49,769 --> 01:40:51,239
that looks like this a ball one looks

2316
01:40:51,239 --> 01:40:53,489
like this face and so forth and

2317
01:40:53,489 --> 01:40:55,079
therefore data augmentation doesn't do

2318
01:40:55,079 --> 01:40:56,729
anything with pre compute equals true

2319
01:40:56,729 --> 01:40:59,429
because you know we're actually showing

2320
01:40:59,429 --> 01:41:01,079
exactly the same activations each time

2321
01:41:01,079 --> 01:41:03,510
we can then set pre compute equals false

2322
01:41:03,510 --> 01:41:06,360
which means it's still only training

2323
01:41:06,360 --> 01:41:09,209
those last two layers that we added it's

2324
01:41:09,209 --> 01:41:11,699
still frozen but data augmentations now

2325
01:41:11,699 --> 01:41:12,929
working because it's actually going

2326
01:41:12,929 --> 01:41:14,429
through and recalculating all of the

2327
01:41:14,429 --> 01:41:17,249
activations from scratch and then

2328
01:41:17,249 --> 01:41:19,949
finally when we unfreeze that's actually

2329
01:41:19,949 --> 01:41:21,360
saying okay now you can go ahead and

2330
01:41:21,360 --> 01:41:23,090
change all of these earlier

2331
01:41:23,090 --> 01:41:28,999
convolutional filters so well you just

2332
01:41:28,999 --> 01:41:31,679
so the only reason to have pre compute

2333
01:41:31,679 --> 01:41:33,749
equals true is it's just much faster so

2334
01:41:33,749 --> 01:41:35,670
it's like it is it's about you know ten

2335
01:41:35,670 --> 01:41:38,249
or more times faster so particularly if

2336
01:41:38,249 --> 01:41:39,360
you're working with like quite a large

2337
01:41:39,360 --> 01:41:42,599
data set you know it can save quite a

2338
01:41:42,599 --> 01:41:45,719
bit of time but it's never there's no

2339
01:41:45,719 --> 01:41:49,499
like companies like accuracy reason ever

2340
01:41:49,499 --> 01:41:51,239
to use pre computed calls true it's just

2341
01:41:51,239 --> 01:41:53,039
a it's just a shortcut it's also like

2342
01:41:53,039 --> 01:41:55,590
quite handy if you're like throwing

2343
01:41:55,590 --> 01:41:57,659
together a quick model you know it can

2344
01:41:57,659 --> 01:42:01,699
take a few seconds to create

2345
01:42:01,699 --> 01:42:03,619
my last question which I think you

2346
01:42:03,619 --> 01:42:07,280
answer is I don't like your suggestions

2347
01:42:07,280 --> 01:42:11,079
to build a model you have this aged yeah

2348
01:42:11,079 --> 01:42:15,229
what if would you like we just wanted

2349
01:42:15,229 --> 01:42:19,760
one initial setting without these like

2350
01:42:19,760 --> 01:42:24,170
checking after each I mean if you want

2351
01:42:24,170 --> 01:42:25,849
it like if your question is like is

2352
01:42:25,849 --> 01:42:27,559
there some shorter version of this

2353
01:42:27,559 --> 01:42:29,719
that's like a bit quicker and easier I

2354
01:42:29,719 --> 01:42:39,159
could like to lead a few things here

2355
01:42:39,159 --> 01:42:41,989
okay I think this is a kind of a minimal

2356
01:42:41,989 --> 01:42:43,820
version to get you a very good result

2357
01:42:43,820 --> 01:42:45,769
which is like don't worry about pre

2358
01:42:45,769 --> 01:42:47,449
compute equals true because that's just

2359
01:42:47,449 --> 01:42:49,550
saving a little bit of time you know so

2360
01:42:49,550 --> 01:42:52,550
so I still suggest use LR find at the

2361
01:42:52,550 --> 01:42:56,030
start to find a good learning rate by

2362
01:42:56,030 --> 01:42:57,650
default everything is frozen from the

2363
01:42:57,650 --> 01:42:59,059
start so then you can just go ahead and

2364
01:42:59,059 --> 01:43:01,159
run two or three epochs or cyclic

2365
01:43:01,159 --> 01:43:05,209
Nichols one unfreeze and then train the

2366
01:43:05,209 --> 01:43:07,039
rest of the network with differential

2367
01:43:07,039 --> 01:43:09,079
learning rates so it's basically three

2368
01:43:09,079 --> 01:43:13,280
steps learning rate finder trained

2369
01:43:13,280 --> 01:43:15,409
frozen network with cycle methods one

2370
01:43:15,409 --> 01:43:18,559
and then trained unfrozen network with

2371
01:43:18,559 --> 01:43:20,479
differential learning rates and cycle

2372
01:43:20,479 --> 01:43:22,969
molecules too so like that's something

2373
01:43:22,969 --> 01:43:26,929
you could turn into I guess five or six

2374
01:43:26,929 --> 01:43:30,829
lines of code at all I think it's a

2375
01:43:30,829 --> 01:43:34,219
question provide your own mix book by

2376
01:43:34,219 --> 01:43:36,979
reusing the batch size does the only at

2377
01:43:36,979 --> 01:43:39,409
better speed of training yeah pretty

2378
01:43:39,409 --> 01:43:42,139
much so each batch and again we're going

2379
01:43:42,139 --> 01:43:43,429
to see like all this stuff about

2380
01:43:43,429 --> 01:43:45,739
precomputing batch sizes we dig into the

2381
01:43:45,739 --> 01:43:47,329
details of the algorithms it's going to

2382
01:43:47,329 --> 01:43:49,130
make a lot more sense intuitively but

2383
01:43:49,130 --> 01:43:53,059
basically if you're showing it less

2384
01:43:53,059 --> 01:43:56,150
images each time then it's calculating

2385
01:43:56,150 --> 01:43:58,519
the gradient with less images which

2386
01:43:58,519 --> 01:44:00,289
means it's less accurate which means

2387
01:44:00,289 --> 01:44:02,179
like knowing which direction to go and

2388
01:44:02,179 --> 01:44:04,789
how far to go in that direction is less

2389
01:44:04,789 --> 01:44:06,590
accurate so as you make the batch size

2390
01:44:06,590 --> 01:44:07,159
smaller

2391
01:44:07,159 --> 01:44:09,320
you're basically making it kind of more

2392
01:44:09,320 --> 01:44:11,429
volatile it's

2393
01:44:11,429 --> 01:44:18,139
kind of like it kind of impacts the

2394
01:44:18,139 --> 01:44:20,159
optimal learning rate that you would

2395
01:44:20,159 --> 01:44:22,260
need to use but in practice where only

2396
01:44:22,260 --> 01:44:24,389
you know I generally find only dividing

2397
01:44:24,389 --> 01:44:26,909
with the batch size by like 2 or 4 it

2398
01:44:26,909 --> 01:44:28,199
doesn't seem to change things very much

2399
01:44:28,199 --> 01:44:30,869
should I reveals the learning rate of

2400
01:44:30,869 --> 01:44:33,329
quality me if you if you change the

2401
01:44:33,329 --> 01:44:34,979
batch size by much you can rerun the

2402
01:44:34,979 --> 01:44:36,719
learning rate finder to see if it's

2403
01:44:36,719 --> 01:44:38,760
changed if I match but it it I was in

2404
01:44:38,760 --> 01:44:40,079
for only generally looking at like a

2405
01:44:40,079 --> 01:44:42,179
power of 10 it probably is not going to

2406
01:44:42,179 --> 01:44:43,499
change the it's not that you can't

2407
01:44:43,499 --> 01:44:48,090
because plus back there

2408
01:44:48,090 --> 01:44:50,429
this is sort of a conceptual basic

2409
01:44:50,429 --> 01:44:51,780
questions we're going back to the

2410
01:44:51,780 --> 01:44:53,699
previous night where you should put in

2411
01:44:53,699 --> 01:44:55,679
the thought behind sorry yeah this is

2412
01:44:55,679 --> 01:44:57,840
well one for conceptual so a basic

2413
01:44:57,840 --> 01:44:59,940
question we've actually really slide

2414
01:44:59,940 --> 01:45:01,440
where you should what the different

2415
01:45:01,440 --> 01:45:07,889
layers were doing yes from this slide I

2416
01:45:07,889 --> 01:45:11,429
understand right the meaning of sync the

2417
01:45:11,429 --> 01:45:12,960
third column relative to the fourth

2418
01:45:12,960 --> 01:45:16,290
column is that what you're interpreting

2419
01:45:16,290 --> 01:45:19,500
what the layer is doing based on what

2420
01:45:19,500 --> 01:45:23,010
the image is actually yeah so we're

2421
01:45:23,010 --> 01:45:24,510
going to look at this in more detail so

2422
01:45:24,510 --> 01:45:26,550
these these gray ones basically say this

2423
01:45:26,550 --> 01:45:28,920
is kind of what the filter looks like so

2424
01:45:28,920 --> 01:45:31,170
on the first layer you can see exactly

2425
01:45:31,170 --> 01:45:32,730
what the filter looks like because the

2426
01:45:32,730 --> 01:45:34,920
input to it of pixels right so you can

2427
01:45:34,920 --> 01:45:36,449
absolutely say and remember we looked at

2428
01:45:36,449 --> 01:45:38,940
what a convolutional kernel was like was

2429
01:45:38,940 --> 01:45:41,040
that three by three thing so this look

2430
01:45:41,040 --> 01:45:43,020
like there's seven by seven kernels you

2431
01:45:43,020 --> 01:45:44,429
can say this is actually what it looks

2432
01:45:44,429 --> 01:45:47,369
like but later on it's combined you know

2433
01:45:47,369 --> 01:45:50,449
the the the input to it are themselves

2434
01:45:50,449 --> 01:45:52,409
activations which are combinations of

2435
01:45:52,409 --> 01:45:54,179
activations relation to activations so

2436
01:45:54,179 --> 01:45:56,820
you can't draw it but there's clever

2437
01:45:56,820 --> 01:45:58,710
technique that I learned focus created

2438
01:45:58,710 --> 01:46:00,570
which allowed them to say this is kind

2439
01:46:00,570 --> 01:46:02,460
of what the filters tended to look like

2440
01:46:02,460 --> 01:46:04,770
on average alright so this is kind of

2441
01:46:04,770 --> 01:46:06,570
what the photos look like and then here

2442
01:46:06,570 --> 01:46:10,110
is specific examples of patches of image

2443
01:46:10,110 --> 01:46:14,369
which activated that filter highly so

2444
01:46:14,369 --> 01:46:16,230
yet the pictures are the ones that I

2445
01:46:16,230 --> 01:46:17,849
kind of find more useful because it

2446
01:46:17,849 --> 01:46:21,929
tells you this kernel is kind of a mini

2447
01:46:21,929 --> 01:46:35,070
cycle we all find right how do we know

2448
01:46:35,070 --> 01:46:40,290
that's it well we'll come back well we

2449
01:46:40,290 --> 01:46:41,790
may come back to that if not in this

2450
01:46:41,790 --> 01:46:45,210
part in the next part that probably a

2451
01:46:45,210 --> 01:46:48,030
part two actually because this paper

2452
01:46:48,030 --> 01:46:49,860
this paper uses to create these things

2453
01:46:49,860 --> 01:46:50,909
this paper uses something called a

2454
01:46:50,909 --> 01:46:53,309
deconvolution which I'm pretty sure we

2455
01:46:53,309 --> 01:46:54,719
won't do in this part but we will do it

2456
01:46:54,719 --> 01:46:57,420
in part two so if you're interested

2457
01:46:57,420 --> 01:47:00,090
check out the paper it's it's in the

2458
01:47:00,090 --> 01:47:01,860
notebook has a link to it xyler in

2459
01:47:01,860 --> 01:47:06,060
Fergus it's a very clever technique and

2460
01:47:06,060 --> 01:47:10,880
not terribly intuitive

2461
01:47:10,880 --> 01:47:18,590
um right so so you mentioned that it was

2462
01:47:18,590 --> 01:47:20,630
good that the dog took up the full

2463
01:47:20,630 --> 01:47:22,340
picture and it would have been a problem

2464
01:47:22,340 --> 01:47:24,409
if it was kind of like off in one of the

2465
01:47:24,409 --> 01:47:27,260
corners in really tiny well what would

2466
01:47:27,260 --> 01:47:29,600
you what would you technique have been

2467
01:47:29,600 --> 01:47:34,040
to try to make that work something that

2468
01:47:34,040 --> 01:47:36,260
we'll learn about in part two but

2469
01:47:36,260 --> 01:47:38,449
basically there's a technique that

2470
01:47:38,449 --> 01:47:41,540
allows you to to kind of figure out

2471
01:47:41,540 --> 01:47:43,580
roughly which parts of an image and most

2472
01:47:43,580 --> 01:47:45,530
likely to have the interesting things in

2473
01:47:45,530 --> 01:47:47,810
them and then you can like crop out

2474
01:47:47,810 --> 01:47:50,060
those bits if you're interested in

2475
01:47:50,060 --> 01:47:52,250
learning about it we did cover it

2476
01:47:52,250 --> 01:47:55,730
briefly in lesson seven of part one but

2477
01:47:55,730 --> 01:47:58,250
I'm going to actually do it properly in

2478
01:47:58,250 --> 01:48:01,610
part two of this course because I didn't

2479
01:48:01,610 --> 01:48:06,469
really cover it thoroughly enough maybe

2480
01:48:06,469 --> 01:48:07,850
we'll find time to have a quick look at

2481
01:48:07,850 --> 01:48:10,190
it but we'll see I know your Nets

2482
01:48:10,190 --> 01:48:11,300
written some of the code that we need

2483
01:48:11,300 --> 01:48:13,670
already

2484
01:48:13,670 --> 01:48:16,719
ah

2485
01:48:16,719 --> 01:48:18,810
so once I have something like this

2486
01:48:18,810 --> 01:48:24,360
notebook that's basically working I can

2487
01:48:24,360 --> 01:48:28,390
immediately make it better by doing two

2488
01:48:28,390 --> 01:48:32,140
things assuming that the size image I

2489
01:48:32,140 --> 01:48:34,150
was using is smaller than the average

2490
01:48:34,150 --> 01:48:35,980
size of the image that we've been given

2491
01:48:35,980 --> 01:48:38,830
I can increase the size and as I showed

2492
01:48:38,830 --> 01:48:40,330
before with the dog breeds you can

2493
01:48:40,330 --> 01:48:42,670
actually increase it during training the

2494
01:48:42,670 --> 01:48:45,520
other thing I can do is to create is to

2495
01:48:45,520 --> 01:48:48,280
use a better architecture now an

2496
01:48:48,280 --> 01:48:49,840
architect we're going to talk a lot in

2497
01:48:49,840 --> 01:48:51,880
this course about architectures but

2498
01:48:51,880 --> 01:48:58,300
basically there are different ways of

2499
01:48:58,300 --> 01:49:00,580
putting together like what size

2500
01:49:00,580 --> 01:49:02,230
convolutional filters and how are they

2501
01:49:02,230 --> 01:49:05,730
connected to each other and so forth and

2502
01:49:05,730 --> 01:49:07,810
different architectures have different

2503
01:49:07,810 --> 01:49:09,760
like numbers of layers and sizes of

2504
01:49:09,760 --> 01:49:11,650
kernels and number of filters and so

2505
01:49:11,650 --> 01:49:16,780
forth and so there are some the one that

2506
01:49:16,780 --> 01:49:19,050
we've been using ResNet 34 is a great

2507
01:49:19,050 --> 01:49:21,730
starting point and often a good

2508
01:49:21,730 --> 01:49:23,770
finishing point because it's like it's

2509
01:49:23,770 --> 01:49:25,480
pretty it doesn't have too many

2510
01:49:25,480 --> 01:49:27,040
parameters often it works pretty well

2511
01:49:27,040 --> 01:49:28,989
with small amounts of data as we've seen

2512
01:49:28,989 --> 01:49:32,560
and so forth but there's actually an

2513
01:49:32,560 --> 01:49:34,360
architecture that I really like called

2514
01:49:34,360 --> 01:49:37,630
not res net but res next which was

2515
01:49:37,630 --> 01:49:39,850
actually the second-place winner in last

2516
01:49:39,850 --> 01:49:44,590
year's image net competition and like

2517
01:49:44,590 --> 01:49:46,930
ResNet you can put a number after the

2518
01:49:46,930 --> 01:49:49,300
res next to say like how big it is and

2519
01:49:49,300 --> 01:49:52,510
like my next step after resume 34 is

2520
01:49:52,510 --> 01:49:55,300
always res next 50 now you'll find res

2521
01:49:55,300 --> 01:49:58,120
next 50 takes like can take like twice

2522
01:49:58,120 --> 01:50:01,719
as long as ResNet 34 that can take like

2523
01:50:01,719 --> 01:50:06,090
2 to 4 times as much memory as retina 34

2524
01:50:06,090 --> 01:50:08,260
so what I wanted to do was I wanted to

2525
01:50:08,260 --> 01:50:10,390
rerun that previous notebook with res

2526
01:50:10,390 --> 01:50:12,910
next and increasing the image size to

2527
01:50:12,910 --> 01:50:15,699
turn on a node so here I just said

2528
01:50:15,699 --> 01:50:18,219
architecture equals res next 50 size

2529
01:50:18,219 --> 01:50:20,440
equals 299 and then I found that I had

2530
01:50:20,440 --> 01:50:21,940
to take the batch size all the way back

2531
01:50:21,940 --> 01:50:25,000
to 28 to get it to fit my GPU is 11 gig

2532
01:50:25,000 --> 01:50:27,850
if you're using AWS or cresol I think

2533
01:50:27,850 --> 01:50:30,550
they're like 12 gigs they might be

2534
01:50:30,550 --> 01:50:32,800
a bit higher but this is what I found I

2535
01:50:32,800 --> 01:50:34,659
had to do so then I this is literally a

2536
01:50:34,659 --> 01:50:36,699
copy of the previous notebook so you can

2537
01:50:36,699 --> 01:50:39,309
actually go file make a copy right and

2538
01:50:39,309 --> 01:50:41,469
then rerun it with with these different

2539
01:50:41,469 --> 01:50:45,400
parameters and so I deleted some of the

2540
01:50:45,400 --> 01:50:47,530
pros and some of the expiratory stuff to

2541
01:50:47,530 --> 01:50:49,869
see you know basically I said everything

2542
01:50:49,869 --> 01:50:52,750
else is the same all the same steps as

2543
01:50:52,750 --> 01:50:54,550
before there's my in fact you can kind

2544
01:50:54,550 --> 01:50:55,989
of see what this minimum service desk

2545
01:50:55,989 --> 01:50:57,460
looks like I didn't need to worry about

2546
01:50:57,460 --> 01:50:59,139
learning rate finder so I just left it

2547
01:50:59,139 --> 01:51:02,409
as is so transforms data equals loan

2548
01:51:02,409 --> 01:51:06,460
equals bit pre computed false feet with

2549
01:51:06,460 --> 01:51:08,940
cycle integrals one and freeze

2550
01:51:08,940 --> 01:51:12,130
differential learning rates bits and

2551
01:51:12,130 --> 01:51:14,739
more and you can see here I didn't do

2552
01:51:14,739 --> 01:51:17,619
the cycle mop thing because I found like

2553
01:51:17,619 --> 01:51:19,540
now that I'm using a bigger architecture

2554
01:51:19,540 --> 01:51:21,309
it's got more parameters it was

2555
01:51:21,309 --> 01:51:23,980
overfitting pretty quickly so rather

2556
01:51:23,980 --> 01:51:25,840
than like cycle length equals one never

2557
01:51:25,840 --> 01:51:28,000
finding the right spot it actually did

2558
01:51:28,000 --> 01:51:29,920
find the right spot and if I used longer

2559
01:51:29,920 --> 01:51:34,599
cycle legs I found that my validation

2560
01:51:34,599 --> 01:51:37,150
error was higher than my training error

2561
01:51:37,150 --> 01:51:40,900
it was over there so check us out though

2562
01:51:40,900 --> 01:51:43,869
by using these you know three steps

2563
01:51:43,869 --> 01:51:47,369
I got plus TTA 99.75

2564
01:51:47,369 --> 01:51:50,380
so what does that mean that means I have

2565
01:51:50,380 --> 01:51:54,400
one incorrect dog for incorrect cats and

2566
01:51:54,400 --> 01:51:58,780
when we look at the pictures of them my

2567
01:51:58,780 --> 01:52:02,050
incorrect dog has a cat now this one is

2568
01:52:02,050 --> 01:52:04,179
not a either this one is not either so

2569
01:52:04,179 --> 01:52:07,449
I've actually got one mistake and then

2570
01:52:07,449 --> 01:52:12,340
my incorrect dog is teeth right so like

2571
01:52:12,340 --> 01:52:15,250
we're at a point where we're now able to

2572
01:52:15,250 --> 01:52:18,639
train a classifier that's so good that

2573
01:52:18,639 --> 01:52:23,409
it has like basically one's dead right

2574
01:52:23,409 --> 01:52:25,239
and so when people say like we have

2575
01:52:25,239 --> 01:52:27,849
superhuman image performance now this is

2576
01:52:27,849 --> 01:52:29,079
kind of what they're talking about right

2577
01:52:29,079 --> 01:52:31,380
so did you actually when I looked at the

2578
01:52:31,380 --> 01:52:34,090
dog breed one I did this morning I was

2579
01:52:34,090 --> 01:52:36,400
like it was it was getting the dog

2580
01:52:36,400 --> 01:52:39,849
breeds much better than I ever could

2581
01:52:39,849 --> 01:52:42,130
so like hits this this is what we can

2582
01:52:42,130 --> 01:52:43,750
get to if you use a really modern

2583
01:52:43,750 --> 01:52:44,350
architect

2584
01:52:44,350 --> 01:52:48,160
like redneck and this suddenly took out

2585
01:52:48,160 --> 01:52:51,790
a tall way and remember don't like 20

2586
01:52:51,790 --> 01:52:55,540
minutes to Train so that's kind of where

2587
01:52:55,540 --> 01:53:02,640
we're up to so if you want to do

2588
01:53:02,640 --> 01:53:04,120
satellite imagery

2589
01:53:04,120 --> 01:53:07,720
instead right then it's the same thing

2590
01:53:07,720 --> 01:53:09,970
and in fact the the planet satellite

2591
01:53:09,970 --> 01:53:11,380
data sets already oh and Chris or if

2592
01:53:11,380 --> 01:53:12,610
you're using Chris or you can jump

2593
01:53:12,610 --> 01:53:17,320
straight there right and I just went

2594
01:53:17,320 --> 01:53:20,020
into this data stash planet and I can do

2595
01:53:20,020 --> 01:53:25,480
exactly the same thing right I can image

2596
01:53:25,480 --> 01:53:28,600
classifier from CSV right and you can

2597
01:53:28,600 --> 01:53:30,010
see these three lines are actually

2598
01:53:30,010 --> 01:53:31,690
exactly the same as my dog breed lines

2599
01:53:31,690 --> 01:53:34,240
you know how big how many lines are in

2600
01:53:34,240 --> 01:53:37,000
the file grab my validation indexes this

2601
01:53:37,000 --> 01:53:38,740
get data as you can see it's identical

2602
01:53:38,740 --> 01:53:42,220
except I've changed side on to top down

2603
01:53:42,220 --> 01:53:44,800
the satellite images about top down so I

2604
01:53:44,800 --> 01:53:47,020
can fit them vertically and they still

2605
01:53:47,020 --> 01:53:49,780
make sense right and so you can see here

2606
01:53:49,780 --> 01:53:51,460
I'm doing this trick round back to size

2607
01:53:51,460 --> 01:53:55,240
equals 64 and train a little bit first

2608
01:53:55,240 --> 01:53:57,100
learning rate find on right and

2609
01:53:57,100 --> 01:53:59,230
interestingly in this case you can see

2610
01:53:59,230 --> 01:54:01,890
it I want really high learning rates I

2611
01:54:01,890 --> 01:54:03,670
don't know what it is about this

2612
01:54:03,670 --> 01:54:05,800
particular data set this is true but

2613
01:54:05,800 --> 01:54:07,810
it's clearly I can use super high

2614
01:54:07,810 --> 01:54:08,920
learning rate so I use a lot here at a

2615
01:54:08,920 --> 01:54:12,790
point too and so I've trained for a

2616
01:54:12,790 --> 01:54:13,330
while

2617
01:54:13,330 --> 01:54:16,210
differential learning rates right and so

2618
01:54:16,210 --> 01:54:18,970
remember I said like if the data sets

2619
01:54:18,970 --> 01:54:20,980
very different to image net I probably

2620
01:54:20,980 --> 01:54:23,500
want to train those middle layers a lot

2621
01:54:23,500 --> 01:54:25,300
more so I'm using divided by three

2622
01:54:25,300 --> 01:54:27,340
rather than divided by ten all right the

2623
01:54:27,340 --> 01:54:28,930
other than that is the same thing cycle

2624
01:54:28,930 --> 01:54:33,430
Nauticals - all right and then I just

2625
01:54:33,430 --> 01:54:34,510
kind of keep an eye on it so you can

2626
01:54:34,510 --> 01:54:35,980
actually plot the loss if you go and

2627
01:54:35,980 --> 01:54:37,990
learned up shared a plot loss you can

2628
01:54:37,990 --> 01:54:40,900
see here that here's the first cycle is

2629
01:54:40,900 --> 01:54:43,150
the second cycle is the third cycle

2630
01:54:43,150 --> 01:54:45,220
right so you can see it gets better pops

2631
01:54:45,220 --> 01:54:46,960
out gets better pops out if better pops

2632
01:54:46,960 --> 01:54:48,220
out and each time it finds something

2633
01:54:48,220 --> 01:54:50,160
better than the last time

2634
01:54:50,160 --> 01:54:53,440
then set the size up to 128 and just

2635
01:54:53,440 --> 01:54:55,360
repeat exactly the last few steps and

2636
01:54:55,360 --> 01:54:58,500
then set up to 256

2637
01:54:58,500 --> 01:55:02,170
repeat the last two steps and then do

2638
01:55:02,170 --> 01:55:05,650
TTA and if you submit this and this gets

2639
01:55:05,650 --> 01:55:10,300
about 30th place in this competition so

2640
01:55:10,300 --> 01:55:12,820
these basic steps work super well this

2641
01:55:12,820 --> 01:55:14,590
this thing where I went all the way back

2642
01:55:14,590 --> 01:55:19,630
to a size of 64 I wouldn't do that if I

2643
01:55:19,630 --> 01:55:21,820
was doing like dogs and cats or dog

2644
01:55:21,820 --> 01:55:23,590
breeds because like this is so small

2645
01:55:23,590 --> 01:55:26,500
that if if the thing I was working on is

2646
01:55:26,500 --> 01:55:29,520
very similar to imagenet I would kind of

2647
01:55:29,520 --> 01:55:32,469
destroy those imagenet weights like 64

2648
01:55:32,469 --> 01:55:34,929
by 64 is so small but in this case the

2649
01:55:34,929 --> 01:55:36,460
satellite imagery data it's so different

2650
01:55:36,460 --> 01:55:38,590
to imagenet um you know I really found

2651
01:55:38,590 --> 01:55:39,460
that it worked pretty well

2652
01:55:39,460 --> 01:55:43,300
start right back to these tiny images it

2653
01:55:43,300 --> 01:55:45,330
really helped me to avoid overfitting

2654
01:55:45,330 --> 01:55:48,610
and interestingly using this kind of

2655
01:55:48,610 --> 01:55:50,500
approach I actually found that even with

2656
01:55:50,500 --> 01:55:53,860
using only 128 by 128 I was getting like

2657
01:55:53,860 --> 01:55:55,929
much better cackled results than really

2658
01:55:55,929 --> 01:55:57,610
everybody on the leader board

2659
01:55:57,610 --> 01:56:00,130
and when I say 30th place this is a very

2660
01:56:00,130 --> 01:56:02,710
recent competition right and so I find

2661
01:56:02,710 --> 01:56:05,710
like in the last year like a lot of

2662
01:56:05,710 --> 01:56:07,210
people have got a lot better at computer

2663
01:56:07,210 --> 01:56:09,190
vision and so the people in the top 50

2664
01:56:09,190 --> 01:56:10,510
in this competition were generally

2665
01:56:10,510 --> 01:56:13,060
ensemble in dozens of models lots of

2666
01:56:13,060 --> 01:56:15,640
people on a team lots of pre-processing

2667
01:56:15,640 --> 01:56:18,340
specific satellite data and so forth so

2668
01:56:18,340 --> 01:56:21,250
like to be able to get xxx using this

2669
01:56:21,250 --> 01:56:22,750
totally standard technique is pretty

2670
01:56:22,750 --> 01:56:24,960
cool

2671
01:56:24,960 --> 01:56:28,120
alright so now that we've got to this

2672
01:56:28,120 --> 01:56:29,620
point right we've got through two

2673
01:56:29,620 --> 01:56:32,650
lessons if you're still here then

2674
01:56:32,650 --> 01:56:35,260
hopefully you're thinking okay this is

2675
01:56:35,260 --> 01:56:37,690
actually pretty useful I want to do more

2676
01:56:37,690 --> 01:56:41,170
in which case Kressel might not be where

2677
01:56:41,170 --> 01:56:43,719
you want to stay the issues with Kressel

2678
01:56:43,719 --> 01:56:45,909
I mean it's it's it's pretty handy it's

2679
01:56:45,909 --> 01:56:47,679
pretty cheap and something we haven't

2680
01:56:47,679 --> 01:56:50,020
talked about much is paper space is

2681
01:56:50,020 --> 01:56:52,210
another great choice by the way paper

2682
01:56:52,210 --> 01:56:53,409
space are short they're going to be

2683
01:56:53,409 --> 01:56:55,780
releasing kress or like instant Drupal

2684
01:56:55,780 --> 01:56:57,159
notebooks unfortunately they're not

2685
01:56:57,159 --> 01:56:59,739
ready quite yet but they do have an

2686
01:56:59,739 --> 01:57:01,810
ability to basically they have the best

2687
01:57:01,810 --> 01:57:04,719
price performance relationship right now

2688
01:57:04,719 --> 01:57:08,110
and they you can SSH into them and use

2689
01:57:08,110 --> 01:57:10,540
them so they're also a great choice and

2690
01:57:10,540 --> 01:57:11,560
probably by the time this

2691
01:57:11,560 --> 01:57:13,870
the MOOC will probably have a separate

2692
01:57:13,870 --> 01:57:15,940
lesson showing you how to set up set up

2693
01:57:15,940 --> 01:57:18,100
paper space because there they're likely

2694
01:57:18,100 --> 01:57:20,800
to be a great option but at some point

2695
01:57:20,800 --> 01:57:21,880
you're probably going to want to look at

2696
01:57:21,880 --> 01:57:26,260
AWS a couple of reasons why the first is

2697
01:57:26,260 --> 01:57:30,460
as you all know by now amazon have been

2698
01:57:30,460 --> 01:57:33,370
kind enough to donate about $200,000

2699
01:57:33,370 --> 01:57:35,980
worth of compute time to this course so

2700
01:57:35,980 --> 01:57:37,840
I want to say thank you very much to

2701
01:57:37,840 --> 01:57:40,720
Amazon we've all been given credit so

2702
01:57:40,720 --> 01:57:42,190
everybody this year so thanks very much

2703
01:57:42,190 --> 01:57:46,420
hey don't worry we're so sorry you're

2704
01:57:46,420 --> 01:57:48,010
sure in the MOOC we didn't get it for

2705
01:57:48,010 --> 01:57:50,440
you but everybody here is like AWS

2706
01:57:50,440 --> 01:57:53,170
credits for everybody so um but you can

2707
01:57:53,170 --> 01:57:55,330
get even if you're not here in person

2708
01:57:55,330 --> 01:57:57,130
you can get AWS credits from lots of

2709
01:57:57,130 --> 01:58:00,130
places github has a student pack Google

2710
01:58:00,130 --> 01:58:02,050
for github student pack that's like 150

2711
01:58:02,050 --> 01:58:05,200
bucks worth of credits AWS educate can

2712
01:58:05,200 --> 01:58:08,320
get credits these our office students so

2713
01:58:08,320 --> 01:58:09,370
there's lots of places you can get

2714
01:58:09,370 --> 01:58:13,050
started on AWS pretty much everybody

2715
01:58:13,050 --> 01:58:15,250
everybody a lot of the people that you

2716
01:58:15,250 --> 01:58:18,660
might work with will be using AWS

2717
01:58:18,660 --> 01:58:21,700
because it's like super flexible right

2718
01:58:21,700 --> 01:58:25,600
now AWS has the fastest available GPUs

2719
01:58:25,600 --> 01:58:28,950
you can get in the cloud they're p3s

2720
01:58:28,950 --> 01:58:31,090
they're kind of expensive at three bucks

2721
01:58:31,090 --> 01:58:32,860
an hour but if you've got like a model

2722
01:58:32,860 --> 01:58:34,630
where you've done all the steps before

2723
01:58:34,630 --> 01:58:35,830
you're thinking this is looking pretty

2724
01:58:35,830 --> 01:58:38,470
good you know for 6 bucks you could get

2725
01:58:38,470 --> 01:58:41,800
a p3 for 2 hours and run turbo speed

2726
01:58:41,800 --> 01:58:45,880
right um we didn't start with AWS

2727
01:58:45,880 --> 01:58:48,130
because well hey it's like twice as

2728
01:58:48,130 --> 01:58:49,990
expensive as Chris Hall for the cheapest

2729
01:58:49,990 --> 01:58:54,310
GPU and being a Texan setup right but I

2730
01:58:54,310 --> 01:58:57,190
wanted to kind of go through and show

2731
01:58:57,190 --> 01:58:59,650
you how to get your AWS setup and so

2732
01:58:59,650 --> 01:59:01,060
we're going to be going slightly over

2733
01:59:01,060 --> 01:59:03,790
time to do that but I want to show you a

2734
01:59:03,790 --> 01:59:05,350
very quick place I feel prettier if you

2735
01:59:05,350 --> 01:59:07,300
have to but I want to show you very

2736
01:59:07,300 --> 01:59:10,270
quickly how you can get your AWS setup

2737
01:59:10,270 --> 01:59:14,050
right from scratch so basically you have

2738
01:59:14,050 --> 01:59:17,140
to go to consult on AWS but amazon.com

2739
01:59:17,140 --> 01:59:19,360
and it'll take you to the console right

2740
01:59:19,360 --> 01:59:22,300
and so you can follow along on the video

2741
01:59:22,300 --> 01:59:23,119
with this

2742
01:59:23,119 --> 01:59:27,080
quickly from here you have to go to AC -

2743
01:59:27,080 --> 01:59:29,659
this is where you set up your instances

2744
01:59:29,659 --> 01:59:33,619
and so from ec2 you need to do what's

2745
01:59:33,619 --> 01:59:35,690
called launching an instance so

2746
01:59:35,690 --> 01:59:36,920
launching an instance means you're

2747
01:59:36,920 --> 01:59:38,960
basically creating a computer right now

2748
01:59:38,960 --> 01:59:41,239
creating a computer on Amazon so I say

2749
01:59:41,239 --> 01:59:44,150
launch instance and what we've done is

2750
01:59:44,150 --> 01:59:47,480
we've created a fast AI it's got an amo

2751
01:59:47,480 --> 01:59:49,940
and ami is like a template for how your

2752
01:59:49,940 --> 01:59:51,710
computer's going to begin so if you've

2753
01:59:51,710 --> 01:59:54,260
got a community a Mis and type in fast

2754
01:59:54,260 --> 01:59:56,719
AI you'll see that there's one there

2755
01:59:56,719 --> 02:00:00,560
called fast AI part 1 version 2 for the

2756
02:00:00,560 --> 02:00:04,090
p2 ok so I'm going to select that and

2757
02:00:04,090 --> 02:00:06,409
then we need to say what kind of

2758
02:00:06,409 --> 02:00:08,900
computer do you want and so I can say I

2759
02:00:08,900 --> 02:00:13,219
want a GPU compute computer and then I

2760
02:00:13,219 --> 02:00:16,670
can say I want a p2 x large this is the

2761
02:00:16,670 --> 02:00:19,340
cheapest reasonably effective for deep

2762
02:00:19,340 --> 02:00:21,260
learning instance type they have and

2763
02:00:21,260 --> 02:00:25,520
then I can say launch and then I can say

2764
02:00:25,520 --> 02:00:29,510
launch and so at this point they asked

2765
02:00:29,510 --> 02:00:33,530
you to choose a key pair right now

2766
02:00:33,530 --> 02:00:35,630
if you don't have a key pair you have to

2767
02:00:35,630 --> 02:00:39,699
create one right so to create a key pair

2768
02:00:39,699 --> 02:00:44,480
you need to open your terminal if you

2769
02:00:44,480 --> 02:00:47,119
don't have a terminal if you've got a

2770
02:00:47,119 --> 02:00:48,830
Mac or Linux box you've definitely got

2771
02:00:48,830 --> 02:00:50,810
one if you've got Windows hopefully

2772
02:00:50,810 --> 02:00:53,929
you've got Ubuntu if you don't already

2773
02:00:53,929 --> 02:00:56,270
have Ubuntu setup you can go to the

2774
02:00:56,270 --> 02:01:01,820
Windows Store and click on Ubuntu right

2775
02:01:01,820 --> 02:01:04,280
we'll get it from the Windows Store so

2776
02:01:04,280 --> 02:01:08,230
from there you basically go SSH

2777
02:01:08,230 --> 02:01:12,320
- caged in and that will create like a

2778
02:01:12,320 --> 02:01:15,020
special password for your computer to be

2779
02:01:15,020 --> 02:01:16,969
able to log in to Amazon and then you

2780
02:01:16,969 --> 02:01:19,489
just hit enter three times okay and

2781
02:01:19,489 --> 02:01:22,250
that's going to create for you your key

2782
02:01:22,250 --> 02:01:24,560
you can use to get into Amazon alright

2783
02:01:24,560 --> 02:01:26,659
so then what I do is I copy that key

2784
02:01:26,659 --> 02:01:28,280
somewhere that I know where it is so

2785
02:01:28,280 --> 02:01:30,610
it'll be in the dot SSH folder

2786
02:01:30,610 --> 02:01:33,730
it's called IDRs a dub and so I'm going

2787
02:01:33,730 --> 02:01:39,490
to copy it to my hard drive so if you're

2788
02:01:39,490 --> 02:01:41,170
in a macro and Linux it'll already be in

2789
02:01:41,170 --> 02:01:42,550
an easy to find place it'll be in your

2790
02:01:42,550 --> 02:01:49,360
SSH folder that in documents so from

2791
02:01:49,360 --> 02:01:53,530
there back in AWS you have to tell it

2792
02:01:53,530 --> 02:01:55,300
that you've created this key so you can

2793
02:01:55,300 --> 02:01:59,260
go to key pairs and you say import key

2794
02:01:59,260 --> 02:02:02,020
pair and you just browse to that file

2795
02:02:02,020 --> 02:02:06,490
that you just created there it is

2796
02:02:06,490 --> 02:02:11,020
I say import okay so if you've ever used

2797
02:02:11,020 --> 02:02:13,420
SSH before you've already got the key

2798
02:02:13,420 --> 02:02:14,980
pair you don't have to do those depths

2799
02:02:14,980 --> 02:02:17,260
if you've used AWS before you've already

2800
02:02:17,260 --> 02:02:18,910
imported it you don't have to do that

2801
02:02:18,910 --> 02:02:20,770
step maybe haven't done any of those

2802
02:02:20,770 --> 02:02:24,430
things you have to do both steps so now

2803
02:02:24,430 --> 02:02:30,510
I can go ahead and launch my instance

2804
02:02:30,510 --> 02:02:35,520
community I am eyes search last day I

2805
02:02:35,520 --> 02:02:41,010
select launch and so now it asks me

2806
02:02:41,010 --> 02:02:43,810
what's where's your key pair and I can

2807
02:02:43,810 --> 02:02:48,990
choose that one that I just grabbed okay

2808
02:02:48,990 --> 02:02:51,400
so this is going to go ahead and create

2809
02:02:51,400 --> 02:02:55,390
a new computer for me to log into and

2810
02:02:55,390 --> 02:02:56,680
you can see here it says the following

2811
02:02:56,680 --> 02:02:58,870
have been initiated and so if I click on

2812
02:02:58,870 --> 02:03:02,770
that it'll show me this new computer

2813
02:03:02,770 --> 02:03:05,950
that I've created okay so it'll be able

2814
02:03:05,950 --> 02:03:08,520
to log into it

2815
02:03:08,520 --> 02:03:11,790
I need to know its IP address so here it

2816
02:03:11,790 --> 02:03:14,100
is the IP address there okay so I can

2817
02:03:14,100 --> 02:03:17,460
copy that and that's the IP address of

2818
02:03:17,460 --> 02:03:20,700
my computer so to get to this computer I

2819
02:03:20,700 --> 02:03:23,310
need to SSH to it so SSH into a computer

2820
02:03:23,310 --> 02:03:25,110
means connecting to that computer so

2821
02:03:25,110 --> 02:03:26,310
that it's like you're typing in that

2822
02:03:26,310 --> 02:03:29,960
computer so I type SSH and they username

2823
02:03:29,960 --> 02:03:33,690
for this instance is always Ubuntu right

2824
02:03:33,690 --> 02:03:36,420
and then I can paste in that IP address

2825
02:03:36,420 --> 02:03:38,910
and then there's one more thing I have

2826
02:03:38,910 --> 02:03:41,730
to do which is I have to connect up the

2827
02:03:41,730 --> 02:03:44,340
jupiter notebook on that instance to the

2828
02:03:44,340 --> 02:03:46,740
jupiter notebook on my machine and so to

2829
02:03:46,740 --> 02:03:48,930
do that there's just a particular flag

2830
02:03:48,930 --> 02:03:50,970
that i said okay we can talk about it on

2831
02:03:50,970 --> 02:03:52,440
the forums as to exactly what it does

2832
02:03:52,440 --> 02:03:55,220
but you just type - l.a today date

2833
02:03:55,220 --> 02:03:59,970
localhost 8 8 8 8 ok so like once you've

2834
02:03:59,970 --> 02:04:01,470
done it once you can like save that as

2835
02:04:01,470 --> 02:04:03,450
an alias and type in the same thing

2836
02:04:03,450 --> 02:04:05,250
every time

2837
02:04:05,250 --> 02:04:07,590
so we can check here we can see it says

2838
02:04:07,590 --> 02:04:09,510
that it's running so we should be able

2839
02:04:09,510 --> 02:04:12,570
to now hit enter first time ever which

2840
02:04:12,570 --> 02:04:14,700
sit reconnect to it it does checks this

2841
02:04:14,700 --> 02:04:19,830
is okay I'll say yes and then that goes

2842
02:04:19,830 --> 02:04:27,420
ahead and SSH is in so this ami is all

2843
02:04:27,420 --> 02:04:29,700
set up for you alright so you'll find

2844
02:04:29,700 --> 02:04:31,380
that the very first time you log in it

2845
02:04:31,380 --> 02:04:32,880
takes a few extra seconds because it

2846
02:04:32,880 --> 02:04:34,080
just kind of is getting everything set

2847
02:04:34,080 --> 02:04:36,600
up but once it's logged in you'll see

2848
02:04:36,600 --> 02:04:38,370
there that there's a directory called

2849
02:04:38,370 --> 02:04:41,820
fast AI and the fast AI directory

2850
02:04:41,820 --> 02:04:45,120
contains our fast AI repo that contains

2851
02:04:45,120 --> 02:04:48,810
all the notebooks or the code etc so I

2852
02:04:48,810 --> 02:04:51,690
can just go CD faster all right first

2853
02:04:51,690 --> 02:04:53,640
thing you do when you get in is to make

2854
02:04:53,640 --> 02:04:55,140
sure it's updated so you just go git

2855
02:04:55,140 --> 02:04:59,880
pull right and that updates to make sure

2856
02:04:59,880 --> 02:05:01,800
that your repo is the same as the most

2857
02:05:01,800 --> 02:05:05,190
recent video and so as you can see there

2858
02:05:05,190 --> 02:05:05,610
we go

2859
02:05:05,610 --> 02:05:06,840
let's make sure it's got all the most

2860
02:05:06,840 --> 02:05:08,520
recent code the second thing you should

2861
02:05:08,520 --> 02:05:11,970
do is type Condor and update you can

2862
02:05:11,970 --> 02:05:13,800
just do this maybe once a month or so

2863
02:05:13,800 --> 02:05:15,690
and that makes sure that the libraries

2864
02:05:15,690 --> 02:05:17,760
there are all the most recent libraries

2865
02:05:17,760 --> 02:05:19,260
I'm not going to run that so it takes a

2866
02:05:19,260 --> 02:05:20,440
couple of minutes okay

2867
02:05:20,440 --> 02:05:22,450
and then the last step is to type

2868
02:05:22,450 --> 02:05:26,650
particular notebook okay

2869
02:05:26,650 --> 02:05:29,800
so this is going to go ahead and launch

2870
02:05:29,800 --> 02:05:32,530
the triplet notebook server on this

2871
02:05:32,530 --> 02:05:35,200
machine again the first time I do it the

2872
02:05:35,200 --> 02:05:37,540
first time you do everything on AWS

2873
02:05:37,540 --> 02:05:40,450
it just takes like a minute or two and

2874
02:05:40,450 --> 02:05:42,190
then once you've done it in the future

2875
02:05:42,190 --> 02:05:44,380
we just as fast as running it locally

2876
02:05:44,380 --> 02:05:47,530
basically okay so you can see it's going

2877
02:05:47,530 --> 02:05:49,900
ahead and firing out the notebook and so

2878
02:05:49,900 --> 02:05:51,460
what's going to happen is that because

2879
02:05:51,460 --> 02:05:53,740
when we SSH into it we said to both

2880
02:05:53,740 --> 02:05:56,890
connect our notebook port to the remote

2881
02:05:56,890 --> 02:05:58,810
notebook port we're just going to be

2882
02:05:58,810 --> 02:06:00,880
able to use this locally so I see he

2883
02:06:00,880 --> 02:06:03,130
says here copy paste this URL so I'm

2884
02:06:03,130 --> 02:06:06,280
going to grab that URL and I'm going to

2885
02:06:06,280 --> 02:06:11,980
paste it into my browser and that's it

2886
02:06:11,980 --> 02:06:15,730
okay so this notebook is now actually

2887
02:06:15,730 --> 02:06:17,920
not running on my machine it's actually

2888
02:06:17,920 --> 02:06:21,130
running on AWS okay using the AWS GPU

2889
02:06:21,130 --> 02:06:23,230
it's got a lot of memory it's not the

2890
02:06:23,230 --> 02:06:25,870
fastest around but it's not terrible

2891
02:06:25,870 --> 02:06:28,480
you can always fire up a p3 if you want

2892
02:06:28,480 --> 02:06:30,070
something that's super fast this is

2893
02:06:30,070 --> 02:06:33,580
costing me ninety cents a minute okay so

2894
02:06:33,580 --> 02:06:36,010
when you're finished please don't forget

2895
02:06:36,010 --> 02:06:38,400
to shut it down right so to shut it down

2896
02:06:38,400 --> 02:06:43,030
you can right-click on it and say

2897
02:06:43,030 --> 02:06:48,850
instance date stop okay we've got five

2898
02:06:48,850 --> 02:06:51,670
hundred bucks of credit assuming that

2899
02:06:51,670 --> 02:06:53,380
you put your code down in the

2900
02:06:53,380 --> 02:06:55,750
spreadsheet one thing I forgot to do the

2901
02:06:55,750 --> 02:06:57,250
first time I showed you this by the way

2902
02:06:57,250 --> 02:07:02,170
I said make sure you choose a p2 the

2903
02:07:02,170 --> 02:07:03,430
second time I went through I didn't

2904
02:07:03,430 --> 02:07:05,530
choose p2 by mistake so just don't

2905
02:07:05,530 --> 02:07:09,370
forget choose gpq compute P - do you

2906
02:07:09,370 --> 02:07:11,379
have a question

2907
02:07:11,379 --> 02:07:15,659
my Bernice it's an hour thank you

2908
02:07:15,659 --> 02:07:20,379
90 cents an hour it also costs like I

2909
02:07:20,379 --> 02:07:21,789
don't know three or four bucks a month

2910
02:07:21,789 --> 02:07:24,249
for the storage as well thanks for

2911
02:07:24,249 --> 02:07:26,109
checking that all right see you next

2912
02:07:26,109 --> 02:07:29,309
week sorry we're a bit over

